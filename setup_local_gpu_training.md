# ğŸš€ BrickBox ë¡œì»¬ GPU í•™ìŠµ í™˜ê²½ êµ¬ì¶• ê°€ì´ë“œ

## ğŸ“‹ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### **ìµœì†Œ ìš”êµ¬ì‚¬í•­**
- **GPU**: NVIDIA GTX 1060 6GB ì´ìƒ (ê¶Œì¥: RTX 3060 12GB ì´ìƒ)
- **RAM**: 16GB ì´ìƒ
- **ì €ì¥ê³µê°„**: 50GB ì´ìƒ ì—¬ìœ ê³µê°„
- **CUDA**: 11.8 ì´ìƒ
- **Python**: 3.8-3.11

### **í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„**
- **GPU**: NVIDIA GeForce GTX 750 Ti (2GB VRAM) âš ï¸ **ì œí•œì **
- **CUDA**: 12.6 ì§€ì›
- **PyTorch**: 2.8.0+cpu (CUDA ë¯¸ì§€ì›)

---

## ğŸ”§ 1ë‹¨ê³„: CUDA ì§€ì› PyTorch ì„¤ì¹˜

### **ê¸°ì¡´ PyTorch ì œê±°**
```bash
pip uninstall torch torchvision torchaudio
```

### **CUDA 12.1 í˜¸í™˜ PyTorch ì„¤ì¹˜**
```bash
# CUDA 12.1 ë²„ì „ PyTorch ì„¤ì¹˜
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### **ì„¤ì¹˜ í™•ì¸**
```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"
```

---

## ğŸ§± 2ë‹¨ê³„: BrickBox í•™ìŠµ í™˜ê²½ ì„¤ì •

### **í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜**
```bash
# YOLO í•™ìŠµ íŒ¨í‚¤ì§€
pip install ultralytics
pip install opencv-python
pip install matplotlib
pip install seaborn

# ë°ì´í„° ì²˜ë¦¬
pip install pandas
pip install numpy
pip install pillow

# Supabase ì—°ë™
pip install supabase
pip install python-dotenv

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹°
pip install tqdm
pip install wandb  # ì„ íƒì‚¬í•­: ì‹¤í—˜ ì¶”ì 
```

### **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
```bash
# .env íŒŒì¼ ìƒì„±
cat > .env << EOF
VITE_SUPABASE_URL=https://npferbxuxocbfnfbpcnz.supabase.co
VITE_SUPABASE_ANON_KEY=your_anon_key_here
VITE_SUPABASE_SERVICE_ROLE=your_service_key_here

# GPU í•™ìŠµ ì„¤ì •
CUDA_VISIBLE_DEVICES=0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
EOF
```

---

## ğŸ¯ 3ë‹¨ê³„: ë¡œì»¬ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

### **ë¡œì»¬ GPU í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸**
```python
#!/usr/bin/env python3
"""
ğŸ§± BrickBox ë¡œì»¬ GPU í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
GTX 750 Ti ìµœì í™” ì„¤ì • í¬í•¨
"""

import os
import sys
import torch
import logging
from pathlib import Path
from ultralytics import YOLO
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LocalGPUTrainer:
    """ë¡œì»¬ GPU í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.device = self.setup_device()
        self.setup_memory_optimization()
    
    def setup_device(self):
        """GPU ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if torch.cuda.is_available():
            device = 'cuda'
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"ğŸš€ GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_name} ({gpu_memory:.1f}GB)")
            
            # GTX 750 Ti ìµœì í™”
            if "750" in gpu_name:
                logger.warning("âš ï¸ GTX 750 Ti ê°ì§€ - ë©”ëª¨ë¦¬ ìµœì í™” ëª¨ë“œ í™œì„±í™”")
                torch.cuda.set_per_process_memory_fraction(0.7)  # 70% ë©”ëª¨ë¦¬ ì‚¬ìš©
        else:
            device = 'cpu'
            logger.warning("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
        
        return device
    
    def setup_memory_optimization(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •"""
        if torch.cuda.is_available():
            # ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ì„¤ì •
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'
            torch.cuda.empty_cache()
            logger.info("ğŸ”§ GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì™„ë£Œ")
    
    def train_yolo_model(self, dataset_path: str, model_size: str = 'n'):
        """YOLO ëª¨ë¸ í•™ìŠµ"""
        logger.info(f"ğŸš€ YOLO{model_size} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        
        # ëª¨ë¸ í¬ê¸°ë³„ ì„¤ì • (GTX 750 Ti ìµœì í™”)
        model_configs = {
            'n': {'model': 'yolo11n.pt', 'batch': 4, 'imgsz': 416},   # ê°€ì¥ ê°€ë²¼ì›€
            's': {'model': 'yolo11s.pt', 'batch': 2, 'imgsz': 416},   # ì¤‘ê°„
            'm': {'model': 'yolo11m.pt', 'batch': 1, 'imgsz': 320},   # ë¬´ê±°ì›€
        }
        
        config = model_configs.get(model_size, model_configs['n'])
        
        # YOLO ëª¨ë¸ ì´ˆê¸°í™”
        model = YOLO(config['model'])
        
        # í•™ìŠµ ì„¤ì •
        training_name = f'brickbox_local_{model_size}_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
        
        logger.info(f"ğŸ“Š í•™ìŠµ ì„¤ì •:")
        logger.info(f"   - ëª¨ë¸: {config['model']}")
        logger.info(f"   - ë°°ì¹˜ í¬ê¸°: {config['batch']}")
        logger.info(f"   - ì´ë¯¸ì§€ í¬ê¸°: {config['imgsz']}")
        logger.info(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
        
        try:
            # í•™ìŠµ ì‹¤í–‰
            results = model.train(
                data=dataset_path,
                epochs=50,  # GTX 750 Tiìš©ìœ¼ë¡œ ì—í¬í¬ ìˆ˜ ì¡°ì •
                batch=config['batch'],
                imgsz=config['imgsz'],
                device=self.device,
                project='./output/local_training',
                name=training_name,
                save=True,
                plots=True,
                val=True,
                patience=10,
                save_period=10,
                # ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜
                workers=2,  # CPU ì›Œì»¤ ìˆ˜ ì œí•œ
                cache=True,  # ë°ì´í„° ìºì‹±
                amp=True,    # ìë™ í˜¼í•© ì •ë°€ë„
            )
            
            logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
            return results
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                logger.error("âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - ë°°ì¹˜ í¬ê¸°ë¥¼ ë” ì¤„ì´ê±°ë‚˜ ëª¨ë¸ í¬ê¸°ë¥¼ ë³€ê²½í•˜ì„¸ìš”")
                logger.info("ğŸ’¡ í•´ê²° ë°©ë²•:")
                logger.info("   - model_sizeë¥¼ 'n'ìœ¼ë¡œ ë³€ê²½")
                logger.info("   - imgszë¥¼ 320ìœ¼ë¡œ ë³€ê²½")
                logger.info("   - batchë¥¼ 1ë¡œ ë³€ê²½")
            else:
                logger.error(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python local_gpu_trainer.py <dataset.yaml> [model_size]")
        print("ëª¨ë¸ í¬ê¸°: n (ê°€ì¥ ê°€ë²¼ì›€), s (ì¤‘ê°„), m (ë¬´ê±°ì›€)")
        sys.exit(1)
    
    dataset_path = sys.argv[1]
    model_size = sys.argv[2] if len(sys.argv) > 2 else 'n'
    
    try:
        trainer = LocalGPUTrainer()
        results = trainer.train_yolo_model(dataset_path, model_size)
        print("ğŸ‰ ë¡œì»¬ GPU í•™ìŠµ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

---

## ğŸ“Š 4ë‹¨ê³„: ë°ì´í„°ì…‹ ì¤€ë¹„

### **YOLO í˜•ì‹ ë°ì´í„°ì…‹ êµ¬ì¡°**
```
dataset/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ labels/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â””â”€â”€ data.yaml
```

### **data.yaml íŒŒì¼ ì˜ˆì‹œ**
```yaml
path: ./dataset
train: images/train
val: images/val
test: images/test

nc: 1
names: ['lego_part']
```

---

## ğŸš€ 5ë‹¨ê³„: í•™ìŠµ ì‹¤í–‰

### **ê¸°ë³¸ í•™ìŠµ (GTX 750 Ti ìµœì í™”)**
```bash
# ê°€ì¥ ê°€ë²¼ìš´ ëª¨ë¸ë¡œ í•™ìŠµ
python local_gpu_trainer.py dataset/data.yaml n

# ì¤‘ê°„ í¬ê¸° ëª¨ë¸ë¡œ í•™ìŠµ (ë©”ëª¨ë¦¬ ì—¬ìœ  ìˆì„ ë•Œ)
python local_gpu_trainer.py dataset/data.yaml s
```

### **í•™ìŠµ ëª¨ë‹ˆí„°ë§**
```bash
# í•™ìŠµ ì§„í–‰ ìƒí™© í™•ì¸
tensorboard --logdir ./output/local_training

# GPU ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
nvidia-smi -l 1
```

---

## âš ï¸ GTX 750 Ti ì œí•œì‚¬í•­ ë° í•´ê²°ì±…

### **ë©”ëª¨ë¦¬ ì œí•œ (2GB VRAM)**
- **ë°°ì¹˜ í¬ê¸°**: ìµœëŒ€ 4 (YOLO11n ê¸°ì¤€)
- **ì´ë¯¸ì§€ í¬ê¸°**: 416x416 ê¶Œì¥
- **ëª¨ë¸ í¬ê¸°**: YOLO11në§Œ ê¶Œì¥

### **ì„±ëŠ¥ ìµœì í™”**
```python
# ë©”ëª¨ë¦¬ ì ˆì•½ ì„¤ì •
torch.cuda.set_per_process_memory_fraction(0.7)
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'

# í•™ìŠµ ì„¤ì •
batch_size = 2  # GTX 750 Tiìš©
imgsz = 416     # ì‘ì€ ì´ë¯¸ì§€ í¬ê¸°
workers = 2     # CPU ì›Œì»¤ ìˆ˜ ì œí•œ
```

### **ëŒ€ì•ˆ ì†”ë£¨ì…˜**
1. **Google Colab ì‚¬ìš©**: ë¬´ë£Œ GPU (T4 16GB)
2. **í´ë¼ìš°ë“œ GPU**: AWS, GCP, Azure
3. **ì—…ê·¸ë ˆì´ë“œ**: RTX 3060 12GB ì´ìƒ ê¶Œì¥

---

## ğŸ“ˆ 6ë‹¨ê³„: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### **í•™ìŠµ ì§„í–‰ ìƒí™© í™•ì¸**
```python
# í•™ìŠµ ì¤‘ ë©”íŠ¸ë¦­ í™•ì¸
import matplotlib.pyplot as plt
from ultralytics import YOLO

model = YOLO('path/to/best.pt')
results = model.val()

# ê²°ê³¼ ì‹œê°í™”
results.plot()
```

### **ëª¨ë¸ ì„±ëŠ¥ í‰ê°€**
```python
# ê²€ì¦ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
metrics = model.val(data='dataset/data.yaml')
print(f"mAP50: {metrics.box.map50}")
print(f"mAP50-95: {metrics.box.map}")
```

---

## ğŸ¯ 7ë‹¨ê³„: ëª¨ë¸ ë°°í¬

### **ONNX ë³€í™˜**
```python
# í•™ìŠµëœ ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
model = YOLO('path/to/best.pt')
model.export(format='onnx', imgsz=416, optimize=True)
```

### **Supabase ì—…ë¡œë“œ**
```python
# Supabaseì— ëª¨ë¸ ì—…ë¡œë“œ
from supabase import create_client

supabase = create_client(url, key)
with open('best.onnx', 'rb') as f:
    supabase.storage.from_('models').upload('local_model.onnx', f.read())
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### **GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜**
```bash
# í•´ê²° ë°©ë²•
1. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°: batch=1
2. ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸°: imgsz=320
3. ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸°: YOLO11n ì‚¬ìš©
4. ë©”ëª¨ë¦¬ ì •ë¦¬: torch.cuda.empty_cache()
```

### **CUDA ë²„ì „ ë¶ˆì¼ì¹˜**
```bash
# PyTorch ì¬ì„¤ì¹˜
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### **í•™ìŠµ ì†ë„ ê°œì„ **
```python
# í˜¼í•© ì •ë°€ë„ ì‚¬ìš©
amp=True

# ë°ì´í„° ë¡œë”© ìµœì í™”
workers=4
cache=True

# GPU ë©”ëª¨ë¦¬ ìµœì í™”
torch.backends.cudnn.benchmark = True
```

---

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

### **ì„¤ì¹˜ í™•ì¸**
- [ ] CUDA ì§€ì› PyTorch ì„¤ì¹˜
- [ ] Ultralytics ì„¤ì¹˜
- [ ] GPU ì¸ì‹ í™•ì¸
- [ ] ë°ì´í„°ì…‹ ì¤€ë¹„

### **í•™ìŠµ ì‹¤í–‰**
- [ ] ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
- [ ] ë°°ì¹˜ í¬ê¸° ì¡°ì •
- [ ] í•™ìŠµ ëª¨ë‹ˆí„°ë§
- [ ] ê²°ê³¼ ì €ì¥

### **ë°°í¬**
- [ ] ONNX ë³€í™˜
- [ ] Supabase ì—…ë¡œë“œ
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

---

## ğŸ‰ ì™„ë£Œ!

ì´ì œ ë¡œì»¬ GPU í™˜ê²½ì—ì„œ BrickBox YOLO ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

**ì£¼ì˜**: GTX 750 TiëŠ” ë©”ëª¨ë¦¬ê°€ ì œí•œì ì´ë¯€ë¡œ YOLO11n ëª¨ë¸ê³¼ ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
