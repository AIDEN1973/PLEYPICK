# 학습 품질 저하 근본 원인 분석 및 대책

## 현재 상황

**검증 결과:**
- Recall: 17.5% (목표 95%, **18.4% 달성**)
- mAP50: 53.0% (목표 90%, **58.9% 달성**)
- mAP50-95: 38.6% (목표 60%, **64.3% 달성**)
- Precision: 87.5% (양호)

**판정: 불량 - 프로덕션 배포 불가**

---

## 근본 원인 분석

### 1. 데이터셋 크기 부족 (최우선 문제)

**현재 상태:**
- 학습 이미지: **160개**
- 검증 이미지: **40개**
- 총 이미지: **200개**

**문제점:**
- YOLO 학습에 권장되는 최소 이미지 수: **1,000-5,000개**
- 현재 데이터셋은 권장량의 **4-20%** 수준
- 딥러닝 모델이 일반화하기에는 **매우 부족**

**영향:**
- 모델이 충분한 패턴을 학습하지 못함
- 과소적합(Underfitting) 발생
- Recall이 극히 낮은 주된 원인

---

### 2. 데이터 다양성 부족

**현재 상태:**
- 단일 부품만 포함 (6179330)
- 다양한 부품, 각도, 조명 조건 필요

**문제점:**
- 학습 데이터가 단일 부품에 치우침
- 다양한 케이스를 커버하지 못함

---

### 3. 학습 설정 최적화 여지

**현재 설정:**
- epochs: 100
- batch_size: 32 (stage1), 16 (stage2)
- learning_rate: 0.01
- mosaic: 0.3 (stage1), 0.5 (stage2)
- confidence threshold: 0.3 (학습), 0.25 (평가)

**개선 가능 사항:**
- 데이터 증강 강화
- 학습률 스케줄링 최적화
- Early stopping 조정

---

### 4. 모델 아키텍처 검토 필요

**현재 모델:**
- `brickbox_s_seg_stage2` (YOLO11s-seg 기반)

**고려사항:**
- 데이터셋이 작을 때는 더 작은 모델(n-seg)이 나을 수 있음
- 또는 더 큰 모델(m-seg)로 capacity 증가

---

## 근본 대책 (우선순위 순)

### 🔴 1순위: 데이터셋 크기 확대 (필수)

**목표:**
- 최소 **1,000개** 이미지 (현재의 5배)
- 권장 **5,000개** 이미지 (현재의 25배)

**실행 방법:**
```bash
# 합성 데이터셋 생성 확대
python scripts/prepare_yolo_dataset.py

# 여러 부품 포함하여 데이터셋 생성
# 부품당 200개 이미지 × 5개 부품 = 1,000개
# 부품당 200개 이미지 × 25개 부품 = 5,000개
```

**예상 효과:**
- Recall: 17.5% → **60-80%** (1,000개 기준)
- Recall: 17.5% → **85-95%** (5,000개 기준)

---

### 🟠 2순위: 데이터 증강 강화

**현재 설정:**
- mosaic: 0.3-0.5
- copy_paste: 0.4-0.6
- 기타 증강: 보통 수준

**개선 방안:**
```python
# 데이터 증강 강화 설정
augmentation = {
    'mosaic': 1.0,  # 최대 활용
    'copy_paste': 1.0,  # 최대 활용
    'mixup': 0.3,  # 추가
    'degrees': 10.0,  # 회전 증가
    'translate': 0.2,  # 이동 증가
    'scale': 0.9,  # 스케일 범위 증가
    'shear': 10.0,  # 기울임 추가
    'perspective': 0.001,  # 원근 변환 유지
    'flipud': 0.0,  # 상하 반전 (LEGO는 의미 없음)
    'fliplr': 0.5,  # 좌우 반전 유지
    'hsv_h': 0.015,  # 색상 변환 강화
    'hsv_s': 0.7,
    'hsv_v': 0.4,
    'erasing': 0.4,  # 지우기 증강 증가
}
```

**예상 효과:**
- 현재 데이터셋 200개 → 증강 후 **수천 개 효과**
- Recall: 17.5% → **40-50%** (데이터셋 크기 유지 시)

---

### 🟡 3순위: 학습 하이퍼파라미터 최적화

**개선 방안:**

```python
# 학습률 조정 (작은 데이터셋에 맞춤)
training_args = {
    'lr0': 0.005,  # 0.01 → 0.005 (작은 데이터셋은 더 작은 LR)
    'lrf': 0.1,  # 최종 학습률 조정
    'warmup_epochs': 5,  # 3 → 5 (더 긴 워밍업)
    'warmup_momentum': 0.8,
    'momentum': 0.937,
    'weight_decay': 0.0005,
    
    # Loss 가중치 조정 (Recall 향상에 집중)
    'box': 10.0,  # 7.5 → 10.0 (바운딩 박스 정확도 강조)
    'cls': 0.5,
    'dfl': 2.0,  # 1.5 → 2.0
    'pose': 12.0,
    
    # Early stopping 조정
    'patience': 25,  # 15 → 25 (작은 데이터셋은 더 많은 에폭 필요)
    
    # 평가 임계값 조정
    'conf': 0.15,  # 0.3 → 0.15 (더 많은 후보 수집)
    'iou': 0.50,
}
```

**예상 효과:**
- Recall: 17.5% → **30-40%**
- mAP50: 53.0% → **60-70%**

---

### 🟢 4순위: 모델 아키텍처 검토

**옵션 1: 더 작은 모델 (현재 데이터셋 크기 기준)**
```python
# YOLO11n-seg 사용 (더 작은 모델 = 과적합 위험 감소)
model = YOLO('yolo11n-seg.pt')
```

**옵션 2: 더 큰 모델 (데이터셋 확대 후)**
```python
# YOLO11m-seg 사용 (더 큰 capacity = 더 복잡한 패턴 학습)
model = YOLO('yolo11m-seg.pt')
```

---

### 🔵 5순위: 전이 학습 전략 재검토

**현재:**
- 사전학습 모델 사용 (YOLO11s-seg)
- Fine-tuning 진행

**개선 방안:**
- 더 많은 레이어 Freeze 시도
- 학습률 스케줄링 세밀 조정
- 단계적 Unfreeze 전략

---

## 실행 계획

### 단기 조치 (1주일 내)

1. **데이터셋 확대**
   - 현재: 총 **200개** 이미지 (단일 부품 또는 소수 부품)
   - 목표: 총 **1,000개** 이상 이미지
   - 권장 구성:
     - **옵션 A (여러 부품)**: 10개 부품 × 부품당 100개 = 총 1,000개
     - **옵션 B (집중)**: 5개 부품 × 부품당 200개 = 총 1,000개
     - **옵션 C (단일 부품 집중)**: 1개 부품 × 부품당 1,000개 = 총 1,000개

2. **데이터 증강 강화**
   - `scripts/local_yolo_training.py` 수정
   - mosaic, copy_paste 최대 활용
   - mixup 추가

3. **학습 파라미터 조정**
   - learning_rate 감소 (0.01 → 0.005)
   - confidence threshold 감소 (0.3 → 0.15)
   - patience 증가 (15 → 25)

### 중기 조치 (1개월 내)

1. **데이터셋 확대 완료**
   - 목표 **5,000개** 이미지
   - 다양한 부품, 각도, 조명 조건

2. **하이퍼파라미터 튜닝**
   - 그리드 서치 또는 베이지안 최적화
   - 최적 설정 찾기

### 장기 조치 (3개월 내)

1. **모델 아키텍처 실험**
   - 다양한 모델 크기 테스트
   - 최적 아키텍처 선택

2. **지속적인 데이터 수집**
   - 실제 사용 중 발생하는 실패 케이스 수집
   - 지속적인 모델 개선

---

## 예상 성능 개선

### 시나리오 1: 데이터셋 1,000개 + 증강 강화
- Recall: 17.5% → **60-75%**
- mAP50: 53.0% → **70-80%**
- mAP50-95: 38.6% → **50-60%**
- **판정: 개선됨 (아직 SLO 미달)**

### 시나리오 2: 데이터셋 5,000개 + 최적화
- Recall: 17.5% → **85-95%** ✅
- mAP50: 53.0% → **85-92%** ✅
- mAP50-95: 38.6% → **60-70%** ✅
- **판정: 양호 (SLO 기준 충족 예상)**

---

## 즉시 실행 가능한 조치

### 1. Confidence Threshold 조정 (5분)

```bash
# 검증 시 threshold 낮추기
python scripts/validate_registered_model.py --device auto
# 스크립트 내부에서 conf=0.15로 변경 필요
```

**예상 효과:** Recall 17.5% → 40-50%

### 2. 데이터 증강 즉시 강화 (10분)

`scripts/local_yolo_training.py` 수정:
- mosaic: 0.3 → 1.0
- copy_paste: 0.4 → 1.0
- mixup 추가

### 3. 학습 재실행 (2-3시간)

개선된 설정으로 재학습:
```bash
python scripts/local_yolo_training.py \
  --part_id <part_id> \
  --model_stage stage2 \
  --epochs 150 \
  --batch_size 16
```

---

## 결론

**근본 원인:**
1. 데이터셋 크기 부족 (200개는 너무 적음)
2. 데이터 다양성 부족

**근본 해결책:**
1. 데이터셋 확대 (최소 1,000개, 권장 5,000개)
2. 데이터 증강 강화
3. 학습 파라미터 최적화

**우선순위:**
1. 데이터셋 확대가 가장 중요 (80% 영향)
2. 증강 강화 (15% 영향)
3. 하이퍼파라미터 튜닝 (5% 영향)

