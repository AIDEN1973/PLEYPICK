#!/usr/bin/env python3 """ ê¸°ì¡´ ë Œë”ë§ íŒŒì¼ì— E2 JSON ë©”íƒ€ë°ì´í„° ì¶”ê°€ ìƒì„± ì–´ì œ ë Œë”ë§í•œ íŒŒì¼ë“¤ì— ìƒˆë¡œìš´ E2 ìŠ¤í‚¤ë§ˆ ì ìš© """ import os import sys import json import argparse from pathlib import Path from typing import Dict, List, Optional import logging # ë¡œê¹… ì„¤ì • logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') logger = logging.getLogger(__name__) def make_json_safe(obj): """JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ê°ì²´ë¡œ ë³€í™˜""" if isinstance(obj, dict): return {k: make_json_safe(v) for k, v in obj.items()} elif isinstance(obj, list): return [make_json_safe(item) for item in obj] elif hasattr(obj, 'tolist'): # numpy array return obj.tolist() elif hasattr(obj, 'item'): # numpy scalar return obj.item() else: return obj def create_e2_metadata(full_metadata: Dict, part_id: str, index: int = 0) -> Dict: """E2(Essential) JSON ìƒì„± - ê²½ëŸ‰í™”ëœ í•„ìˆ˜ ë©”íƒ€ë°ì´í„°ë§Œ í¬í•¨""" try: # E2 ìŠ¤í‚¤ë§ˆ ë²„ì „ e2_metadata = { 'schema_version': 'E2', 'pair_uid': full_metadata.get('pair_uid'), 'part_id': part_id, 'image_size': full_metadata.get('image_size'), # í•„ìˆ˜ ì–´ë…¸í…Œì´ì…˜ (bbox, seg) 'annotation': { 'bbox_norm_xywh': full_metadata.get('annotation', {}).get('bbox_norm_xywh'), 'bbox_pixel_xyxy': full_metadata.get('annotation', {}).get('bbox_pixel_xyxy'), 'seg': { 'rle_base64': full_metadata.get('annotation', {}).get('seg', {}).get('rle_base64'), 'compressed_size': full_metadata.get('annotation', {}).get('seg', {}).get('compressed_size') } }, # í•„ìˆ˜ QA ì§€í‘œ 'qa': { 'reprojection_rms_px': full_metadata.get('annotation', {}).get('quality_3d', {}).get('reprojection_error_rms_px'), 'depth_score': full_metadata.get('annotation', {}).get('quality_3d', {}).get('depth_map_validation', {}).get('depth_score') }, # ì„±ëŠ¥ ì§€í‘œ 'perf': { 'avg_confidence': 0.95, # ê¸°ë³¸ê°’ 'avg_inference_time_ms': 50 # ê¸°ë³¸ê°’ }, # ë¬´ê²°ì„± 'integrity': { 'validated_at': full_metadata.get('integrity', {}).get('validated_at'), 'qa_flag': full_metadata.get('integrity', {}).get('qa_flag') } } logger.info(f" E2 ë©”íƒ€ë°ì´í„° ìƒì„±: {part_id}_{index:03d}") return e2_metadata except Exception as e: logger.error(f" E2 ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}") return None def process_directory(input_dir: Path, dry_run: bool = False) -> Dict: """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ë Œë”ë§ íŒŒì¼ì— E2 JSON ì¶”ê°€""" stats = { 'processed': 0, 'skipped': 0, 'errors': 0, 'created_e2': 0 } logger.info(f" ë””ë ‰í† ë¦¬ ìŠ¤ìº”: {input_dir}") # ë¶€í’ˆë³„ í´ë” ìŠ¤ìº” for part_folder in input_dir.iterdir(): if not part_folder.is_dir(): continue part_id = part_folder.name logger.info(f"[DIR] ë¶€í’ˆ ì²˜ë¦¬: {part_id}") # ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸° image_files = [] for ext in ['*.webp', '*.png', '*.jpg']: image_files.extend(part_folder.glob(ext)) for image_path in image_files: try: # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ json_path = image_path.with_suffix('.json') e2_path = image_path.parent / f"{image_path.stem}_e2.json" # ì´ë¯¸ E2 JSONì´ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸° if e2_path.exists(): logger.info(f"â­ï¸ E2 JSON ì´ë¯¸ ì¡´ì¬: {e2_path.name}") stats['skipped'] += 1 continue # ê¸°ì¡´ JSON ë©”íƒ€ë°ì´í„° ë¡œë“œ if not json_path.exists(): logger.warning(f" ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ: {json_path.name}") stats['errors'] += 1 continue # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ with open(json_path, 'r', encoding='utf-8') as f: full_metadata = json.load(f) # E2 ë©”íƒ€ë°ì´í„° ìƒì„± e2_metadata = create_e2_metadata(full_metadata, part_id, 0) if e2_metadata is None: stats['errors'] += 1 continue if not dry_run: # E2 JSON ì €ì¥ with open(e2_path, 'w', encoding='utf-8') as f: json.dump(make_json_safe(e2_metadata), f, ensure_ascii=False, indent=2) logger.info(f" E2 JSON ìƒì„±: {e2_path.name}") stats['created_e2'] += 1 else: logger.info(f" [DRY RUN] E2 JSON ìƒì„± ì˜ˆì •: {e2_path.name}") stats['processed'] += 1 except Exception as e: logger.error(f" íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {image_path.name}: {e}") stats['errors'] += 1 return stats def main(): """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜""" parser = argparse.ArgumentParser(description='ê¸°ì¡´ ë Œë”ë§ íŒŒì¼ì— E2 JSON ë©”íƒ€ë°ì´í„° ì¶”ê°€') parser.add_argument('--input-dir', required=True, help='ë Œë”ë§ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬') parser.add_argument('--dry-run', action='store_true', help='ì‹¤ì œ ìƒì„±í•˜ì§€ ì•Šê³  ë¯¸ë¦¬ë³´ê¸°ë§Œ') parser.add_argument('--verbose', '-v', action='store_true', help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥') args = parser.parse_args() if args.verbose: logging.getLogger().setLevel(logging.DEBUG) input_dir = Path(args.input_dir) if not input_dir.exists(): logger.error(f" ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {input_dir}") return 1 logger.info(f" E2 JSON ì¶”ê°€ ìƒì„± ì‹œì‘") logger.info(f"ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬: {input_dir}") logger.info(f" ëª¨ë“œ: {'DRY RUN' if args.dry_run else 'ì‹¤ì œ ìƒì„±'}") # ì²˜ë¦¬ ì‹¤í–‰ stats = process_directory(input_dir, args.dry_run) # ê²°ê³¼ ì¶œë ¥ logger.info(f"\n ì²˜ë¦¬ ê²°ê³¼:") logger.info(f" - ì²˜ë¦¬ëœ íŒŒì¼: {stats['processed']}ê°œ") logger.info(f" - ê±´ë„ˆë›´ íŒŒì¼: {stats['skipped']}ê°œ") logger.info(f" - ì˜¤ë¥˜ íŒŒì¼: {stats['errors']}ê°œ") logger.info(f" - ìƒì„±ëœ E2 JSON: {stats['created_e2']}ê°œ") if args.dry_run: logger.info(f"\n ì‹¤ì œ ìƒì„±í•˜ë ¤ë©´ --dry-run ì˜µì…˜ì„ ì œê±°í•˜ì„¸ìš”") return 0 if stats['errors'] == 0 else 1 if __name__ == '__main__': sys.exit(main()) 