#!/usr/bin/env python3 """ BrickBox YOLO ë°ì´í„°ì…‹ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸ ë Œë”ë§ ì™„ë£Œ í›„ YOLO í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ ì¤€ë¹„ - ë°ì´í„°ì…‹ ê²€ì¦ ë° ì •ë¦¬ - í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í•  - YOLO í¬ë§· ê²€ì¦ - ë°ì´í„° ì¦ê°• ì„¤ì • """ import os import sys import json import shutil import random import logging from pathlib import Path from typing import Dict, List, Optional, Tuple from datetime import datetime import argparse # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€ project_root = Path(__file__).parent.parent sys.path.append(str(project_root)) class YOLODatasetPreparer: """YOLO ë°ì´í„°ì…‹ ì¤€ë¹„ í´ë˜ìŠ¤""" def __init__(self, output_dir: str = None): self.project_root = project_root self.output_dir = Path(output_dir) if output_dir else project_root / "output" / "synthetic" self.prepared_dir = self.output_dir / "prepared" # ë¡œê¹… ì„¤ì • self.setup_logging() # ë””ë ‰í† ë¦¬ ìƒì„± self.create_directories() def setup_logging(self): """ë¡œê¹… ì„¤ì •""" log_dir = self.output_dir / "logs" log_dir.mkdir(parents=True, exist_ok=True) logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[ logging.FileHandler(log_dir / f"dataset_preparation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"), logging.StreamHandler() ] ) self.logger = logging.getLogger(__name__) def create_directories(self): """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±""" directories = [ self.prepared_dir, self.prepared_dir / "images" / "train", self.prepared_dir / "images" / "val", self.prepared_dir / "images" / "test", self.prepared_dir / "labels" / "train", self.prepared_dir / "labels" / "val", self.prepared_dir / "labels" / "test" ] for directory in directories: directory.mkdir(parents=True, exist_ok=True) self.logger.info(f"[DIR] ë””ë ‰í† ë¦¬ ìƒì„±: {directory}") def scan_rendered_data(self) -> Dict: """ë Œë”ë§ëœ ë°ì´í„° ìŠ¤ìº”""" self.logger.info(" ë Œë”ë§ëœ ë°ì´í„° ìŠ¤ìº” ì‹œì‘...") scan_results = { 'total_parts': 0, 'total_images': 0, 'total_labels': 0, 'parts_data': {}, 'missing_labels': [], 'invalid_images': [], 'invalid_labels': [] } try: # ë¶€í’ˆë³„ ë””ë ‰í† ë¦¬ ìŠ¤ìº” for part_dir in self.output_dir.iterdir(): if not part_dir.is_dir() or part_dir.name.startswith('.'): continue part_id = part_dir.name images_dir = part_dir / "images" labels_dir = part_dir / "labels" if not images_dir.exists(): self.logger.warning(f" {part_id}: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì—†ìŒ") continue # ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” image_files = list(images_dir.glob("*.webp")) label_files = list(labels_dir.glob("*.txt")) if labels_dir.exists() else [] part_data = { 'part_id': part_id, 'image_count': len(image_files), 'label_count': len(label_files), 'images': [str(f) for f in image_files], 'labels': [str(f) for f in label_files], 'missing_labels': [], 'invalid_images': [], 'invalid_labels': [] } # íŒŒì¼ ë§¤ì¹­ ê²€ì‚¬ for img_file in image_files: label_file = labels_dir / f"{img_file.stem}.txt" if not label_file.exists(): part_data['missing_labels'].append(str(img_file)) scan_results['missing_labels'].append(str(img_file)) # ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì‚¬ for img_file in image_files: if not self.validate_image_file(img_file): part_data['invalid_images'].append(str(img_file)) scan_results['invalid_images'].append(str(img_file)) # ë¼ë²¨ ìœ íš¨ì„± ê²€ì‚¬ for label_file in label_files: if not self.validate_label_file(label_file): part_data['invalid_labels'].append(str(label_file)) scan_results['invalid_labels'].append(str(label_file)) scan_results['parts_data'][part_id] = part_data scan_results['total_parts'] += 1 scan_results['total_images'] += len(image_files) scan_results['total_labels'] += len(label_files) self.logger.info(f" {part_id}: {len(image_files)}ê°œ ì´ë¯¸ì§€, {len(label_files)}ê°œ ë¼ë²¨") self.logger.info(f" ë°ì´í„° ìŠ¤ìº” ì™„ë£Œ: {scan_results['total_parts']}ê°œ ë¶€í’ˆ, {scan_results['total_images']}ê°œ ì´ë¯¸ì§€") except Exception as e: self.logger.error(f" ë°ì´í„° ìŠ¤ìº” ì‹¤íŒ¨: {e}") scan_results['error'] = str(e) return scan_results def validate_image_file(self, image_path: Path) -> bool: """ì´ë¯¸ì§€ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ (WebP, JPG, PNG ì§€ì›)""" try: # íŒŒì¼ ì¡´ì¬ ë° í¬ê¸° ê²€ì‚¬ if not image_path.exists() or image_path.stat().st_size == 0: return False # ì§€ì›ë˜ëŠ” ì´ë¯¸ì§€ í˜•ì‹ ê²€ì‚¬ with open(image_path, 'rb') as f: header = f.read(12) # WebP í˜•ì‹ ê²€ì‚¬ if image_path.suffix.lower() == '.webp': if not header.startswith(b'RIFF') or b'WEBP' not in header: return False # JPEG í˜•ì‹ ê²€ì‚¬ elif image_path.suffix.lower() in ['.jpg', '.jpeg']: if not (header.startswith(b'\xff\xd8\xff') or header.startswith(b'\xff\xd8')): return False # PNG í˜•ì‹ ê²€ì‚¬ elif image_path.suffix.lower() == '.png': if not header.startswith(b'\x89PNG\r\n\x1a\n'): return False return True except Exception: return False def validate_label_file(self, label_path: Path) -> bool: """ë¼ë²¨ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ (YOLO + Segmentation ì§€ì›)""" try: if not label_path.exists() or label_path.stat().st_size == 0: return False with open(label_path, 'r') as f: lines = f.readlines() for line in lines: line = line.strip() if not line: continue parts = line.split() if len(parts) < 5: # ìµœì†Œ class_id x y w h return False # ìˆ«ì í˜•ì‹ ê²€ì‚¬ try: class_id = int(parts[0]) x, y, w, h = map(float, parts[1:5]) # ì¢Œí‘œ ë²”ìœ„ ê²€ì‚¬ (0-1) if not (0 <= x <= 1 and 0 <= y <= 1 and 0 <= w <= 1 and 0 <= h <= 1): return False # Segmentation ì¢Œí‘œ ê²€ì‚¬ (5ê°œ ì´ìƒì˜ ì¢Œí‘œê°€ ìˆëŠ” ê²½ìš°) if len(parts) > 5: # segmentation ì¢Œí‘œëŠ” ì§ìˆ˜ ê°œì—¬ì•¼ í•¨ (x1,y1,x2,y2,...) if (len(parts) - 5) % 2 != 0: return False # segmentation ì¢Œí‘œë„ 0-1 ë²”ìœ„ ë‚´ì— ìˆì–´ì•¼ í•¨ for i in range(5, len(parts)): coord = float(parts[i]) if not (0 <= coord <= 1): return False except ValueError: return False return True except Exception: return False def split_dataset(self, scan_results: Dict, train_ratio: float = 0.8, val_ratio: float = 0.1, test_ratio: float = 0.1) -> Dict: """ë°ì´í„°ì…‹ ë¶„í• """ self.logger.info(" ë°ì´í„°ì…‹ ë¶„í•  ì‹œì‘...") if abs(train_ratio + val_ratio + test_ratio - 1.0) > 0.001: raise ValueError("ë¶„í•  ë¹„ìœ¨ì˜ í•©ì´ 1.0ì´ì–´ì•¼ í•©ë‹ˆë‹¤") split_results = { 'train': {'parts': [], 'images': 0, 'labels': 0}, 'val': {'parts': [], 'images': 0, 'labels': 0}, 'test': {'parts': [], 'images': 0, 'labels': 0} } try: # ë¶€í’ˆë³„ë¡œ ë¶„í•  part_ids = list(scan_results['parts_data'].keys()) random.shuffle(part_ids) total_parts = len(part_ids) train_count = int(total_parts * train_ratio) val_count = int(total_parts * val_ratio) test_count = total_parts - train_count - val_count # ë¶„í•  ì‹¤í–‰ train_parts = part_ids[:train_count] val_parts = part_ids[train_count:train_count + val_count] test_parts = part_ids[train_count + val_count:] split_results['train']['parts'] = train_parts split_results['val']['parts'] = val_parts split_results['test']['parts'] = test_parts # í†µê³„ ê³„ì‚° for split_name, parts in [('train', train_parts), ('val', val_parts), ('test', test_parts)]: for part_id in parts: part_data = scan_results['parts_data'][part_id] split_results[split_name]['images'] += part_data['image_count'] split_results[split_name]['labels'] += part_data['label_count'] self.logger.info(f" ë¶„í•  ê²°ê³¼:") self.logger.info(f" - í›ˆë ¨: {len(train_parts)}ê°œ ë¶€í’ˆ, {split_results['train']['images']}ê°œ ì´ë¯¸ì§€") self.logger.info(f" - ê²€ì¦: {len(val_parts)}ê°œ ë¶€í’ˆ, {split_results['val']['images']}ê°œ ì´ë¯¸ì§€") self.logger.info(f" - í…ŒìŠ¤íŠ¸: {len(test_parts)}ê°œ ë¶€í’ˆ, {split_results['test']['images']}ê°œ ì´ë¯¸ì§€") except Exception as e: self.logger.error(f" ë°ì´í„°ì…‹ ë¶„í•  ì‹¤íŒ¨: {e}") raise return split_results def copy_dataset_files(self, scan_results: Dict, split_results: Dict): """ë°ì´í„°ì…‹ íŒŒì¼ ë³µì‚¬""" self.logger.info("[DIR] ë°ì´í„°ì…‹ íŒŒì¼ ë³µì‚¬ ì‹œì‘...") try: for split_name, split_data in split_results.items(): if split_name == 'parts': # ë©”íƒ€ë°ì´í„° ìŠ¤í‚µ continue split_images_dir = self.prepared_dir / "images" / split_name split_labels_dir = self.prepared_dir / "labels" / split_name copied_images = 0 copied_labels = 0 for part_id in split_data['parts']: part_data = scan_results['parts_data'][part_id] # ì´ë¯¸ì§€ ë³µì‚¬ for img_path in part_data['images']: src_path = Path(img_path) dst_path = split_images_dir / f"{part_id}_{src_path.name}" if src_path.exists(): shutil.copy2(src_path, dst_path) copied_images += 1 # ë¼ë²¨ ë³µì‚¬ for label_path in part_data['labels']: src_path = Path(label_path) dst_path = split_labels_dir / f"{part_id}_{src_path.name}" if src_path.exists(): shutil.copy2(src_path, dst_path) copied_labels += 1 self.logger.info(f"[DIR] {split_name}: {copied_images}ê°œ ì´ë¯¸ì§€, {copied_labels}ê°œ ë¼ë²¨ ë³µì‚¬ ì™„ë£Œ") except Exception as e: self.logger.error(f" íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {e}") raise def create_yolo_config(self, split_results: Dict): """YOLO ì„¤ì • íŒŒì¼ ìƒì„±""" self.logger.info("ğŸ“ YOLO ì„¤ì • íŒŒì¼ ìƒì„±...") config_path = self.prepared_dir / "data.yaml" yolo_config = { 'path': str(self.prepared_dir.absolute()), 'train': 'images/train', 'val': 'images/val', 'test': 'images/test', 'nc': 1, # í´ë˜ìŠ¤ ìˆ˜ (lego_part) 'names': ['lego_part'] } with open(config_path, 'w', encoding='utf-8') as f: import yaml yaml.dump(yolo_config, f, default_flow_style=False, allow_unicode=True) self.logger.info(f" YOLO ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}") def generate_dataset_report(self, scan_results: Dict, split_results: Dict) -> Dict: """ë°ì´í„°ì…‹ ë³´ê³ ì„œ ìƒì„±""" self.logger.info(" ë°ì´í„°ì…‹ ë³´ê³ ì„œ ìƒì„±...") report = { 'generation_time': datetime.now().isoformat(), 'dataset_info': { 'total_parts': scan_results['total_parts'], 'total_images': scan_results['total_images'], 'total_labels': scan_results['total_labels'], 'missing_labels': len(scan_results['missing_labels']), 'invalid_images': len(scan_results['invalid_images']), 'invalid_labels': len(scan_results['invalid_labels']) }, 'split_info': { 'train': { 'parts_count': len(split_results['train']['parts']), 'images_count': split_results['train']['images'], 'labels_count': split_results['train']['labels'] }, 'val': { 'parts_count': len(split_results['val']['parts']), 'images_count': split_results['val']['images'], 'labels_count': split_results['val']['labels'] }, 'test': { 'parts_count': len(split_results['test']['parts']), 'images_count': split_results['test']['images'], 'labels_count': split_results['test']['labels'] } }, 'quality_issues': { 'missing_labels': scan_results['missing_labels'][:10], # ì²˜ìŒ 10ê°œë§Œ 'invalid_images': scan_results['invalid_images'][:10], 'invalid_labels': scan_results['invalid_labels'][:10] } } # ë³´ê³ ì„œ ì €ì¥ report_path = self.prepared_dir / "dataset_report.json" with open(report_path, 'w', encoding='utf-8') as f: json.dump(report, f, indent=2, ensure_ascii=False) self.logger.info(f" ë°ì´í„°ì…‹ ë³´ê³ ì„œ ì €ì¥: {report_path}") return report def prepare_dataset(self, train_ratio: float = 0.8, val_ratio: float = 0.1, test_ratio: float = 0.1) -> Dict: """ì „ì²´ ë°ì´í„°ì…‹ ì¤€ë¹„ íŒŒì´í”„ë¼ì¸""" self.logger.info(" YOLO ë°ì´í„°ì…‹ ì¤€ë¹„ íŒŒì´í”„ë¼ì¸ ì‹œì‘...") preparation_results = { 'start_time': datetime.now(), 'scan_results': None, 'split_results': None, 'report': None, 'success': False, 'error': None } try: # 1. ë Œë”ë§ëœ ë°ì´í„° ìŠ¤ìº” self.logger.info(" 1ë‹¨ê³„: ë Œë”ë§ëœ ë°ì´í„° ìŠ¤ìº”") scan_results = self.scan_rendered_data() preparation_results['scan_results'] = scan_results if scan_results.get('error'): raise ValueError(f"ë°ì´í„° ìŠ¤ìº” ì‹¤íŒ¨: {scan_results['error']}") # 2. ë°ì´í„°ì…‹ ë¶„í•  self.logger.info(" 2ë‹¨ê³„: ë°ì´í„°ì…‹ ë¶„í• ") split_results = self.split_dataset(scan_results, train_ratio, val_ratio, test_ratio) preparation_results['split_results'] = split_results # 3. íŒŒì¼ ë³µì‚¬ self.logger.info("[DIR] 3ë‹¨ê³„: íŒŒì¼ ë³µì‚¬") self.copy_dataset_files(scan_results, split_results) # 4. YOLO ì„¤ì • íŒŒì¼ ìƒì„± self.logger.info("ğŸ“ 4ë‹¨ê³„: YOLO ì„¤ì • íŒŒì¼ ìƒì„±") self.create_yolo_config(split_results) # 5. ë³´ê³ ì„œ ìƒì„± self.logger.info(" 5ë‹¨ê³„: ë³´ê³ ì„œ ìƒì„±") report = self.generate_dataset_report(scan_results, split_results) preparation_results['report'] = report preparation_results['success'] = True preparation_results['end_time'] = datetime.now() self.logger.info(" ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!") self.logger.info(f"[DIR] ì¤€ë¹„ëœ ë°ì´í„°ì…‹: {self.prepared_dir}") except Exception as e: preparation_results['error'] = str(e) preparation_results['end_time'] = datetime.now() self.logger.error(f" ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨: {e}") return preparation_results def main(): """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜""" parser = argparse.ArgumentParser(description='YOLO ë°ì´í„°ì…‹ ì¤€ë¹„') parser.add_argument('--output-dir', type=str, help='ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ') parser.add_argument('--train-ratio', type=float, default=0.8, help='í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨') parser.add_argument('--val-ratio', type=float, default=0.1, help='ê²€ì¦ ë°ì´í„° ë¹„ìœ¨') parser.add_argument('--test-ratio', type=float, default=0.1, help='í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨') parser.add_argument('--scan-only', action='store_true', help='ë°ì´í„° ìŠ¤ìº”ë§Œ ì‹¤í–‰') args = parser.parse_args() # ì¤€ë¹„ê¸° ì´ˆê¸°í™” preparer = YOLODatasetPreparer(args.output_dir) try: if args.scan_only: # ë°ì´í„° ìŠ¤ìº”ë§Œ ì‹¤í–‰ scan_results = preparer.scan_rendered_data() print("ë°ì´í„° ìŠ¤ìº” ê²°ê³¼:") print(json.dumps(scan_results, indent=2, ensure_ascii=False)) else: # ì „ì²´ ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤í–‰ results = preparer.prepare_dataset(args.train_ratio, args.val_ratio, args.test_ratio) print("ë°ì´í„°ì…‹ ì¤€ë¹„ ê²°ê³¼:") print(json.dumps(results, indent=2, ensure_ascii=False)) except Exception as e: print(f" ì‹¤í–‰ ì‹¤íŒ¨: {e}") sys.exit(1) if __name__ == "__main__": main() 