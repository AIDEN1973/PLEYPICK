#!/usr/bin/env python3
"""
ğŸ§± BrickBox ë ˆê³  YOLO í•™ìŠµ í†µí•© íŒŒì´í”„ë¼ì¸

ë Œë”ë§ ì™„ë£Œ í›„ ì „ì²´ YOLO í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ìë™í™”í•˜ëŠ” í†µí•© ìŠ¤í¬ë¦½íŠ¸
1. ë°ì´í„°ì…‹ ì¤€ë¹„
2. YOLO ëª¨ë¸ í•™ìŠµ
3. ëª¨ë¸ ê²€ì¦
4. ëª¨ë¸ ë°°í¬
5. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import os
import sys
import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# í•˜ìœ„ ìŠ¤í¬ë¦½íŠ¸ import
try:
    from prepare_yolo_dataset import YOLODatasetPreparer
    from train_yolo_lego import LegoYOLOTrainer
    from deploy_yolo_model import YOLOModelDeployer
    SCRIPTS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ í•˜ìœ„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    SCRIPTS_AVAILABLE = False

class LegoYOLOTrainingPipeline:
    """ë ˆê³  YOLO í•™ìŠµ í†µí•© íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config_path: str = None):
        self.project_root = project_root
        self.pipeline_dir = project_root / "output" / "pipeline"
        self.config_path = config_path
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.create_directories()
        
        # ì„¤ì • ë¡œë“œ
        self.config = self.load_config()
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_dir = self.pipeline_dir / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        directories = [
            self.pipeline_dir,
            self.pipeline_dir / "logs",
            self.pipeline_dir / "results"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {directory}")
    
    def load_config(self) -> Dict:
        """íŒŒì´í”„ë¼ì¸ ì„¤ì • ë¡œë“œ"""
        default_config = {
            'dataset': {
                'output_dir': str(project_root / "output" / "synthetic"),
                'train_ratio': 0.8,
                'val_ratio': 0.1,
                'test_ratio': 0.1,
                'min_images_per_part': 10,
                'max_images_per_part': 1000
            },
            'training': {
                'epochs': 100,
                'batch_size': 16,
                'imgsz': 640,
                'device': 'auto',
                'patience': 10,
                'save_period': 10
            },
            'deployment': {
                'model_name': 'lego_yolo_custom',
                'backup_existing': True,
                'test_deployed': True
            },
            'pipeline': {
                'skip_dataset_prep': False,
                'skip_training': False,
                'skip_deployment': False,
                'continue_on_error': False
            }
        }
        
        if self.config_path and Path(self.config_path).exists():
            with open(self.config_path, 'r', encoding='utf-8') as f:
                import yaml
                user_config = yaml.safe_load(f)
                self.update_config(default_config, user_config)
        
        return default_config
    
    def update_config(self, base_config: Dict, user_config: Dict):
        """ì„¤ì • ì—…ë°ì´íŠ¸ (ì¬ê·€ì )"""
        for key, value in user_config.items():
            if key in base_config and isinstance(base_config[key], dict) and isinstance(value, dict):
                self.update_config(base_config[key], value)
            else:
                base_config[key] = value
    
    def check_prerequisites(self) -> Tuple[bool, List[str]]:
        """ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
        self.logger.info("ğŸ” ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸...")
        
        issues = []
        
        # 1. í•˜ìœ„ ìŠ¤í¬ë¦½íŠ¸ í™•ì¸
        if not SCRIPTS_AVAILABLE:
            issues.append("í•˜ìœ„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # 2. ë Œë”ë§ëœ ë°ì´í„° í™•ì¸
        dataset_dir = Path(self.config['dataset']['output_dir'])
        if not dataset_dir.exists():
            issues.append(f"ë Œë”ë§ëœ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {dataset_dir}")
        else:
            # ì´ë¯¸ì§€ íŒŒì¼ í™•ì¸
            image_files = list(dataset_dir.rglob("*.webp"))
            if not image_files:
                issues.append("ë Œë”ë§ëœ WebP ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            else:
                self.logger.info(f"ğŸ“Š ë°œê²¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ")
        
        # 3. í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸
        try:
            import ultralytics
            self.logger.info("âœ… ultralytics íŒ¨í‚¤ì§€ í™•ì¸")
        except ImportError:
            issues.append("ultralytics íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            import torch
            self.logger.info("âœ… PyTorch íŒ¨í‚¤ì§€ í™•ì¸")
        except ImportError:
            issues.append("PyTorch íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # 4. GPU ê°€ìš©ì„± í™•ì¸
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                self.logger.info(f"ğŸš€ GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_count}ê°œ")
            else:
                self.logger.warning("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤")
        except:
            pass
        
        success = len(issues) == 0
        if success:
            self.logger.info("âœ… ëª¨ë“  ì‚¬ì „ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±")
        else:
            self.logger.error(f"âŒ ì‚¬ì „ ìš”êµ¬ì‚¬í•­ ì‹¤íŒ¨: {issues}")
        
        return success, issues
    
    def prepare_dataset(self) -> Dict:
        """1ë‹¨ê³„: ë°ì´í„°ì…‹ ì¤€ë¹„"""
        self.logger.info("ğŸ“Š 1ë‹¨ê³„: ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹œì‘...")
        
        try:
            preparer = YOLODatasetPreparer(self.config['dataset']['output_dir'])
            
            results = preparer.prepare_dataset(
                train_ratio=self.config['dataset']['train_ratio'],
                val_ratio=self.config['dataset']['val_ratio'],
                test_ratio=self.config['dataset']['test_ratio']
            )
            
            if results['success']:
                self.logger.info("âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ")
            else:
                self.logger.error(f"âŒ ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨: {results.get('error', 'Unknown error')}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def train_model(self) -> Dict:
        """2ë‹¨ê³„: YOLO ëª¨ë¸ í•™ìŠµ"""
        self.logger.info("ğŸš€ 2ë‹¨ê³„: YOLO ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        
        try:
            trainer = LegoYOLOTrainer()
            
            results = trainer.run_full_pipeline()
            
            if results['success']:
                self.logger.info("âœ… YOLO ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            else:
                self.logger.error(f"âŒ YOLO ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {results.get('error', 'Unknown error')}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ YOLO ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def deploy_model(self, model_path: str = None) -> Dict:
        """3ë‹¨ê³„: ëª¨ë¸ ë°°í¬"""
        self.logger.info("ğŸš€ 3ë‹¨ê³„: ëª¨ë¸ ë°°í¬ ì‹œì‘...")
        
        try:
            deployer = YOLOModelDeployer()
            
            results = deployer.run_deployment_pipeline(
                model_path=model_path,
                model_name=self.config['deployment']['model_name']
            )
            
            if results['success']:
                self.logger.info("âœ… ëª¨ë¸ ë°°í¬ ì™„ë£Œ")
            else:
                self.logger.error(f"âŒ ëª¨ë¸ ë°°í¬ ì‹¤íŒ¨: {results.get('error', 'Unknown error')}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë°°í¬ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def run_full_pipeline(self) -> Dict:
        """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.logger.info("ğŸ¯ ë ˆê³  YOLO í•™ìŠµ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
        
        pipeline_results = {
            'start_time': datetime.now(),
            'prerequisites': None,
            'dataset_preparation': None,
            'training': None,
            'deployment': None,
            'success': False,
            'total_time': None,
            'error': None
        }
        
        start_time = time.time()
        
        try:
            # 0. ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸
            self.logger.info("ğŸ” 0ë‹¨ê³„: ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸")
            prereq_success, prereq_issues = self.check_prerequisites()
            pipeline_results['prerequisites'] = {
                'success': prereq_success,
                'issues': prereq_issues
            }
            
            if not prereq_success:
                raise ValueError(f"ì‚¬ì „ ìš”êµ¬ì‚¬í•­ ì‹¤íŒ¨: {prereq_issues}")
            
            # 1. ë°ì´í„°ì…‹ ì¤€ë¹„
            if not self.config['pipeline']['skip_dataset_prep']:
                self.logger.info("ğŸ“Š 1ë‹¨ê³„: ë°ì´í„°ì…‹ ì¤€ë¹„")
                dataset_results = self.prepare_dataset()
                pipeline_results['dataset_preparation'] = dataset_results
                
                if not dataset_results['success']:
                    if not self.config['pipeline']['continue_on_error']:
                        raise ValueError(f"ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨: {dataset_results.get('error', 'Unknown error')}")
                    else:
                        self.logger.warning("âš ï¸ ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨í–ˆì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤")
            else:
                self.logger.info("â­ï¸ ë°ì´í„°ì…‹ ì¤€ë¹„ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
                pipeline_results['dataset_preparation'] = {'success': True, 'skipped': True}
            
            # 2. ëª¨ë¸ í•™ìŠµ
            if not self.config['pipeline']['skip_training']:
                self.logger.info("ğŸš€ 2ë‹¨ê³„: YOLO ëª¨ë¸ í•™ìŠµ")
                training_results = self.train_model()
                pipeline_results['training'] = training_results
                
                if not training_results['success']:
                    if not self.config['pipeline']['continue_on_error']:
                        raise ValueError(f"ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {training_results.get('error', 'Unknown error')}")
                    else:
                        self.logger.warning("âš ï¸ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨í–ˆì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤")
            else:
                self.logger.info("â­ï¸ ëª¨ë¸ í•™ìŠµ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
                pipeline_results['training'] = {'success': True, 'skipped': True}
            
            # 3. ëª¨ë¸ ë°°í¬
            if not self.config['pipeline']['skip_deployment']:
                self.logger.info("ğŸš€ 3ë‹¨ê³„: ëª¨ë¸ ë°°í¬")
                
                # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ ì°¾ê¸°
                model_path = None
                if pipeline_results['training'] and pipeline_results['training'].get('training'):
                    model_path = pipeline_results['training']['training'].get('model_path')
                
                deployment_results = self.deploy_model(model_path)
                pipeline_results['deployment'] = deployment_results
                
                if not deployment_results['success']:
                    if not self.config['pipeline']['continue_on_error']:
                        raise ValueError(f"ëª¨ë¸ ë°°í¬ ì‹¤íŒ¨: {deployment_results.get('error', 'Unknown error')}")
                    else:
                        self.logger.warning("âš ï¸ ëª¨ë¸ ë°°í¬ ì‹¤íŒ¨í–ˆì§€ë§Œ íŒŒì´í”„ë¼ì¸ì„ ì™„ë£Œí•©ë‹ˆë‹¤")
            else:
                self.logger.info("â­ï¸ ëª¨ë¸ ë°°í¬ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
                pipeline_results['deployment'] = {'success': True, 'skipped': True}
            
            pipeline_results['success'] = True
            pipeline_results['end_time'] = datetime.now()
            pipeline_results['total_time'] = time.time() - start_time
            
            self.logger.info("ğŸ‰ ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            self.logger.info(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {pipeline_results['total_time']:.2f}ì´ˆ")
            
        except Exception as e:
            pipeline_results['error'] = str(e)
            pipeline_results['end_time'] = datetime.now()
            pipeline_results['total_time'] = time.time() - start_time
            self.logger.error(f"âŒ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
        
        # ê²°ê³¼ ì €ì¥
        results_file = self.pipeline_dir / f"pipeline_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(pipeline_results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì €ì¥: {results_file}")
        
        return pipeline_results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë ˆê³  YOLO í•™ìŠµ í†µí•© íŒŒì´í”„ë¼ì¸')
    parser.add_argument('--config', type=str, help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--skip-dataset', action='store_true', help='ë°ì´í„°ì…‹ ì¤€ë¹„ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-training', action='store_true', help='ëª¨ë¸ í•™ìŠµ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-deployment', action='store_true', help='ëª¨ë¸ ë°°í¬ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--continue-on-error', action='store_true', help='ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê³„ì† ì§„í–‰')
    parser.add_argument('--check-prerequisites', action='store_true', help='ì‚¬ì „ ìš”êµ¬ì‚¬í•­ë§Œ í™•ì¸')
    
    args = parser.parse_args()
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = LegoYOLOTrainingPipeline(args.config)
    
    # ì„¤ì • ì—…ë°ì´íŠ¸
    if args.skip_dataset:
        pipeline.config['pipeline']['skip_dataset_prep'] = True
    if args.skip_training:
        pipeline.config['pipeline']['skip_training'] = True
    if args.skip_deployment:
        pipeline.config['pipeline']['skip_deployment'] = True
    if args.continue_on_error:
        pipeline.config['pipeline']['continue_on_error'] = True
    
    try:
        if args.check_prerequisites:
            # ì‚¬ì „ ìš”êµ¬ì‚¬í•­ë§Œ í™•ì¸
            success, issues = pipeline.check_prerequisites()
            print(f"ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸: {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
            if issues:
                print("ë°œê²¬ëœ ë¬¸ì œì :")
                for issue in issues:
                    print(f"  - {issue}")
            
        else:
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            results = pipeline.run_full_pipeline()
            print("í•™ìŠµ íŒŒì´í”„ë¼ì¸ ê²°ê³¼:")
            print(json.dumps(results, indent=2, ensure_ascii=False))
            
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
