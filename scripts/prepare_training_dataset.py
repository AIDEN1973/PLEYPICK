#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•©ì„± ë°ì´í„°ë¥¼ YOLO í•™ìŠµ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import json
import shutil
import argparse
from pathlib import Path
from typing import List, Dict
import random
from dataset_version_manager import DatasetVersionManager

# ì¸ì½”ë”© ì„¤ì •
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

def convert_json_to_yolo(json_path: str) -> str:
    """JSON ì–´ë…¸í…Œì´ì…˜ì„ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    bbox = data.get('bounding_box', {})
    polygon_uv = data.get('polygon_uv', [])
    
    # YOLO í˜•ì‹: class_id center_x center_y width height
    class_id = 0  # ë‹¨ì¼ í´ë˜ìŠ¤: lego_part
    center_x = bbox.get('center_x', 0.5)
    center_y = bbox.get('center_y', 0.5)
    width = bbox.get('width', 0.7)
    height = bbox.get('height', 0.7)
    
    # Segmentation polygon (optional)
    polygon_str = ""
    if polygon_uv:
        # Flatten polygon coordinates
        coords = []
        for point in polygon_uv:
            coords.extend([point[0], point[1]])
        polygon_str = " " + " ".join([f"{c:.6f}" for c in coords])
    
    return f"{class_id} {center_x:.6f} {center_y:.6f} {width:.6f} {height:.6f}{polygon_str}"

def prepare_dataset(
    source_dir: str = "output/synthetic",
    target_dir: str = "output/dataset_synthetic",
    train_split: float = 0.8,
    force_rebuild: bool = False
):
    """ìŠ¤ë§ˆíŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ - ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›"""
    source_path = Path(source_dir)
    target_path = Path(target_dir)
    
    print(f"[DIR] ìŠ¤ë§ˆíŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹œì‘: {source_dir} -> {target_dir}")
    
    # ê°•ì œ ì¬ìƒì„± ëª¨ë“œ
    if force_rebuild and target_path.exists():
        print("ğŸ—‘ï¸ ê°•ì œ ì¬ìƒì„± ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„°ì…‹ í´ë” ì‚­ì œ ì¤‘...")
        shutil.rmtree(target_path)
        print("[OK] ê¸°ì¡´ ë°ì´í„°ì…‹ í´ë” ì‚­ì œ ì™„ë£Œ")
    elif target_path.exists():
        print("[RETRY] ì¦ë¶„ ì—…ë°ì´íŠ¸ ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„°ì…‹ ìœ ì§€í•˜ê³  ìƒˆ íŒŒì¼ë§Œ ì¶”ê°€")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    (target_path / "images" / "train").mkdir(parents=True, exist_ok=True)
    (target_path / "images" / "val").mkdir(parents=True, exist_ok=True)
    # [FIX] ìˆ˜ì •ë¨: labelsëŠ” element_idë³„ë¡œ ìƒì„±ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒì„±í•˜ì§€ ì•ŠìŒ
    (target_path / "metadata").mkdir(parents=True, exist_ok=True)
    
    print("[DIR] dataset_synthetic í´ë” êµ¬ì¡° ìƒì„± ì¤‘...")
    
    # ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘ (dataset_synthetic êµ¬ì¡° ìš°ì„ , êµ¬ êµ¬ì¡° í´ë°±)
    image_files = []
    
    # [FIX] ìˆ˜ì •ë¨: dataset_synthetic êµ¬ì¡° ìš°ì„  í™•ì¸
    dataset_synthetic_path = source_path / "dataset_synthetic"
    if dataset_synthetic_path.exists():
        # dataset_synthetic/images/train/{element_id}/ êµ¬ì¡°
        train_images_path = dataset_synthetic_path / "images" / "train"
        train_labels_path = dataset_synthetic_path / "labels"
        meta_path = dataset_synthetic_path / "meta"
        meta_e_path = dataset_synthetic_path / "meta-e"
        
        if train_images_path.exists():
            print(f"[DIR] dataset_synthetic êµ¬ì¡° ê°ì§€: {train_images_path}")
            for element_dir in train_images_path.iterdir():
                if element_dir.is_dir():
                    element_id = element_dir.name
                    images_dir = element_dir
                    labels_dir = train_labels_path / element_id
                    meta_dir = meta_path / element_id
                    meta_e_dir = meta_e_path / element_id
                    
                    for img_file in images_dir.glob("*.webp"):
                        json_file = meta_dir / img_file.with_suffix('.json').name if meta_dir.exists() else None
                        label_file = labels_dir / img_file.with_suffix('.txt').name if labels_dir.exists() else None
                        e2_json_file = meta_e_dir / f"{img_file.stem}_e2.json" if meta_e_dir.exists() else None
                        
                        if json_file and json_file.exists():
                            image_files.append({
                                'image': img_file,
                                'json': json_file,
                                'label': label_file if label_file and label_file.exists() else None,
                                'e2_json': e2_json_file if e2_json_file and e2_json_file.exists() else None
                            })
    
    # êµ¬ êµ¬ì¡° í´ë°±: output/synthetic/{element_id}/images/*.webp
    if len(image_files) == 0:
        print(f"[DIR] êµ¬ êµ¬ì¡° í™•ì¸: {source_path}")
        for element_dir in source_path.iterdir():
            if element_dir.is_dir() and not element_dir.name.startswith('.') and element_dir.name != 'dataset_synthetic':
                images_dir = element_dir / "images"
                labels_dir = element_dir / "labels"
                meta_dir = element_dir / "meta"
                meta_e_dir = element_dir / "meta-e"
                
                if images_dir.exists():
                    for img_file in images_dir.glob("*.webp"):
                        json_file = meta_dir / img_file.with_suffix('.json').name if meta_dir.exists() else None
                        label_file = labels_dir / img_file.with_suffix('.txt').name if labels_dir.exists() else None
                        e2_json_file = meta_e_dir / f"{img_file.stem}_e2.json" if meta_e_dir.exists() else None
                        
                        if json_file and json_file.exists():
                            image_files.append({
                                'image': img_file,
                                'json': json_file,
                                'label': label_file if label_file and label_file.exists() else None,
                                'e2_json': e2_json_file if e2_json_file and e2_json_file.exists() else None
                            })
                else:
                    # êµ¬êµ¬ì¡° í´ë°±: element_id/*.webp
                    for img_file in element_dir.glob("*.webp"):
                        json_file = img_file.with_suffix('.json')
                        if json_file.exists():
                            image_files.append({
                                'image': img_file,
                                'json': json_file,
                                'label': None,
                                'e2_json': None
                            })
    
    print(f"ì „ì²´ ì´ë¯¸ì§€ ìˆ˜: {len(image_files)}")
    
    # ì¦ë¶„ ì—…ë°ì´íŠ¸ ëª¨ë“œ: ê¸°ì¡´ íŒŒì¼ê³¼ ë¹„êµ (element_id í´ë” êµ¬ì¡° ê³ ë ¤)
    if target_path.exists() and not force_rebuild:
        existing_files = set()
        # [FIX] ìˆ˜ì •ë¨: element_id í´ë” êµ¬ì¡°ë¥¼ ê³ ë ¤í•˜ì—¬ ê¸°ì¡´ íŒŒì¼ ìˆ˜ì§‘
        for split in ['train', 'val']:
            split_path = target_path / "images" / split
            if split_path.exists():
                for element_dir in split_path.iterdir():
                    if element_dir.is_dir():
                        for img_file in element_dir.glob("*.webp"):
                            # element_idì™€ íŒŒì¼ëª…ì„ ì¡°í•©í•˜ì—¬ ê³ ìœ  í‚¤ ìƒì„±
                            element_id = element_dir.name
                            existing_files.add(f"{element_id}/{img_file.name}")
        
        # ìƒˆ íŒŒì¼ë§Œ í•„í„°ë§ (element_id/íŒŒì¼ëª… ì¡°í•©ìœ¼ë¡œ ë¹„êµ)
        new_image_files = []
        for file_dict in image_files:
            img_file = file_dict['image']
            img_path = Path(img_file)
            
            # element_id ì¶”ì¶œ (ìœ„ì™€ ë™ì¼í•œ ë¡œì§)
            parts = list(img_path.parts)
            element_id = None
            try:
                train_idx = parts.index('train')
                if train_idx + 1 < len(parts):
                    element_id = parts[train_idx + 1]
            except ValueError:
                try:
                    images_idx = parts.index('images')
                    if images_idx + 1 < len(parts):
                        element_id = parts[images_idx + 1]
                except ValueError:
                    if img_path.parent.name not in ('images', 'train', 'val'):
                        element_id = img_path.parent.name
                    elif img_path.parent.parent.name not in ('images', 'train', 'val', 'dataset_synthetic'):
                        element_id = img_path.parent.parent.name
            
            file_key = f"{element_id}/{img_file.name}" if element_id else img_file.name
            if file_key not in existing_files:
                new_image_files.append(file_dict)
        
        print(f"[RETRY] ì¦ë¶„ ì—…ë°ì´íŠ¸: ìƒˆ íŒŒì¼ {len(new_image_files)}ê°œ, ê¸°ì¡´ íŒŒì¼ {len(existing_files)}ê°œ")
        image_files = new_image_files
    
    if len(image_files) == 0:
        print("[ERROR] ì²˜ë¦¬í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ëœë¤ ì…”í”Œ
    random.shuffle(image_files)
    
    # Train/Val ë¶„í• 
    split_idx = int(len(image_files) * train_split)
    train_files = image_files[:split_idx]
    val_files = image_files[split_idx:]
    
    print(f"ğŸ“Š ë¶„í•  ê²°ê³¼:")
    print(f"  - Train: {len(train_files)}ê°œ ({len(train_files)/len(image_files)*100:.1f}%)")
    print(f"  - Val: {len(val_files)}ê°œ ({len(val_files)/len(image_files)*100:.1f}%)")
    print(f"  - ëª©í‘œ ë¹„ìœ¨: {train_split*100:.0f}%:{100-train_split*100:.0f}%")
    
    # íŒŒì¼ ë³µì‚¬ ë° ë³€í™˜ (element_id í´ë” êµ¬ì¡° ìœ ì§€)
    copied_images = 0
    copied_labels = 0
    copied_metadata = 0
    
    # [FIX] ìˆ˜ì •ë¨: element_idë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í´ë” êµ¬ì¡° ìœ ì§€
    files_by_element = {}
    for file_dict in image_files:
        img_file = file_dict['image']
        img_path = Path(img_file)
        element_id = None
        
        # element_id ì¶”ì¶œ: ê²½ë¡œ êµ¬ì¡° ë¶„ì„
        parts = list(img_path.parts)
        # dataset_synthetic/images/train/{element_id}/íŒŒì¼ëª….webp êµ¬ì¡° ì°¾ê¸°
        try:
            train_idx = parts.index('train')
            if train_idx + 1 < len(parts):
                element_id = parts[train_idx + 1]
        except ValueError:
            pass
        
        # trainì´ ì—†ìœ¼ë©´ images í´ë” ë‹¤ìŒ ì°¾ê¸°
        if not element_id:
            try:
                images_idx = parts.index('images')
                if images_idx + 1 < len(parts):
                    element_id = parts[images_idx + 1]
            except ValueError:
                pass
        
        # ì—¬ì „íˆ ì—†ìœ¼ë©´ ë¶€ëª¨ ë””ë ‰í† ë¦¬ ì´ë¦„ ì‚¬ìš©
        if not element_id:
            # dataset_synthetic êµ¬ì¡°ê°€ ì•„ë‹Œ ê²½ìš°: {element_id}/images/íŒŒì¼ëª….webp
            parent = img_path.parent
            if parent.name not in ('images', 'train', 'val'):
                element_id = parent.name
            elif parent.parent.name not in ('images', 'train', 'val', 'dataset_synthetic'):
                element_id = parent.parent.name
            else:
                # ìµœì¢… í´ë°±: íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
                element_id = img_file.stem.split('_')[0] if '_' in img_file.stem else 'unknown'
        
        if element_id not in files_by_element:
            files_by_element[element_id] = {'train': [], 'val': []}
        
        # train/val ë¶„ë¥˜
        if file_dict in train_files:
            files_by_element[element_id]['train'].append(file_dict)
        elif file_dict in val_files:
            files_by_element[element_id]['val'].append(file_dict)
    
    # element_idë³„ í´ë” êµ¬ì¡° ìœ ì§€í•˜ë©° íŒŒì¼ ì´ë™
    for element_id, splits in files_by_element.items():
        for split_name in ['train', 'val']:
            files = splits[split_name]
            if not files:
                continue
                
            # element_id í´ë” ìƒì„±
            element_img_dir = target_path / "images" / split_name / element_id
            # [FIX] ìˆ˜ì •ë¨: labelsëŠ” train/val êµ¬ë¶„ ì—†ì´ labels/{element_id}/ êµ¬ì¡°
            element_label_dir = target_path / "labels" / element_id
            
            element_img_dir.mkdir(parents=True, exist_ok=True)
            element_label_dir.mkdir(parents=True, exist_ok=True)
            
            for file_dict in files:
                img_file = file_dict['image']
                json_file = file_dict['json']
                label_file = file_dict.get('label')
                e2_json_file = file_dict.get('e2_json')
                
                # [FIX] ìˆ˜ì •ë¨: ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ train/valë¡œ ë¶„í• 
                img_dest = element_img_dir / img_file.name
                img_src_abs = Path(img_file).absolute()
                img_dest_abs = img_dest.absolute()
                
                if split_name == 'train':
                    # train íŒŒì¼ì€ ì´ë¯¸ dataset_synthetic/images/train/{element_id}/ì— ìˆìŒ
                    # sourceì™€ targetì´ ê°™ìœ¼ë¯€ë¡œ ë³µì‚¬ ë¶ˆí•„ìš” (ì´ë¯¸ ì˜¬ë°”ë¥¸ ìœ„ì¹˜)
                    if img_src_abs == img_dest_abs:
                        copied_images += 1  # ì´ë¯¸ ì˜¬ë°”ë¥¸ ìœ„ì¹˜
                    elif not img_dest.exists():
                        # ê²½ë¡œê°€ ë‹¤ë¥¸ ê²½ìš°ë§Œ ë³µì‚¬ (êµ¬ êµ¬ì¡°ì—ì„œ ìƒˆ êµ¬ì¡°ë¡œ ë³€í™˜ ì‹œ)
                        shutil.copy2(img_file, img_dest)
                        copied_images += 1
                    else:
                        copied_images += 1  # ì´ë¯¸ ì¡´ì¬
                else:  # val
                    # valì€ train í´ë”ì˜ ì›ë³¸ íŒŒì¼ì„ val í´ë”ë¡œ ë³µì‚¬ (ì›ë³¸ ìœ ì§€)
                    # ì›ë³¸: dataset_synthetic/images/train/{element_id}/íŒŒì¼.webp
                    # ë³µì‚¬: dataset_synthetic/images/val/{element_id}/íŒŒì¼.webp
                    if not img_dest.exists():
                        # sourceì™€ targetì´ ê°™ì€ ê²½ë¡œë©´ ì›ë³¸ì€ trainì— ìˆìŒ
                        if img_src_abs == (target_path / "images" / "train" / element_id / img_file.name).absolute():
                            # ê°™ì€ íŒŒì¼ì„ valë¡œ ë³µì‚¬ (ì›ë³¸ train ìœ ì§€)
                            shutil.copy2(img_file, img_dest)
                            copied_images += 1
                        else:
                            # ë‹¤ë¥¸ ê²½ë¡œì—ì„œ ë³µì‚¬
                            shutil.copy2(img_file, img_dest)
                            copied_images += 1
                    else:
                        copied_images += 1  # ì´ë¯¸ ì¡´ì¬
                
                # ë¼ë²¨ íŒŒì¼ ì²˜ë¦¬ (labels/{element_id}/ êµ¬ì¡°)
                label_dest = target_path / "labels" / element_id / img_file.with_suffix('.txt').name
                label_src = Path(label_file) if label_file and Path(label_file).exists() else None
                
                if split_name == 'train':
                    # train ë¼ë²¨ì€ ë³µì‚¬
                    if label_src and label_src.exists():
                        if not label_dest.exists():
                            shutil.copy2(str(label_src), str(label_dest))
                            copied_labels += 1
                    else:
                        # ì–´ë…¸í…Œì´ì…˜ ë³€í™˜
                        if not label_dest.exists():
                            yolo_label = convert_json_to_yolo(str(json_file))
                            label_dest.write_text(yolo_label, encoding='utf-8')
                            copied_labels += 1
                else:  # val
                    # val ë¼ë²¨ì€ sourceì—ì„œ ë³µì‚¬ (ì´ë™ ì•„ë‹˜)
                    if label_src and label_src.exists():
                        if not label_dest.exists():
                            shutil.copy2(str(label_src), str(label_dest))
                            copied_labels += 1
                    else:
                        # ì–´ë…¸í…Œì´ì…˜ ë³€í™˜
                        if not label_dest.exists():
                            yolo_label = convert_json_to_yolo(str(json_file))
                            label_dest.write_text(yolo_label, encoding='utf-8')
                            copied_labels += 1
                
                # ë©”íƒ€ë°ì´í„° ë³µì‚¬ (metadata í´ë”ëŠ” ê³µí†µ)
                metadata_dest = target_path / "metadata" / json_file.name
                if not metadata_dest.exists():
                    shutil.copy2(json_file, metadata_dest)
                    copied_metadata += 1
                
                # E2 JSON íŒŒì¼ë„ ë³µì‚¬
                if e2_json_file and Path(e2_json_file).exists():
                    e2_metadata_dest = target_path / "metadata" / Path(e2_json_file).name
                    if not e2_metadata_dest.exists():
                        shutil.copy2(e2_json_file, e2_metadata_dest)
                        copied_metadata += 1
    
    # data.yaml íŒŒì¼ ìƒì„±
    print("ğŸ“„ data.yaml íŒŒì¼ ìƒì„± ì¤‘...")
    data_yaml_content = f"""# BrickBox Synthetic Dataset Configuration
path: {target_path.absolute()}
train: images/train
val: images/val

# Classes
nc: 1  # number of classes
names: ['lego_part']  # class names

# Dataset info
total_images: {len(image_files)}
train_images: {len(train_files)}
val_images: {len(val_files)}
"""
    
    yaml_path = target_path / "data.yaml"
    yaml_path.write_text(data_yaml_content, encoding='utf-8')
    
    print("[OK] ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"ğŸ“Š ì¤€ë¹„ëœ íŒŒì¼: ì´ë¯¸ì§€ {copied_images}ê°œ, ë¼ë²¨ {copied_labels}ê°œ, ë©”íƒ€ë°ì´í„° {copied_metadata}ê°œ")
    print(f"[DIR] ì €ì¥ ìœ„ì¹˜: {target_path.absolute()}")
    
    # ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œì— ë“±ë¡
    try:
        version_manager = DatasetVersionManager(str(source_path.parent))
        version = version_manager.create_version(
            str(target_path), 
            description=f"ìë™ ìƒì„± - {len(image_files)}ê°œ ì´ë¯¸ì§€"
        )
        print(f"[REPORT] ë°ì´í„°ì…‹ ë²„ì „ {version} ìƒì„±ë¨")
    except Exception as e:
        print(f"[WARNING] ë²„ì „ ê´€ë¦¬ ì‹¤íŒ¨: {e}")
        # ë²„ì „ ê´€ë¦¬ëŠ” ì„ íƒì‚¬í•­ì´ë¯€ë¡œ ê³„ì† ì§„í–‰
    
    return {
        'images': copied_images,
        'labels': copied_labels,
        'metadata': copied_metadata,
        'train_files': len(train_files),
        'val_files': len(val_files)
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Prepare synthetic dataset for YOLO training')
    parser.add_argument('--source', default='output/synthetic', help='Source directory')
    parser.add_argument('--output', default='output/dataset_synthetic', help='Output directory')
    parser.add_argument('--train-split', type=float, default=0.8, help='Train split ratio')
    parser.add_argument('--force-rebuild', action='store_true', help='Force rebuild from scratch')
    
    args = parser.parse_args()
    
    try:
        result = prepare_dataset(
            source_dir=args.source,
            target_dir=args.output,
            train_split=args.train_split,
            force_rebuild=args.force_rebuild
        )
        print(f"[OK] ì„±ê³µ: {result}")
    except Exception as e:
        print(f"[ERROR] ì˜¤ë¥˜: {e}")
        sys.exit(1)