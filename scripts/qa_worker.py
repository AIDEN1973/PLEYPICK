#!/usr/bin/env python3 """ BrickBox QA Worker 심화 품질 검증, 중복 제거, 깊이/가림 검증 """ import os import sys import json import time import logging from datetime import datetime from typing import Dict, List, Optional, Tuple import numpy as np import cv2 from PIL import Image import imagehash # 로깅 설정 logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') logger = logging.getLogger(__name__) class QAWorker: """품질 검증 워커""" def __init__(self, supabase_client=None, config_path=None): self.supabase = supabase_client self.quality_thresholds = self._load_quality_thresholds(config_path) def _load_quality_thresholds(self, config_path=None): """품질 임계치 로드 (기본값: 기술문서 기준)""" # 기본값: 기술문서 최소 기준 default_thresholds = { 'ssim_min': 0.965, # 문서 기준: SSIM≥0.965 'snr_min': 30.0, # 문서 기준: SNR≥30 'sharpness_min': 0.7, # 기본값 'noise_max': 0.15, # 기본값 'contrast_min': 0.3, # 기본값 'reprojection_rms_max': 1.5, # 문서 기준: RMS≤1.5px 'depth_score_min': 0.85, # 문서 기준: DepthScore≥0.85 'brightness_min': 0.2, # 기본값 'brightness_max': 0.8, # 기본값 'color_saturation_min': 0.1 # 기본값 } # 환경변수에서 오버라이드 import os for key in default_thresholds: env_key = f"QA_{key.upper()}" if env_key in os.environ: try: default_thresholds[key] = float(os.environ[env_key]) except ValueError: logger.warning(f" 잘못된 환경변수 값: {env_key}") # 설정 파일에서 로드 if config_path and os.path.exists(config_path): try: with open(config_path, 'r') as f: config = json.load(f) if 'quality_thresholds' in config: default_thresholds.update(config['quality_thresholds']) logger.info(f" QA 임계치 로드: {config_path}") except Exception as e: logger.warning(f" 설정 파일 로드 실패: {e}") logger.info(f" QA 임계치 설정: {default_thresholds}") return default_thresholds def calculate_advanced_quality_metrics(self, image_path: str) -> Dict: """고급 품질 지표 계산""" try: if not os.path.exists(image_path): logger.error(f" 이미지 파일 없음: {image_path}") return {} # 이미지 로드 image = cv2.imread(image_path) if image is None: logger.error(f" 이미지 로드 실패: {image_path}") return {} # 그레이스케일 변환 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # SSIM 계산 ssim_score = self._calculate_ssim_advanced(gray) # SNR 계산 snr_score = self._calculate_snr_advanced(gray) # Sharpness 계산 sharpness_score = self._calculate_sharpness_advanced(gray) # Noise Level 계산 noise_level = self._calculate_noise_level_advanced(gray) # Contrast 계산 contrast_score = self._calculate_contrast_advanced(gray) # Brightness 계산 brightness_score = self._calculate_brightness_advanced(gray) # Color Saturation 계산 color_saturation = self._calculate_color_saturation(image) # Depth Score 계산 (메타데이터에서) depth_score = self._calculate_depth_score(image_path) # Reprojection RMS 계산 (메타데이터에서) reprojection_rms = self._calculate_reprojection_rms(image_path) metrics = { 'ssim': ssim_score, 'snr': snr_score, 'sharpness': sharpness_score, 'noise_level': noise_level, 'contrast': contrast_score, 'brightness': brightness_score, 'color_saturation': color_saturation, 'depth_score': depth_score, 'reprojection_rms': reprojection_rms, 'timestamp': datetime.now().isoformat() } logger.info(f" 품질 지표 계산 완료: {os.path.basename(image_path)}") return metrics except Exception as e: logger.error(f" 품질 지표 계산 실패: {e}") return {} def _calculate_color_saturation(self, image) -> float: """Color Saturation 계산""" try: # HSV 변환 hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) # S 채널 평균 saturation = np.mean(hsv[:, :, 1]) / 255.0 return float(saturation) except Exception as e: logger.error(f" Color Saturation 계산 실패: {e}") return 0.0 def _calculate_depth_score(self, image_path: str) -> float: """Depth Score 계산 (메타데이터에서)""" try: # 메타데이터 파일 경로 추정 metadata_path = image_path.replace('.webp', '.json').replace('.jpg', '.json') if os.path.exists(metadata_path): with open(metadata_path, 'r') as f: metadata = json.load(f) depth_score = metadata.get('depth_score', 0.85) # 기본값 return float(depth_score) return 0.85 # 기본값 except Exception as e: logger.error(f" Depth Score 계산 실패: {e}") return 0.85 def _calculate_reprojection_rms(self, image_path: str) -> float: """Reprojection RMS 계산 (메타데이터에서)""" try: # 메타데이터 파일 경로 추정 metadata_path = image_path.replace('.webp', '.json').replace('.jpg', '.json') if os.path.exists(metadata_path): with open(metadata_path, 'r') as f: metadata = json.load(f) rms = metadata.get('reprojection_rms', 1.0) # 기본값 return float(rms) return 1.0 # 기본값 except Exception as e: logger.error(f" Reprojection RMS 계산 실패: {e}") return 1.0 def _calculate_ssim_advanced(self, image: np.ndarray) -> float: """고급 SSIM 계산""" try: # Gaussian blur 적용 blurred = cv2.GaussianBlur(image, (11, 11), 1.5) # 구조적 유사성 계산 mu1 = cv2.GaussianBlur(image, (11, 11), 1.5) mu2 = cv2.GaussianBlur(blurred, (11, 11), 1.5) mu1_sq = mu1 * mu1 mu2_sq = mu2 * mu2 mu1_mu2 = mu1 * mu2 sigma1_sq = cv2.GaussianBlur(image * image, (11, 11), 1.5) - mu1_sq sigma2_sq = cv2.GaussianBlur(blurred * blurred, (11, 11), 1.5) - mu2_sq sigma12 = cv2.GaussianBlur(image * blurred, (11, 11), 1.5) - mu1_mu2 # SSIM 공식 적용 C1 = 0.01 ** 2 C2 = 0.03 ** 2 ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \ ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)) return float(np.mean(ssim_map)) except Exception as e: logger.error(f" SSIM 계산 실패: {e}") return 0.0 def _calculate_snr_advanced(self, image: np.ndarray) -> float: """고급 SNR 계산""" try: # 신호와 노이즈 분리 signal = cv2.GaussianBlur(image, (5, 5), 0) noise = image.astype(np.float32) - signal.astype(np.float32) # 신호와 노이즈의 표준편차 계산 signal_power = np.var(signal) noise_power = np.var(noise) if noise_power > 0: snr_db = 10 * np.log10(signal_power / noise_power) return float(snr_db) else: return 50.0 except Exception as e: logger.error(f" SNR 계산 실패: {e}") return 0.0 def _calculate_sharpness_advanced(self, image: np.ndarray) -> float: """고급 Sharpness 계산""" try: # Laplacian 필터 적용 laplacian = cv2.Laplacian(image, cv2.CV_64F) sharpness = laplacian.var() # 정규화 (0-1 범위) normalized_sharpness = min(sharpness / 1000.0, 1.0) return float(normalized_sharpness) except Exception as e: logger.error(f" Sharpness 계산 실패: {e}") return 0.0 def _calculate_noise_level_advanced(self, image: np.ndarray) -> float: """고급 Noise Level 계산""" try: # 강한 블러 적용 blurred = cv2.GaussianBlur(image, (15, 15), 3.0) # 노이즈 추정 (원본 - 블러) noise = np.abs(image.astype(np.float32) - blurred.astype(np.float32)) noise_level = np.mean(noise) / 255.0 return float(noise_level) except Exception as e: logger.error(f" Noise Level 계산 실패: {e}") return 0.0 def _calculate_contrast_advanced(self, image: np.ndarray) -> float: """고급 Contrast 계산""" try: # 표준편차로 대비 계산 contrast = np.std(image) / 255.0 return float(contrast) except Exception as e: logger.error(f" Contrast 계산 실패: {e}") return 0.0 def _calculate_brightness_advanced(self, image: np.ndarray) -> float: """고급 Brightness 계산""" try: # 평균 밝기 계산 brightness = np.mean(image) / 255.0 return float(brightness) except Exception as e: logger.error(f" Brightness 계산 실패: {e}") return 0.0 def calculate_image_hash(self, image_path: str) -> str: """이미지 해시 계산 (중복 검출용)""" try: if not os.path.exists(image_path): logger.error(f" 이미지 파일 없음: {image_path}") return None # PIL로 이미지 로드 image = Image.open(image_path) # 다양한 해시 계산 phash = imagehash.phash(image) dhash = imagehash.dhash(image) ahash = imagehash.average_hash(image) # 결합된 해시 combined_hash = f"{phash}{dhash}{ahash}" logger.info(f" 이미지 해시 계산: {os.path.basename(image_path)}") return combined_hash except Exception as e: logger.error(f" 이미지 해시 계산 실패: {e}") return None def detect_duplicates(self, image_path: str, existing_hashes: List[str]) -> Tuple[bool, str]: """중복 이미지 검출""" try: # 현재 이미지 해시 계산 current_hash = self.calculate_image_hash(image_path) if not current_hash: return False, None # 기존 해시와 비교 for existing_hash in existing_hashes: # 해시 유사도 계산 (간단한 문자열 거리) similarity = self._calculate_hash_similarity(current_hash, existing_hash) if similarity > 0.95: # 95% 이상 유사하면 중복 logger.warning(f" 중복 이미지 검출: {os.path.basename(image_path)}") return True, existing_hash return False, current_hash except Exception as e: logger.error(f" 중복 검출 실패: {e}") return False, None def _calculate_hash_similarity(self, hash1: str, hash2: str) -> float: """해시 유사도 계산""" try: if len(hash1) != len(hash2): return 0.0 # 해시 길이 length = len(hash1) # 일치하는 문자 수 계산 matches = sum(1 for a, b in zip(hash1, hash2) if a == b) # 유사도 계산 similarity = matches / length return similarity except Exception as e: logger.error(f" 해시 유사도 계산 실패: {e}") return 0.0 def validate_quality(self, metrics: Dict) -> Tuple[bool, List[str]]: """품질 검증""" try: issues = [] # SSIM 검증 if metrics.get('ssim', 0) < self.quality_thresholds['ssim_min']: issues.append(f"SSIM too low: {metrics.get('ssim', 0):.3f} < {self.quality_thresholds['ssim_min']}") # SNR 검증 if metrics.get('snr', 0) < self.quality_thresholds['snr_min']: issues.append(f"SNR too low: {metrics.get('snr', 0):.1f} < {self.quality_thresholds['snr_min']}") # Sharpness 검증 if metrics.get('sharpness', 0) < self.quality_thresholds['sharpness_min']: issues.append(f"Sharpness too low: {metrics.get('sharpness', 0):.3f} < {self.quality_thresholds['sharpness_min']}") # Noise 검증 if metrics.get('noise_level', 0) > self.quality_thresholds['noise_max']: issues.append(f"Noise too high: {metrics.get('noise_level', 0):.3f} > {self.quality_thresholds['noise_max']}") # Contrast 검증 if metrics.get('contrast', 0) < self.quality_thresholds['contrast_min']: issues.append(f"Contrast too low: {metrics.get('contrast', 0):.3f} < {self.quality_thresholds['contrast_min']}") # Brightness 검증 brightness = metrics.get('brightness', 1.0) if brightness < self.quality_thresholds['brightness_min'] or brightness > self.quality_thresholds['brightness_max']: issues.append(f"Brightness out of range: {brightness:.3f} (min: {self.quality_thresholds['brightness_min']}, max: {self.quality_thresholds['brightness_max']})") # Color Saturation 검증 if metrics.get('color_saturation', 0) < self.quality_thresholds['color_saturation_min']: issues.append(f"Color saturation too low: {metrics.get('color_saturation', 0):.3f} < {self.quality_thresholds['color_saturation_min']}") # Depth Score 검증 if metrics.get('depth_score', 1.0) < self.quality_thresholds['depth_score_min']: issues.append(f"Depth score too low: {metrics.get('depth_score', 0):.3f} < {self.quality_thresholds['depth_score_min']}") # Reprojection RMS 검증 if metrics.get('reprojection_rms', 0.0) > self.quality_thresholds['reprojection_rms_max']: issues.append(f"Reprojection RMS too high: {metrics.get('reprojection_rms', 0):.3f} > {self.quality_thresholds['reprojection_rms_max']}") is_valid = len(issues) == 0 if is_valid: logger.info(" 품질 검증 통과") else: logger.warning(f" 품질 검증 실패: {len(issues)}개 이슈") for issue in issues: logger.warning(f" - {issue}") return is_valid, issues except Exception as e: logger.error(f" 품질 검증 실패: {e}") return False, [str(e)] def process_qa_verification(self, image_path: str, part_id: str, frame_id: str = None) -> Dict: """QA 검증 처리 (세분화된 로깅)""" try: logger.info(f" QA 검증 시작: {part_id}") # frame_id 생성 (제공되지 않은 경우) if not frame_id: frame_id = f"{part_id}_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}" start_time = time.time() # 품질 지표 계산 metrics = self.calculate_advanced_quality_metrics(image_path) if not metrics: return {'status': 'failed', 'reason': 'metrics_calculation_failed'} # 품질 검증 is_valid, issues = self.validate_quality(metrics) # 중복 검출 (실제 구현에서는 DB에서 기존 해시 조회) is_duplicate, image_hash = self.detect_duplicates(image_path, []) # 처리 시간 계산 processing_time_ms = int((time.time() - start_time) * 1000) # 세분화된 QA 로그 생성 qa_log_entry = { 'frame_id': frame_id, 'part_id': part_id, 'image_id': f"{part_id}_{os.path.basename(image_path)}", 'timestamp': datetime.now().isoformat(), # 품질 지표 (세부) 'ssim_score': metrics.get('ssim', 0.0), 'snr_db': metrics.get('snr', 0.0), 'sharpness': metrics.get('sharpness', 0.0), 'noise_level': metrics.get('noise_level', 0.0), 'contrast': metrics.get('contrast', 0.0), 'brightness': metrics.get('brightness', 0.0), 'color_saturation': metrics.get('color_saturation', 0.0), # 3D 품질 지표 'reprojection_error_rms_px': metrics.get('reprojection_error_rms', 0.0), 'depth_score': metrics.get('depth_score', 0.0), 'occlusion_ratio': metrics.get('occlusion_ratio', 0.0), # 중복 검출 'phash': image_hash, 'is_duplicate': is_duplicate, 'duplicate_of': None, # 실제 구현에서는 원본 이미지 ID # 품질 검증 결과 'quality_valid': is_valid, 'quality_issues': issues, 'qa_status': 'PASS' if is_valid and not is_duplicate else 'FAIL', # 메타데이터 'image_path': image_path, 'metadata_path': image_path.replace('.webp', '.json'), 'processing_time_ms': processing_time_ms, 'worker_version': '1.0' } # qa_logs 테이블에 저장 if self.supabase: try: result = self.supabase.table('qa_logs').insert(qa_log_entry).execute() if result.data: logger.info(f" QA 로그 저장 완료: {frame_id}") else: logger.warning(f" QA 로그 저장 실패: {result}") except Exception as e: logger.error(f" QA 로그 저장 실패: {e}") # 결과 생성 (기존 호환성 유지) result = { 'part_id': part_id, 'image_path': image_path, 'frame_id': frame_id, 'timestamp': datetime.now().isoformat(), 'quality_metrics': metrics, 'quality_valid': is_valid, 'quality_issues': issues, 'is_duplicate': is_duplicate, 'image_hash': image_hash, 'qa_status': 'passed' if is_valid and not is_duplicate else 'failed', 'processing_time_ms': processing_time_ms } logger.info(f" QA 검증 완료: {part_id} - {result['qa_status']}") return result except Exception as e: logger.error(f" QA 검증 실패: {e}") return {'status': 'failed', 'reason': str(e)} def get_qa_statistics(self, part_id: str = None, days: int = 7) -> Dict: """QA 통계 조회""" try: if not self.supabase: return {} # 기본 쿼리 query = self.supabase.table('qa_logs').select('*') # part_id 필터 if part_id: query = query.eq('part_id', part_id) # 날짜 필터 if days > 0: from_date = datetime.now() - timedelta(days=days) query = query.gte('timestamp', from_date.isoformat()) result = query.execute() if not result.data: return {} # 통계 계산 logs = result.data total_logs = len(logs) passed_logs = len([log for log in logs if log.get('quality_valid', False)]) failed_logs = total_logs - passed_logs # 평균 지표 계산 avg_ssim = sum(log.get('ssim_score', 0) for log in logs) / total_logs if total_logs > 0 else 0 avg_snr = sum(log.get('snr_db', 0) for log in logs) / total_logs if total_logs > 0 else 0 avg_sharpness = sum(log.get('sharpness', 0) for log in logs) / total_logs if total_logs > 0 else 0 avg_noise = sum(log.get('noise_level', 0) for log in logs) / total_logs if total_logs > 0 else 0 # 중복 통계 duplicate_count = len([log for log in logs if log.get('is_duplicate', False)]) # 처리 시간 통계 processing_times = [log.get('processing_time_ms', 0) for log in logs if log.get('processing_time_ms')] avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0 statistics = { 'total_logs': total_logs, 'passed_logs': passed_logs, 'failed_logs': failed_logs, 'pass_rate': (passed_logs / total_logs * 100) if total_logs > 0 else 0, 'avg_ssim': round(avg_ssim, 4), 'avg_snr': round(avg_snr, 2), 'avg_sharpness': round(avg_sharpness, 4), 'avg_noise': round(avg_noise, 4), 'duplicate_count': duplicate_count, 'duplicate_rate': (duplicate_count / total_logs * 100) if total_logs > 0 else 0, 'avg_processing_time_ms': round(avg_processing_time, 2), 'period_days': days } logger.info(f" QA 통계 조회 완료: {total_logs}건") return statistics except Exception as e: logger.error(f" QA 통계 조회 실패: {e}") return {} def get_quality_trends(self, part_id: str, days: int = 30) -> List[Dict]: """품질 추세 조회""" try: if not self.supabase: return [] from_date = datetime.now() - timedelta(days=days) result = self.supabase.table('qa_logs')\ .select('timestamp, ssim_score, snr_db, sharpness, noise_level, quality_valid')\ .eq('part_id', part_id)\ .gte('timestamp', from_date.isoformat())\ .order('timestamp')\ .execute() trends = [] for log in result.data: trends.append({ 'timestamp': log.get('timestamp'), 'ssim_score': log.get('ssim_score', 0), 'snr_db': log.get('snr_db', 0), 'sharpness': log.get('sharpness', 0), 'noise_level': log.get('noise_level', 0), 'quality_valid': log.get('quality_valid', False) }) logger.info(f" 품질 추세 조회 완료: {part_id} - {len(trends)}건") return trends except Exception as e: logger.error(f" 품질 추세 조회 실패: {e}") return [] def get_duplicate_analysis(self, days: int = 7) -> Dict: """중복 분석""" try: if not self.supabase: return {} from_date = datetime.now() - timedelta(days=days) result = self.supabase.table('qa_logs')\ .select('phash, part_id, is_duplicate, duplicate_of')\ .gte('timestamp', from_date.isoformat())\ .execute() # 중복 통계 계산 phash_groups = {} for log in result.data: phash = log.get('phash') if phash: if phash not in phash_groups: phash_groups[phash] = [] phash_groups[phash].append(log) # 중복 그룹 분석 duplicate_groups = {phash: logs for phash, logs in phash_groups.items() if len(logs) > 1} total_duplicates = sum(len(logs) for logs in duplicate_groups.values()) unique_duplicates = len(duplicate_groups) analysis = { 'total_duplicates': total_duplicates, 'unique_duplicate_groups': unique_duplicates, 'duplicate_groups': duplicate_groups, 'period_days': days } logger.info(f" 중복 분석 완료: {unique_duplicates}개 그룹, {total_duplicates}개 중복") return analysis except Exception as e: logger.error(f" 중복 분석 실패: {e}") return {} def save_qa_results(self, results: List[Dict], output_path: str): """QA 결과 저장""" try: os.makedirs(os.path.dirname(output_path), exist_ok=True) with open(output_path, 'w', encoding='utf-8') as f: json.dump(results, f, ensure_ascii=False, indent=2) logger.info(f" QA 결과 저장: {output_path}") return True except Exception as e: logger.error(f" QA 결과 저장 실패: {e}") return False def main(): """메인 함수""" try: # QA 워커 초기화 qa_worker = QAWorker() # 테스트 이미지 경로 test_image_path = "test_image.webp" test_part_id = "test_part_001" # QA 검증 처리 result = qa_worker.process_qa_verification(test_image_path, test_part_id) # 결과 저장 output_path = "results/qa_results.json" qa_worker.save_qa_results([result], output_path) logger.info(" QA Worker 완료") return True except Exception as e: logger.error(f" QA 워커 실행 실패: {e}") return False if __name__ == "__main__": main() 