#!/usr/bin/env python3 """ BrickBox ë¡œì»¬ YOLO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ë¡œì»¬ PCì—ì„œ YOLO ëª¨ë¸ì„ í•™ìŠµí•˜ê³  Supabaseì— ì—…ë¡œë“œí•©ë‹ˆë‹¤. """ import os import sys import argparse import json import time import logging from datetime import datetime from pathlib import Path # YOLO ê´€ë ¨ import try: from ultralytics import YOLO import torch import torchvision YOLO_AVAILABLE = True except ImportError: YOLO_AVAILABLE = False print(" YOLO ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install ultralyticsë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.") # Supabase í´ë¼ì´ì–¸íŠ¸ try: from supabase import create_client, Client SUPABASE_AVAILABLE = True except ImportError: SUPABASE_AVAILABLE = False print(" Supabase í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install supabaseë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.") # ë¡œê¹… ì„¤ì • logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[ logging.FileHandler('training.log'), logging.StreamHandler() ] ) logger = logging.getLogger(__name__) class LocalYOLOTrainer: def __init__(self, supabase_url=None, supabase_key=None): self.supabase = None if SUPABASE_AVAILABLE and supabase_url and supabase_key: try: self.supabase = create_client(supabase_url, supabase_key) logger.info(" Supabase ì—°ê²° ì„±ê³µ") except Exception as e: logger.warning(f" Supabase ì—°ê²° ì‹¤íŒ¨: {e}") self.device = 'cuda' if torch.cuda.is_available() else 'cpu' logger.info(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}") def check_requirements(self): """í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ í™•ì¸""" if not YOLO_AVAILABLE: logger.error(" YOLO ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install ultralytics") return False if not SUPABASE_AVAILABLE: logger.warning(" Supabase í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ì—…ë¡œë“œê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.") return True def prepare_dataset(self, set_num): """ë°ì´í„°ì…‹ ì¤€ë¹„""" logger.info(f" ë°ì´í„°ì…‹ ì¤€ë¹„: {set_num}") # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • dataset_path = Path("data/synthetic") / set_num if not dataset_path.exists(): logger.error(f" ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dataset_path}") return None # data.yaml íŒŒì¼ ìƒì„± data_yaml = { 'path': str(dataset_path.absolute()), 'train': 'images/train', 'val': 'images/val', 'test': 'images/test', 'nc': 1, # í´ë˜ìŠ¤ ìˆ˜ 'names': ['lego_part'] # í´ë˜ìŠ¤ ì´ë¦„ } yaml_path = dataset_path / "data.yaml" with open(yaml_path, 'w') as f: import yaml yaml.dump(data_yaml, f) logger.info(f" ë°ì´í„°ì…‹ ì„¤ì • ì™„ë£Œ: {yaml_path}") return str(yaml_path) def train_model(self, dataset_path, config): """ëª¨ë¸ í•™ìŠµ""" logger.info(" YOLO ëª¨ë¸ í•™ìŠµ ì‹œì‘...") try: # YOLO ëª¨ë¸ ì´ˆê¸°í™” (YOLOv11n ì‚¬ìš© - detection) model = YOLO('yolo11n.pt') # í•™ìŠµ ì„¤ì • training_args = { 'data': dataset_path, 'epochs': config.get('epochs', 100), 'batch': config.get('batch_size', 16), 'imgsz': config.get('imgsz', 640), 'device': self.device, 'project': 'runs/train', 'name': f'lego_training_{config.get("set_num", "latest")}', 'save': True, 'save_period': 10, 'cache': True, 'workers': 4, 'patience': 15, 'lr0': 0.01, 'lrf': 0.1, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 7.5, 'cls': 0.5, 'dfl': 1.5, 'pose': 12.0, 'kobj': 2.0, 'label_smoothing': 0.0, 'nbs': 64, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.0, 'copy_paste': 0.0, 'auto_augment': 'randaugment', 'erasing': 0.4, 'crop_fraction': 1.0 } logger.info(f" í•™ìŠµ ì„¤ì •: {training_args}") # í•™ìŠµ ì‹œì‘ results = model.train(**training_args) logger.info(" ëª¨ë¸ í•™ìŠµ ì™„ë£Œ") return results except Exception as e: logger.error(f" ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}") raise def evaluate_model(self, model_path, dataset_path): """ëª¨ë¸ í‰ê°€""" logger.info(" ëª¨ë¸ í‰ê°€ ì¤‘...") try: model = YOLO(model_path) results = model.val(data=dataset_path) metrics = { 'mAP50': results.box.map50 if hasattr(results.box, 'map50') else 0.0, 'mAP50_95': results.box.map if hasattr(results.box, 'map') else 0.0, 'precision': results.box.mp if hasattr(results.box, 'mp') else 0.0, 'recall': results.box.mr if hasattr(results.box, 'mr') else 0.0, 'f1_score': 2 * (results.box.mp * results.box.mr) / (results.box.mp + results.box.mr) if (results.box.mp + results.box.mr) > 0 else 0.0 } logger.info(f" í‰ê°€ ê²°ê³¼: {metrics}") return metrics except Exception as e: logger.error(f" ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}") return None def upload_model(self, model_path, metrics, config): """í•™ìŠµëœ ëª¨ë¸ì„ Supabase Storageì— ì—…ë¡œë“œí•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ë“±ë¡""" if not self.supabase: logger.warning(" Supabase ì—°ê²°ì´ ì—†ì–´ ëª¨ë¸ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.") return None try: logger.info(" ëª¨ë¸ ì—…ë¡œë“œ ì¤‘...") # ëª¨ë¸ íŒŒì¼ í¬ê¸° ê³„ì‚° model_size = os.path.getsize(model_path) model_filename = os.path.basename(model_path) # 1. Supabase Storageì— ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ logger.info(f" ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {model_filename}") with open(model_path, 'rb') as f: model_data = f.read() # Storageì— ì—…ë¡œë“œ upload_result = self.supabase.storage.from_('models').upload( model_filename, model_data, file_options={ "content-type": "application/octet-stream", "upsert": True } ) if hasattr(upload_result, 'error') and upload_result.error: raise Exception(f"Storage ì—…ë¡œë“œ ì‹¤íŒ¨: {upload_result.error}") # ê³µê°œ URL ìƒì„± public_url = self.supabase.storage.from_('models').get_public_url(model_filename) logger.info(f" ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {public_url}") # 2. ë°ì´í„°ë² ì´ìŠ¤ì— ëª¨ë¸ ì •ë³´ ì €ì¥ model_info = { 'model_name': f'lego_model_{config.get("set_num", "latest")}', 'model_version': datetime.now().strftime('%Y%m%d_%H%M%S'), 'version': datetime.now().strftime('%Y%m%d_%H%M%S'), 'model_type': 'yolo_pytorch', 'model_path': public_url, 'model_url': public_url, 'pt_model_path': public_url, 'model_size': model_size, 'is_active': True, 'performance_metrics': { 'mAP50': metrics.get('mAP50', 0.0), 'mAP50_95': metrics.get('mAP50-95', 0.0), 'precision': metrics.get('precision', 0.0), 'recall': metrics.get('recall', 0.0), 'f1_score': metrics.get('f1', 0.0) }, 'training_metadata': { 'source': 'local_training', 'set_num': config.get('set_num', 'latest'), 'epochs': config.get('epochs', 100), 'batch_size': config.get('batch_size', 16), 'device': config.get('device', 'auto'), 'training_config': config, 'original_filename': model_filename }, 'created_at': datetime.now().isoformat(), 'updated_at': datetime.now().isoformat() } # ë°ì´í„°ë² ì´ìŠ¤ì— ëª¨ë¸ ì •ë³´ ì €ì¥ response = self.supabase.table('model_registry').insert(model_info).execute() if response.data: logger.info(f" ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_info['model_name']}") logger.info(f"ğŸ”— ëª¨ë¸ URL: {public_url}") return response.data[0] else: logger.error(" ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨") return None except Exception as e: logger.error(f" ëª¨ë¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}") return None def run_training(self, set_num, config): """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰""" logger.info(f" ë¡œì»¬ YOLO í•™ìŠµ ì‹œì‘: {set_num}") # ìš”êµ¬ì‚¬í•­ í™•ì¸ if not self.check_requirements(): return False try: # 1. ë°ì´í„°ì…‹ ì¤€ë¹„ dataset_path = self.prepare_dataset(set_num) if not dataset_path: return False # 2. ëª¨ë¸ í•™ìŠµ results = self.train_model(dataset_path, config) if not results: return False # 3. ëª¨ë¸ í‰ê°€ model_path = results.save_dir / "weights" / "best.pt" metrics = self.evaluate_model(str(model_path), dataset_path) if not metrics: return False # 4. ëª¨ë¸ ì—…ë¡œë“œ uploaded_model = self.upload_model(str(model_path), metrics, config) if uploaded_model: logger.info(" í•™ìŠµ ë° ì—…ë¡œë“œ ì™„ë£Œ!") return True else: logger.warning(" í•™ìŠµì€ ì™„ë£Œë˜ì—ˆì§€ë§Œ ì—…ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.") return True except Exception as e: logger.error(f" í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}") return False def main(): parser = argparse.ArgumentParser(description='BrickBox ë¡œì»¬ YOLO í•™ìŠµ') parser.add_argument('--set_num', default='latest', help='ë ˆê³  ì„¸íŠ¸ ë²ˆí˜¸') parser.add_argument('--epochs', type=int, default=100, help='í•™ìŠµ ì—í­ ìˆ˜') parser.add_argument('--batch_size', type=int, default=16, help='ë°°ì¹˜ í¬ê¸°') parser.add_argument('--imgsz', type=int, default=640, help='ì´ë¯¸ì§€ í¬ê¸°') parser.add_argument('--device', default='auto', help='ì‚¬ìš© ë””ë°”ì´ìŠ¤ (cuda/cpu/auto)') parser.add_argument('--supabase_url', help='Supabase URL') parser.add_argument('--supabase_key', help='Supabase Key') args = parser.parse_args() # ë””ë°”ì´ìŠ¤ ì„¤ì • if args.device == 'auto': device = 'cuda' if torch.cuda.is_available() else 'cpu' else: device = args.device # í•™ìŠµ ì„¤ì • config = { 'set_num': args.set_num, 'epochs': args.epochs, 'batch_size': args.batch_size, 'imgsz': args.imgsz, 'device': device } # Supabase ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°) supabase_url = args.supabase_url or os.getenv('VITE_SUPABASE_URL') supabase_key = args.supabase_key or os.getenv('VITE_SUPABASE_ANON_KEY') # í•™ìŠµ ì‹¤í–‰ trainer = LocalYOLOTrainer(supabase_url, supabase_key) success = trainer.run_training(args.set_num, config) if success: logger.info(" í•™ìŠµ ì™„ë£Œ!") sys.exit(0) else: logger.error(" í•™ìŠµ ì‹¤íŒ¨!") sys.exit(1) if __name__ == "__main__": main() 