#!/usr/bin/env python3
"""
BrickBox ë¡œì»¬ YOLO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ì›¹ UIì—ì„œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” YOLO ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime

# .env íŒŒì¼ ë¡œë“œ (ê°•í™”ëœ ë²„ì „)
try:
    from dotenv import load_dotenv
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ .env íŒŒì¼ ë¡œë“œ
    env_path = Path(__file__).parent.parent / '.env'
    if env_path.exists():
        load_dotenv(env_path, override=True)  # override=Trueë¡œ ê¸°ì¡´ í™˜ê²½ë³€ìˆ˜ ë®ì–´ì“°ê¸°
        print(f"[OK] .env íŒŒì¼ ë¡œë“œë¨: {env_path}")
        
        # í™˜ê²½ë³€ìˆ˜ ê²€ì¦
        required_vars = ['VITE_SUPABASE_URL', 'VITE_SUPABASE_SERVICE_ROLE']
        missing_vars = [var for var in required_vars if not os.getenv(var)]
        if missing_vars:
            print(f"[WARN] í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: {missing_vars}")
        else:
            print("[OK] í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ëª¨ë‘ ë¡œë“œë¨")
    else:
        print(f"[WARN] .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {env_path}")
        print("[INFO] ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©")
except ImportError:
    print("[WARN] python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install python-dotenv")
    print("[INFO] ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©")
except Exception as e:
    print(f"[WARN] .env íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("[INFO] ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©")

# Supabase í´ë¼ì´ì–¸íŠ¸
try:
    from supabase import create_client, Client
except ImportError:
    print("[ERROR] Supabase í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install supabase")
    sys.exit(1)

# YOLO ê´€ë ¨ ì„í¬íŠ¸
try:
    from ultralytics import YOLO
    import torch
except ImportError as e:
    print(f"[ERROR] í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install ultralytics torch")
    sys.exit(1)

def setup_supabase(override_url: str | None = None, override_key: str | None = None):
    """Supabase í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (Service Role ìš°ì„ )"""
    # í™˜ê²½ë³€ìˆ˜ ë””ë²„ê¹…
    print(f"[DEBUG] í™˜ê²½ë³€ìˆ˜ í™•ì¸:")
    print(f"  SUPABASE_URL: {os.getenv('SUPABASE_URL')}")
    print(f"  VITE_SUPABASE_URL: {os.getenv('VITE_SUPABASE_URL')}")
    print(f"  SUPABASE_SERVICE_ROLE_KEY: {os.getenv('SUPABASE_SERVICE_ROLE_KEY')}")
    print(f"  VITE_SUPABASE_SERVICE_ROLE: {os.getenv('VITE_SUPABASE_SERVICE_ROLE')}")
    
    url = (
        override_url
        or
        os.getenv('SUPABASE_URL')
        or os.getenv('VITE_SUPABASE_URL')
        or 'https://npferbxuxocbfnfbpcnz.supabase.co'
    )
    key = (
        override_key
        or
        os.getenv('SUPABASE_SERVICE_ROLE_KEY')
        or os.getenv('VITE_SUPABASE_SERVICE_ROLE')
        or os.getenv('SUPABASE_ANON_KEY')
        or os.getenv('VITE_SUPABASE_ANON_KEY')
        or 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im5wZmVyYnh1eG9jYmZuZmJwY256Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1OTQ3NDk4NSwiZXhwIjoyMDc1MDUwOTg1fQ.pPWhWrb4QBC-DT4dd6Y1p-LlHNd9UTKef3SHEXUDp00'
    )
    
    print(f"[OK] ì‚¬ìš©í•  URL: {url}")
    print(f"[OK] ì‚¬ìš©í•  Key: {key[:20]}...")
    
    return create_client(url, key)

def update_training_progress(supabase, job_id, epoch, total_epochs, metrics=None):
    """í•™ìŠµ ì§„í–‰ë¥ ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë°ì´íŠ¸"""
    try:
        progress_data = {
            'current_epoch': epoch,
            'total_epochs': total_epochs,
            'percent': round((epoch / total_epochs) * 100, 2),
            'status': 'training',
            'metrics': metrics or {}
        }
        
        # training_jobs í…Œì´ë¸” ì—…ë°ì´íŠ¸
        supabase.table('training_jobs').update({
            'progress': progress_data,
            'status': 'training',
            'updated_at': datetime.now().isoformat()
        }).eq('id', job_id).execute()
        
        print(f"[PROGRESS] ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: {epoch}/{total_epochs} ({progress_data['percent']}%)")
        
    except Exception as e:
        print(f"[WARN] ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    import logging
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path("output/training/logs")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    log_file = log_dir / f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger(__name__)

def update_training_status(job_id, status, progress=None, metrics=None):
    """í•™ìŠµ ìƒíƒœë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë°ì´íŠ¸"""
    try:
        import requests
        
        # Supabase API í˜¸ì¶œ (ì‹¤ì œ êµ¬í˜„ ì‹œ í™˜ê²½ë³€ìˆ˜ì—ì„œ URL ê°€ì ¸ì˜¤ê¸°)
        supabase_url = os.getenv('VITE_SUPABASE_URL', 'https://npferbxuxocbfnfbpcnz.supabase.co')
        supabase_key = os.getenv('VITE_SUPABASE_ANON_KEY', 'your-anon-key')
        
        # í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸
        update_data = {
            'status': status,
            'updated_at': datetime.now().isoformat()
        }
        
        if progress:
            update_data['progress'] = progress
            
        if metrics:
            update_data['metrics'] = metrics
        
        # ì‹¤ì œ êµ¬í˜„ ì‹œ Supabase REST API í˜¸ì¶œ
        print(f"[STATS] í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸: {status}")
        if progress:
            print(f"[PROGRESS] ì§„í–‰ë¥ : {progress}")
        if metrics:
            print(f"[STATS] ë©”íŠ¸ë¦­: {metrics}")
            
    except Exception as e:
        print(f"[WARN] ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def prepare_dataset(set_num, part_id=None):
    """ë°ì´í„°ì…‹ ì¤€ë¹„ - ì¤‘ë³µ ë¶€í’ˆ ì œê±° í¬í•¨"""
    print(f"[INFO] ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹œì‘: ì„¸íŠ¸ {set_num}, ë¶€í’ˆ {part_id}")

    # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •
    if set_num == 'latest':
        # ë¶€í’ˆ ë‹¨ìœ„ í•™ìŠµì˜ ê²½ìš° í•´ë‹¹ ë¶€í’ˆ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        if part_id:
            # ë¨¼ì € ì—˜ë¦¬ë¨¼íŠ¸ IDë¡œ ì‹œë„ (ë°ì´í„°ì…‹ì´ ì—˜ë¦¬ë¨¼íŠ¸ IDë¡œ ì €ì¥ë¨)
            dataset_path = Path(f"C:/cursor/brickbox/output/synthetic/{part_id}")
            
        # ì™„ë²½í•œ í´ë” êµ¬ì¡° í™•ì¸
        if dataset_path.exists():
            images_dir = dataset_path / 'images'
            labels_dir = dataset_path / 'labels'
            meta_dir = dataset_path / 'meta'
            meta_e_dir = dataset_path / 'meta-e'
            
            if not images_dir.exists() or not labels_dir.exists():
                print(f"[ERROR] ì™„ë²½í•œ í´ë” êµ¬ì¡°ê°€ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
                print(f"[INFO] í•„ìš”í•œ í´ë”: images/, labels/, meta/, meta-e/")
                dataset_path = None
            else:
                print(f"[OK] ì™„ë²½í•œ í´ë” êµ¬ì¡° í™•ì¸: {dataset_path}")
                print(f"  - images/: {images_dir.exists()}")
                print(f"  - labels/: {labels_dir.exists()}")
                print(f"  - meta/: {meta_dir.exists()}")
                print(f"  - meta-e/: {meta_e_dir.exists()}")
                
                # dataset.yaml ìƒì„± (ì™„ë²½í•œ í´ë” êµ¬ì¡°ìš©)
                dataset_yaml_path = dataset_path / 'dataset.yaml'
                if not dataset_yaml_path.exists():
                    print(f"[INFO] dataset.yaml ìƒì„±: {dataset_yaml_path}")
                    yaml_content = {
                        'path': str(dataset_path.absolute()),
                        'train': 'images/train',
                        'val': 'images/val', 
                        'test': 'images/test',
                        'nc': 1,
                        'names': ['lego_part']
                    }
                    with open(dataset_yaml_path, 'w', encoding='utf-8') as f:
                        import yaml
                        yaml.dump(yaml_content, f, default_flow_style=False, allow_unicode=True)
            
            # ì—˜ë¦¬ë¨¼íŠ¸ IDë¡œ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ part_idë¡œ ì‹œë„
            if not dataset_path or not dataset_path.exists():
                print(f"[WARN] ì—˜ë¦¬ë¨¼íŠ¸ ID {part_id} ë””ë ‰í† ë¦¬ê°€ ì—†ìŒ. part_idë¡œ ê²€ìƒ‰ ì¤‘...")
                
                # parts_masterì—ì„œ part_id ì¡°íšŒ (Service Role ì‚¬ìš©)
                try:
                    import requests
                    import os
                    
                    supabase_url = (
                        os.getenv('SUPABASE_URL')
                        or os.getenv('VITE_SUPABASE_URL')
                        or 'https://npferbxuxocbfnfbpcnz.supabase.co'
                    )
                    supabase_key = (
                        os.getenv('SUPABASE_SERVICE_ROLE_KEY')
                        or os.getenv('VITE_SUPABASE_SERVICE_ROLE')
                        or os.getenv('SUPABASE_ANON_KEY')
                        or os.getenv('VITE_SUPABASE_ANON_KEY')
                    )
                    
                    response = requests.get(
                        f"{supabase_url}/rest/v1/parts_master",
                                    headers={
                                        "apikey": supabase_key,
                                        "Authorization": f"Bearer {supabase_key}",
                                        "Content-Type": "application/json"
                                    },
                        params={
                            "element_id": f"eq.{part_id}",
                            "select": "part_id",
                            "limit": 1
                        }
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        if data and len(data) > 0:
                            actual_part_id = data[0].get('part_id')
                            if actual_part_id:
                                dataset_path = Path(f"C:/cursor/brickbox/output/synthetic/{actual_part_id}")
                                print(f"[CONVERT] part_id {actual_part_id} ë””ë ‰í† ë¦¬ë¡œ ì‹œë„: {dataset_path}")
                except Exception as e:
                    print(f"[WARN] part_id ì¡°íšŒ ì‹¤íŒ¨: {e}")
        else:
            dataset_path = Path("C:/cursor/brickbox/output/synthetic/prepared")
    else:
        dataset_path = Path(f"C:/cursor/brickbox/output/synthetic/set_{set_num}")

    if not dataset_path.exists():
        print(f"[ERROR] ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dataset_path}")
        return None
    
    # dataset.yaml íŒŒì¼ í™•ì¸
    yaml_file = dataset_path / "dataset.yaml"
    if not yaml_file.exists():
        print(f"[ERROR] dataset.yaml íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {yaml_file}")
        return None
    
    # ì¤‘ë³µ ë¶€í’ˆ ì œê±° ì²˜ë¦¬
    if set_num != 'latest':
        filtered_yaml = remove_duplicate_parts(yaml_file, set_num, part_id)
        if filtered_yaml:
            yaml_file = filtered_yaml
    
    print(f"[OK] ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {dataset_path}")
    return str(yaml_file)

def remove_duplicate_parts(yaml_file, set_num, part_id=None):
    """ì´ë¯¸ í•™ìŠµëœ ë¶€í’ˆì„ ì œê±°í•˜ì—¬ ì¤‘ë³µ í•™ìŠµ ë°©ì§€"""
    try:
        import yaml
        from supabase import create_client
        import os
        
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        supabase_url = os.getenv('SUPABASE_URL') or os.getenv('VITE_SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_ANON_KEY') or os.getenv('VITE_SUPABASE_ANON_KEY')
        
        if not supabase_url or not supabase_key:
            print("[WARN] Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ì¤‘ë³µ ì œê±° ìŠ¤í‚µ")
            print("[TIP] SUPABASE_URL, SUPABASE_ANON_KEY ë˜ëŠ” VITE_SUPABASE_URL, VITE_SUPABASE_ANON_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”")
            return None
            
        supabase = create_client(supabase_url, supabase_key)
        
        # dataset.yaml ì½ê¸°
        with open(yaml_file, 'r', encoding='utf-8') as f:
            dataset_config = yaml.safe_load(f)
        
        # ì„¸íŠ¸ ë¶€í’ˆ ëª©ë¡ ì¡°íšŒ
        if part_id:
            # ë¶€í’ˆ ë‹¨ìœ„ í•™ìŠµ: íŠ¹ì • ë¶€í’ˆë§Œ í•™ìŠµ
            target_parts = [part_id]
            print(f"[PART] ë¶€í’ˆ ë‹¨ìœ„ í•™ìŠµ: {part_id}")
        else:
            # ì„¸íŠ¸ ë‹¨ìœ„ í•™ìŠµ: ì„¸íŠ¸ ë‚´ ëª¨ë“  ë¶€í’ˆ
            try:
                # ì„¸íŠ¸ ì •ë³´ ì¡°íšŒ
                set_response = supabase.table('lego_sets').select('id').eq('set_num', set_num).single().execute()
                if not set_response.data:
                    print(f"[ERROR] ì„¸íŠ¸ {set_num}ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return None
                
                # ì„¸íŠ¸ ë¶€í’ˆ ì¡°íšŒ
                parts_response = supabase.table('set_parts').select('part_id').eq('set_id', set_response.data['id']).execute()
                target_parts = [p['part_id'] for p in parts_response.data] if parts_response.data else []
                print(f"[SET] ì„¸íŠ¸ {set_num} ë¶€í’ˆ: {len(target_parts)}ê°œ")
            except Exception as e:
                print(f"[WARN] ì„¸íŠ¸ ë¶€í’ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                return None
        
        if not target_parts:
            print("[ERROR] í•™ìŠµí•  ë¶€í’ˆì´ ì—†ìŒ")
            return None
        
        # ì´ë¯¸ í•™ìŠµëœ ë¶€í’ˆ í™•ì¸
        try:
            # ì—¬ëŸ¬ ë¶€í’ˆ IDë¥¼ í•œ ë²ˆì— ì¡°íšŒ (ì„±ëŠ¥ ìµœì í™”)
            trained_parts = []
            try:
                # Supabase Python í´ë¼ì´ì–¸íŠ¸ì—ì„œ in_() ë©”ì„œë“œ ì‚¬ìš©
                response = supabase.table('part_training_status').select('part_id').in_('part_id', target_parts).eq('status', 'completed').execute()
                if response.data:
                    trained_parts = [p['part_id'] for p in response.data]
                    print(f"[SEARCH] ì¼ê´„ ì¡°íšŒë¡œ {len(trained_parts)}ê°œ ë¶€í’ˆ ìƒíƒœ í™•ì¸")
            except Exception as e:
                print(f"[WARN] ì¼ê´„ ì¡°íšŒ ì‹¤íŒ¨, ê°œë³„ ì¡°íšŒë¡œ ì „í™˜: {e}")
                # ì¼ê´„ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ê°œë³„ ì¡°íšŒë¡œ fallback
                for part_id in target_parts:
                    try:
                        response = supabase.table('part_training_status').select('part_id').eq('part_id', part_id).eq('status', 'completed').execute()
                        if response.data:
                            trained_parts.extend([p['part_id'] for p in response.data])
                    except Exception as e:
                        print(f"[WARN] ë¶€í’ˆ {part_id} ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                        continue
            
            if trained_parts:
                print(f"[SKIP] ì´ë¯¸ í•™ìŠµëœ ë¶€í’ˆ {len(trained_parts)}ê°œ ìŠ¤í‚µ: {trained_parts[:5]}{'...' if len(trained_parts) > 5 else ''}")
                
                # ìƒˆë¡œ í•™ìŠµí•  ë¶€í’ˆë§Œ í•„í„°ë§
                new_parts = [p for p in target_parts if p not in trained_parts]
                
                if not new_parts:
                    print("[ERROR] ëª¨ë“  ë¶€í’ˆì´ ì´ë¯¸ í•™ìŠµë¨ - í•™ìŠµ ì¤‘ë‹¨")
                    return None
                
                print(f"[OK] ìƒˆë¡œ í•™ìŠµí•  ë¶€í’ˆ {len(new_parts)}ê°œ: {new_parts[:5]}{'...' if len(new_parts) > 5 else ''}")
                
                # í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„±
                filtered_yaml = create_filtered_dataset(yaml_file, new_parts)
                return filtered_yaml
            else:
                print("[OK] ëª¨ë“  ë¶€í’ˆì´ ìƒˆë¡œ í•™ìŠµ ëŒ€ìƒ")
                return None
                
        except Exception as e:
            print(f"[WARN] ì¤‘ë³µ ë¶€í’ˆ í™•ì¸ ì‹¤íŒ¨: {e}")
            return None
            
    except Exception as e:
        print(f"[ERROR] ì¤‘ë³µ ë¶€í’ˆ ì œê±° ì‹¤íŒ¨: {e}")
        return None

def create_filtered_dataset(original_yaml, target_parts):
    """íŠ¹ì • ë¶€í’ˆë§Œ í¬í•¨í•˜ëŠ” í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„±"""
    try:
        import yaml
        import shutil
        from pathlib import Path
        import json
        
        print(f"[SEARCH] í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘: {len(target_parts)}ê°œ ë¶€í’ˆ")
        print(f"[SET] ëŒ€ìƒ ë¶€í’ˆ: {target_parts}")
        
        # ì›ë³¸ íŒŒì¼ ë°±ì—…
        backup_yaml = original_yaml.replace('.yaml', '_backup.yaml')
        shutil.copy2(original_yaml, backup_yaml)
        print(f"[SAVE] ì›ë³¸ ë°±ì—…: {backup_yaml}")
        
        # dataset.yaml ì½ê¸°
        with open(original_yaml, 'r', encoding='utf-8') as f:
            dataset_config = yaml.safe_load(f)
        
        # YOLO ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
        train_path = Path(dataset_config.get('train', ''))
        val_path = Path(dataset_config.get('val', ''))
        test_path = Path(dataset_config.get('test', ''))
        
        print(f"ğŸ“ ë°ì´í„°ì…‹ ê²½ë¡œ:")
        print(f"  - Train: {train_path}")
        print(f"  - Val: {val_path}")
        print(f"  - Test: {test_path}")
        
        # ë¶€í’ˆë³„ í´ë˜ìŠ¤ ID ë§¤í•‘ ìƒì„±
        part_to_class_id = {part_id: idx for idx, part_id in enumerate(target_parts)}
        print(f"[LABEL] í´ë˜ìŠ¤ ë§¤í•‘: {part_to_class_id}")
        
        # í•„í„°ë§ëœ íŒŒì¼ë“¤ ìˆ˜ì§‘
        filtered_files = {'train': [], 'val': [], 'test': []}
        total_images = 0
        
        for split_name, split_path in [('train', train_path), ('val', val_path), ('test', test_path)]:
            if not split_path.exists():
                print(f"[WARN] {split_name} ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {split_path}")
                continue
                
            print(f"[SEARCH] {split_name} í´ë” ìŠ¤ìº” ì¤‘...")
            split_images = []
            
            # ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸° (webp í˜•ì‹)
            for image_file in split_path.glob('*.webp'):
                # ë¶€í’ˆ IDê°€ íŒŒì¼ëª…ì— í¬í•¨ëœ ì´ë¯¸ì§€ë§Œ í•„í„°ë§
                # 1. element_id íŒ¨í„´: {element_id}_{sequence}.webp
                # 2. part_id íŒ¨í„´: {part_id}_{sequence}.webp (ì—˜ë¦¬ë¨¼íŠ¸ IDê°€ ì—†ëŠ” ê²½ìš°)
                for part_id in target_parts:
                    # íŒŒì¼ëª…ì´ í•´ë‹¹ ë¶€í’ˆìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                    if image_file.name.startswith(f"{part_id}_"):
                        # ë¼ë²¨ íŒŒì¼ ê²½ë¡œ (labels í´ë”ì—ì„œ)
                        label_path = split_path.parent.parent / "labels" / split_path.name / image_file.with_suffix('.txt').name
                        if label_path.exists():
                            # ë¼ë²¨ íŒŒì¼ì—ì„œ í•´ë‹¹ ë¶€í’ˆì˜ ë¼ë²¨ë§Œ ì¶”ì¶œ
                            filtered_labels = filter_labels_for_parts(label_path, target_parts, part_to_class_id)
                            if filtered_labels:
                                split_images.append(str(image_file))
                                total_images += 1
                                print(f"  [OK] {image_file.name} (ë¶€í’ˆ: {part_id})")
                        break
            
            filtered_files[split_name] = split_images
            print(f"[STATS] {split_name}: {len(split_images)}ê°œ ì´ë¯¸ì§€")
        
        if total_images == 0:
            print("[ERROR] í•„í„°ë§ëœ ì´ë¯¸ì§€ê°€ ì—†ìŒ")
            return None
        
        # í•„í„°ë§ëœ dataset.yaml ìƒì„±
        filtered_config = dataset_config.copy()
        filtered_config['nc'] = len(target_parts)  # í´ë˜ìŠ¤ ìˆ˜
        filtered_config['names'] = target_parts     # í´ë˜ìŠ¤ ì´ë¦„
        
        # í•„í„°ë§ëœ íŒŒì¼ì„ ì„ì‹œ í´ë”ì— ë³µì‚¬
        temp_dir = Path(original_yaml).parent / "filtered_dataset"
        temp_dir.mkdir(exist_ok=True)
        
        # train/val/test í´ë” ìƒì„±
        for split_name in ['train', 'val', 'test']:
            split_dir = temp_dir / split_name
            split_dir.mkdir(exist_ok=True)
            
            for image_path in filtered_files[split_name]:
                src_path = Path(image_path)
                dst_path = split_dir / src_path.name
                shutil.copy2(src_path, dst_path)
                
                # ë¼ë²¨ íŒŒì¼ë„ ë³µì‚¬ (labels í´ë”ì—ì„œ)
                label_src = Path(image_path).parent.parent / "labels" / Path(image_path).parent.name / Path(image_path).with_suffix('.txt').name
                label_dst = split_dir / Path(image_path).with_suffix('.txt').name
                if label_src.exists():
                    # í•„í„°ë§ëœ ë¼ë²¨ ë‚´ìš©ìœ¼ë¡œ ìƒˆ íŒŒì¼ ìƒì„±
                    filtered_labels = filter_labels_for_parts(label_src, target_parts, part_to_class_id)
                    if filtered_labels:
                        with open(label_dst, 'w', encoding='utf-8') as f:
                            for line in filtered_labels:
                                f.write(line + '\n')
        
        # í•„í„°ë§ëœ dataset.yaml ì €ì¥
        temp_yaml = temp_dir / "dataset.yaml"
        with open(temp_yaml, 'w', encoding='utf-8') as f:
            yaml.dump(filtered_config, f, default_flow_style=False)
        
        print(f"[OK] í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {total_images}ê°œ ì´ë¯¸ì§€")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {temp_yaml}")
        return str(temp_yaml)
        
    except Exception as e:
        print(f"[ERROR] í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def filter_labels_for_parts(label_file, target_parts, part_to_class_id):
    """ë¼ë²¨ íŒŒì¼ì—ì„œ íŠ¹ì • ë¶€í’ˆì˜ ë¼ë²¨ë§Œ í•„í„°ë§"""
    try:
        filtered_lines = []
        
        # íŒŒì¼ëª…ì—ì„œ ë¶€í’ˆ ID ì¶”ì¶œ
        file_name = Path(label_file).stem
        print(f"[SEARCH] ë¼ë²¨ íŒŒì¼ ì²˜ë¦¬: {file_name}")
        
        with open(label_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                    
                parts = line.split()
                if len(parts) < 5:
                    continue
                
                # í˜„ì¬ ëª¨ë“  ë¼ë²¨ì´ í´ë˜ìŠ¤ 0ìœ¼ë¡œ í†µí•©ë˜ì–´ ìˆìŒ
                # íŒŒì¼ëª…ì´ target_partsì— í¬í•¨ëœ ë¶€í’ˆìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                for part_id, new_class_id in part_to_class_id.items():
                    if file_name.startswith(f"{part_id}_"):
                        # ìƒˆë¡œìš´ í´ë˜ìŠ¤ IDë¡œ ë³€ê²½
                        parts[0] = str(new_class_id)
                        filtered_lines.append(' '.join(parts))
                        print(f"  [OK] ë¼ë²¨ ë³€í™˜: í´ë˜ìŠ¤ 0 â†’ {new_class_id} (ë¶€í’ˆ: {part_id})")
                        break
        
        return filtered_lines
        
    except Exception as e:
        print(f"[WARN] ë¼ë²¨ í•„í„°ë§ ì‹¤íŒ¨ {label_file}: {e}")
        return []

def update_part_training_status(set_num, part_id, metrics):
    """í•™ìŠµ ì™„ë£Œëœ ë¶€í’ˆë“¤ì˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ì¤‘ë³µ í•™ìŠµ ë°©ì§€"""
    try:
        from supabase import create_client
        import os
        
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Service Role ìš°ì„ )
        supabase_url = os.getenv('SUPABASE_URL') or os.getenv('VITE_SUPABASE_URL')
        supabase_key = (
            os.getenv('SUPABASE_SERVICE_ROLE_KEY')
            or os.getenv('VITE_SUPABASE_SERVICE_ROLE')
            or os.getenv('SUPABASE_ANON_KEY')
            or os.getenv('VITE_SUPABASE_ANON_KEY')
        )
        
        if not supabase_url or not supabase_key:
            print("[WARN] Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ë¶€í’ˆ ìƒíƒœ ì—…ë°ì´íŠ¸ ìŠ¤í‚µ")
            print("[TIP] SUPABASE_URL, SUPABASE_ANON_KEY ë˜ëŠ” VITE_SUPABASE_URL, VITE_SUPABASE_ANON_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”")
            return
            
        supabase = create_client(supabase_url, supabase_key)
        
        if part_id:
            # ë¶€í’ˆ ë‹¨ìœ„ í•™ìŠµ: element_id ì…ë ¥ ê°€ëŠ¥ â†’ part_id ë§¤í•‘
            try:
                pm = create_client(supabase_url, supabase_key)
                # element_id â†’ part_id ë§¤í•‘ ì‹œë„
                resp = pm.table('parts_master').select('part_id').eq('element_id', part_id).limit(1).execute()
                if resp.data and len(resp.data) > 0 and resp.data[0].get('part_id'):
                    actual_part_id = resp.data[0]['part_id']
                    print(f"[CONVERT] element_id {part_id} â†’ part_id {actual_part_id}")
                    part_id = actual_part_id
            except Exception as e:
                print(f"[WARN] element_idâ†’part_id ë§¤í•‘ ì‹¤íŒ¨(ë¬´ì‹œ): {e}")
            # ìµœì¢… part_idë¡œ ì—…ë°ì´íŠ¸
            trained_parts = [part_id]
            print(f"[PART] ë¶€í’ˆ ë‹¨ìœ„ í•™ìŠµ ì™„ë£Œ: {part_id}")
        else:
            # ì„¸íŠ¸ ë‹¨ìœ„ í•™ìŠµ: ì„¸íŠ¸ ë‚´ ëª¨ë“  ë¶€í’ˆ ì—…ë°ì´íŠ¸
            try:
                # ì„¸íŠ¸ ì •ë³´ ì¡°íšŒ
                set_response = supabase.table('lego_sets').select('id').eq('set_num', set_num).single().execute()
                if not set_response.data:
                    print(f"[ERROR] ì„¸íŠ¸ {set_num}ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return
                
                # ì„¸íŠ¸ ë¶€í’ˆ ì¡°íšŒ
                parts_response = supabase.table('set_parts').select('part_id').eq('set_id', set_response.data['id']).execute()
                trained_parts = [p['part_id'] for p in parts_response.data] if parts_response.data else []
                print(f"[SET] ì„¸íŠ¸ ë‹¨ìœ„ í•™ìŠµ ì™„ë£Œ: {len(trained_parts)}ê°œ ë¶€í’ˆ")
            except Exception as e:
                print(f"[WARN] ì„¸íŠ¸ ë¶€í’ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                return
        
        # ê° ë¶€í’ˆì˜ í•™ìŠµ ìƒíƒœë¥¼ 'completed'ë¡œ ì—…ë°ì´íŠ¸ (upsert ì‚¬ìš©)
        for part_id in trained_parts:
            try:
                # ğŸ”§ ìˆ˜ì •ë¨: upsert ì‚¬ìš© (map50_95, f1_score ì œê±° - ì»¬ëŸ¼ ì—†ìŒ)
                result = supabase.table('part_training_status').upsert({
                    'part_id': str(part_id),
                    'status': 'completed',
                    'map50': float(metrics.get('mAP50', 0.0)),
                    'precision': float(metrics.get('precision', 0.0)),
                    'recall': float(metrics.get('recall', 0.0)),
                    'last_trained_at': datetime.now().isoformat(),
                    'updated_at': datetime.now().isoformat()
                }, on_conflict='part_id').execute()
                
                # Supabase ì‘ë‹µ ì²˜ë¦¬ (ì˜¬ë°”ë¥¸ ë°©ì‹)
                if hasattr(result, 'error') and result.error:
                    print(f"[WARN] ë¶€í’ˆ {part_id} ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {result.error}")
                else:
                    print(f"[OK] ë¶€í’ˆ {part_id} í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            except Exception as e:
                print(f"[WARN] ë¶€í’ˆ {part_id} ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        
        print(f"[OK] ì´ {len(trained_parts)}ê°œ ë¶€í’ˆ í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"[ERROR] ë¶€í’ˆ í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def train_hybrid_models(dataset_yaml, config, job_id=None):
    """í•˜ì´ë¸Œë¦¬ë“œ YOLO ëª¨ë¸ í•™ìŠµ (1ë‹¨ê³„ + 2ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰)"""
    print("[HYBRID] í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ ì‹œì‘: 1ë‹¨ê³„ + 2ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰")
    
    # Supabase í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    supabase = None
    if job_id:
        try:
            supabase = setup_supabase()
            print(f"ğŸ“¡ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ë¨ (ì‘ì—… ID: {job_id})")
        except Exception as e:
            print(f"[WARN] Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
    
    results = {}
    
    try:
        # 1ë‹¨ê³„ í•™ìŠµ (YOLO11n-seg)
        print("\n" + "="*60)
        print("ğŸ¯ 1ë‹¨ê³„ í•™ìŠµ ì‹œì‘: YOLO11n-seg (ë¹ ë¥¸ ìŠ¤ìº”)")
        print("="*60)
        
        stage1_config = config.copy()
        stage1_config['model_stage'] = 'stage1'
        stage1_results, stage1_model = train_yolo_model(dataset_yaml, stage1_config, job_id)
        results['stage1'] = stage1_results
        
        print(f"\nâœ… 1ë‹¨ê³„ í•™ìŠµ ì™„ë£Œ: {stage1_results}")
        
        # 2ë‹¨ê³„ í•™ìŠµ (YOLO11s-seg)
        print("\n" + "="*60)
        print("ğŸ¯ 2ë‹¨ê³„ í•™ìŠµ ì‹œì‘: YOLO11s-seg (ì •ë°€ ê²€ì¦)")
        print("="*60)
        
        stage2_config = config.copy()
        stage2_config['model_stage'] = 'stage2'
        stage2_results, stage2_model = train_yolo_model(dataset_yaml, stage2_config, job_id)
        results['stage2'] = stage2_results
        
        print(f"\nâœ… 2ë‹¨ê³„ í•™ìŠµ ì™„ë£Œ: {stage2_results}")
        
        # í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        if supabase and job_id:
            try:
                supabase.table('training_jobs').update({
                    'status': 'completed',
                    'progress': {
                        'current_epoch': config.get('epochs', 100),
                        'total_epochs': config.get('epochs', 100),
                        'percent': 100,
                        'status': 'completed',
                        'stage1_completed': True,
                        'stage2_completed': True
                    },
                    'completed_at': datetime.now().isoformat(),
                    'updated_at': datetime.now().isoformat()
                }).eq('id', job_id).execute()
                print("[OK] í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸ë¨")
            except Exception as e:
                print(f"[WARN] í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ
        print("\n" + "="*60)
        print("ğŸ’¾ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ ì¤‘...")
        print("="*60)
        
        # Stage 1 ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ
        if stage1_model and hasattr(stage1_model, 'save'):
            print("[STAGE1] Stage 1 ëª¨ë¸ ì €ì¥ ì¤‘...")
            stage1_path = Path("public/models/lego_yolo_stage1_latest.pt")
            stage1_model.save(stage1_path)
            print(f"[OK] Stage 1 ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {stage1_path}")
            
            # Stage 1 ONNX ë³€í™˜ // ğŸ”§ ìˆ˜ì •ë¨
            stage1_onnx_path = None
            try:
                stage1_onnx_path = Path("public/models/lego_yolo_stage1_latest.onnx")
                stage1_model.export(format='onnx', imgsz=config.get('imgsz', 640))
                exported_path = Path("public/models/yolo11n.onnx")
                if exported_path.exists():
                    exported_path.rename(stage1_onnx_path)
                    print(f"[OK] Stage 1 ONNX ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {stage1_onnx_path}")
                else:
                    raise FileNotFoundError("ONNX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            except Exception as onnx_error:
                print(f"[WARN] Stage 1 ONNX ë³€í™˜ ì‹¤íŒ¨: {onnx_error}")
                stage1_onnx_path = None
            
            # Stage 1 ëª¨ë¸ ì—…ë¡œë“œ
            try:
                stage1_cfg = config.copy()
                stage1_cfg['model_stage'] = 'stage1'
                upload_and_register_model(str(stage1_path), stage1_onnx_path, stage1_cfg)
            except Exception as e:
                print(f"[WARN] Stage 1 ëª¨ë¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # Stage 2 ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ
        if stage2_model and hasattr(stage2_model, 'save'):
            print("[STAGE2] Stage 2 ëª¨ë¸ ì €ì¥ ì¤‘...")
            stage2_path = Path("public/models/lego_yolo_stage2_latest.pt")
            stage2_model.save(stage2_path)
            print(f"[OK] Stage 2 ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {stage2_path}")
            
            # Stage 2 ONNX ë³€í™˜ // ğŸ”§ ìˆ˜ì •ë¨
            stage2_onnx_path = None
            try:
                stage2_onnx_path = Path("public/models/lego_yolo_stage2_latest.onnx")
                stage2_model.export(format='onnx', imgsz=config.get('imgsz', 640))
                exported_path = Path("public/models/yolo11s-seg.onnx")
                if exported_path.exists():
                    exported_path.rename(stage2_onnx_path)
                    print(f"[OK] Stage 2 ONNX ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {stage2_onnx_path}")
                else:
                    # ë‹¤ë¥¸ ê°€ëŠ¥í•œ ê²½ë¡œ í™•ì¸
                    possible_paths = [
                        Path("public/models/yolo11s.onnx"),
                        Path("public/models/yolo11n.onnx"),
                        Path("public/models/best.onnx")
                    ]
                    for p in possible_paths:
                        if p.exists():
                            p.rename(stage2_onnx_path)
                            print(f"[OK] Stage 2 ONNX ëª¨ë¸ ì´ë™ ì™„ë£Œ: {p} â†’ {stage2_onnx_path}")
                            break
                    else:
                        raise FileNotFoundError("ONNX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            except Exception as onnx_error:
                print(f"[WARN] Stage 2 ONNX ë³€í™˜ ì‹¤íŒ¨: {onnx_error}")
                stage2_onnx_path = None
            
            # Stage 2 ëª¨ë¸ ì—…ë¡œë“œ
            try:
                stage2_cfg = config.copy()
                stage2_cfg['model_stage'] = 'stage2'
                upload_and_register_model(str(stage2_path), stage2_onnx_path, stage2_cfg)
            except Exception as e:
                print(f"[WARN] Stage 2 ëª¨ë¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print("\n" + "="*60)
        print("ğŸ‰ í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ ì™„ë£Œ!")
        print("="*60)
        print(f"1ë‹¨ê³„ (YOLO11n-seg): {stage1_results}")
        print(f"2ë‹¨ê³„ (YOLO11s-seg): {stage2_results}")
        
        return results, {'stage1': stage1_model, 'stage2': stage2_model}
        
    except Exception as e:
        print(f"[ERROR] í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ ì‹¤íŒ¨: {e}")
        if supabase and job_id:
            try:
                supabase.table('training_jobs').update({
                    'status': 'failed',
                    'progress': {
                        'status': 'failed',
                        'error': str(e)
                    },
                    'updated_at': datetime.now().isoformat()
                }).eq('id', job_id).execute()
            except Exception as update_error:
                print(f"[WARN] ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_error}")
        raise e

def train_yolo_model(dataset_yaml, config, job_id=None):
    """YOLO ëª¨ë¸ í•™ìŠµ"""
    print("[START] YOLO ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    # Supabase í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    supabase = None
    if job_id:
        try:
            supabase = setup_supabase()
            print(f"ğŸ“¡ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ë¨ (ì‘ì—… ID: {job_id})")
        except Exception as e:
            print(f"[WARN] ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = 'cuda' if torch.cuda.is_available() and config.get('device') == 'cuda' else 'cpu'
    print(f"[DEVICE] ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # GPU ì‚¬ìš© ë¶ˆê°€ ì‹œ ê²½ê³  ë° CPU ì„¤ì • ì¡°ì •
    if device == 'cpu':
        print("[WARN] GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")
        print("[TIP] GPU ê°€ì†ì„ ì›í•œë‹¤ë©´ PyTorch CUDA ë²„ì „ì„ ì„¤ì¹˜í•˜ì„¸ìš”.")
        print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
    
    # í•˜ì´ë¸Œë¦¬ë“œ YOLO ëª¨ë¸ í•™ìŠµ (2ë‹¨ê³„ ì‹œìŠ¤í…œ)
    # ê¸°ë³¸ê°’: í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ (1ë‹¨ê³„ + 2ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰)
    model_stage = config.get('model_stage', 'hybrid')  # hybrid, stage1, stage2
    
    if model_stage == 'hybrid':
        print("[HYBRID] í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ ì‹œì‘: 1ë‹¨ê³„ + 2ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰")
        return train_hybrid_models(dataset_yaml, config, job_id)
    elif model_stage == 'stage1':
        model = YOLO('yolo11n-seg.pt')  # 1ë‹¨ê³„: ë¹ ë¥¸ ìŠ¤ìº”ìš©
        print("[START] 1ë‹¨ê³„ ëª¨ë¸ (YOLO11n-seg): ë¹ ë¥¸ ì „ì²´ ìŠ¤ìº”")
    else:
        model = YOLO('yolo11s-seg.pt')  # 2ë‹¨ê³„: ì •ë°€ ê²€ì¦ìš©
        print("[TARGET] 2ë‹¨ê³„ ëª¨ë¸ (YOLO11s-seg): ì •ë°€ ê²€ì¦")
    
    # ë‹¨ê³„ë³„ í•™ìŠµ ì„¤ì •
    if model_stage == 'stage1':
        # 1ë‹¨ê³„: ë¹ ë¥¸ ìŠ¤ìº”ìš© ì„¤ì •
        # CPU ì‚¬ìš© ì‹œ ë°°ì¹˜ í¬ê¸°ì™€ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
        batch_size = 4 if device == 'cpu' else config.get('batch_size', 32)
        imgsz = 416 if device == 'cpu' else config.get('imgsz', 640)
        
        training_args = {
            'data': dataset_yaml,
            'epochs': config.get('epochs', 100),  # 1ë‹¨ê³„: ê¸°ìˆ ë¬¸ì„œ ê¶Œì¥ ê¸°ì¤€
            'batch': batch_size,  # CPU ì‹œ ì‘ì€ ë°°ì¹˜
            'imgsz': imgsz,  # CPU ì‹œ ì‘ì€ ì´ë¯¸ì§€
            'device': device,
            'project': 'output/training',
            'name': f'brickbox_stage1_{config.get("set_num", "latest")}_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            'save': True,
            'plots': True,
            'val': True,
            'patience': 25,  # Early stopping (15 â†’ 25, ì‘ì€ ë°ì´í„°ì…‹ì€ ë” ë§ì€ ì—í­ í•„ìš”)
            'save_period': 10,
            'cache': True,
            'workers': 4,
            'optimizer': 'AdamW',
            'lr0': 0.005,  # 0.01 â†’ 0.005 (ì‘ì€ ë°ì´í„°ì…‹ì— ì í•©)
            'lrf': 0.1,  # 0.01 â†’ 0.1 (ë” ë¶€ë“œëŸ¬ìš´ ê°ì†Œ)
            'momentum': 0.937,
            'weight_decay': 0.0005,
            'warmup_epochs': 5,  # 3 â†’ 5 (ë” ê¸´ ì›Œë°ì—…)
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1,
            'box': 10.0,  # 7.5 â†’ 10.0 (ë°”ìš´ë”© ë°•ìŠ¤ ì •í™•ë„ ê°•ì¡°)
            'cls': 0.5,
            'dfl': 1.5,
            'pose': 12.0,
            'kobj': 2.0,
            'label_smoothing': 0.0,
            # 1ë‹¨ê³„ ë°ì´í„° ì¦ê°• (ë°ì´í„°ì…‹ í¬ê¸° ë¶€ì¡± ëŒ€ì‘ - ì¦ê°• ê°•í™”)
            'copy_paste': 1.0,  # ìµœëŒ€ í™œìš© (ë°ì´í„°ì…‹ ì‘ì„ ë•Œ íš¨ê³¼ì )
            'mosaic': 1.0,  # ìµœëŒ€ í™œìš©
            'mixup': 0.1,  # ì¶”ê°€ (ì‘ì€ ë°ì´í„°ì…‹ì— ìœ ìš©)
            'fliplr': 0.5,
            'hsv_h': 0.015,  # ìƒ‰ìƒ ë³€í™˜ ê°•í™”
            'hsv_s': 0.7,  # 0.3 â†’ 0.7
            'hsv_v': 0.4,  # 0.3 â†’ 0.4
            'perspective': 0.0005,
            'erasing': 0.4,  # 0.1 â†’ 0.4 (ë°ì´í„° ë‹¤ì–‘ì„± ì¦ê°€)
            'nbs': 64,
            'overlap_mask': True,
            'mask_ratio': 4,
            'dropout': 0.0,
            # 'val_period': 1,  # YOLOì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ
            # 1ë‹¨ê³„ ì¶”ë¡  ì„¤ì • (ë‚®ì€ ì„ê³„ê°’ - Recall í–¥ìƒ, BOM í™˜ê²½ ìµœì í™”)
            'conf': 0.10,  # 0.15 â†’ 0.10 (BOM í™˜ê²½: ëˆ„ë½ ë°©ì§€ ìµœìš°ì„ )
            'iou': 0.50,
            'max_det': 200,  # 50 â†’ 200 (ë” ë§ì€ íƒì§€ í—ˆìš©, í›„ì† ì‹ë³„ ë‹¨ê³„ì—ì„œ í•„í„°ë§)
        }
    else:
        # 2ë‹¨ê³„: ì •ë°€ ê²€ì¦ìš© ì„¤ì •
        # CPU ì‚¬ìš© ì‹œ ë°°ì¹˜ í¬ê¸°ì™€ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
        batch_size = 2 if device == 'cpu' else config.get('batch_size', 16)
        imgsz = 512 if device == 'cpu' else config.get('imgsz', 768)
        
        training_args = {
            'data': dataset_yaml,
            'epochs': config.get('epochs', 100),  # 2ë‹¨ê³„: ê¸°ìˆ ë¬¸ì„œ ê¶Œì¥ ê¸°ì¤€
            'batch': batch_size,  # CPU ì‹œ ì‘ì€ ë°°ì¹˜
            'imgsz': imgsz,  # CPU ì‹œ ì‘ì€ ì´ë¯¸ì§€
            'device': device,
            'project': 'output/training',
            'name': f'brickbox_stage2_{config.get("set_num", "latest")}_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            'save': True,
            'plots': True,
            'val': True,
            'patience': 25,  # Early stopping (15 â†’ 25, ì‘ì€ ë°ì´í„°ì…‹ì€ ë” ë§ì€ ì—í­ í•„ìš”)
            'save_period': 10,
            'cache': True,
            'workers': 4,
            'optimizer': 'AdamW',
            'lr0': 0.005,  # 0.01 â†’ 0.005 (ì‘ì€ ë°ì´í„°ì…‹ì— ì í•©)
            'lrf': 0.1,  # 0.01 â†’ 0.1 (ë” ë¶€ë“œëŸ¬ìš´ ê°ì†Œ)
            'momentum': 0.937,
            'weight_decay': 0.0005,
            'warmup_epochs': 5,  # 3 â†’ 5 (ë” ê¸´ ì›Œë°ì—…)
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1,
            'box': 10.0,  # 7.5 â†’ 10.0 (ë°”ìš´ë”© ë°•ìŠ¤ ì •í™•ë„ ê°•ì¡°)
            'cls': 0.5,
            'dfl': 1.5,
            'pose': 12.0,
            'kobj': 2.0,
            'label_smoothing': 0.0,
            # 2ë‹¨ê³„ ë°ì´í„° ì¦ê°• (ë°ì´í„°ì…‹ í¬ê¸° ë¶€ì¡± ëŒ€ì‘ - ìµœëŒ€ ì¦ê°•)
            'copy_paste': 1.0,  # ìµœëŒ€ í™œìš©
            'mosaic': 1.0,  # ìµœëŒ€ í™œìš©
            'mixup': 0.2,  # ì¶”ê°€
            'fliplr': 0.5,
            'hsv_h': 0.015,  # ìƒ‰ìƒ ë³€í™˜ ê°•í™”
            'hsv_s': 0.7,  # ìƒ‰ìƒ ì±„ë„ ë³€í™˜ ê°•í™”
            'hsv_v': 0.4,  # ëª…ë„ ë³€í™˜ ê°•í™”
            'perspective': 0.001,
            'erasing': 0.2,
            'nbs': 64,
            'overlap_mask': True,
            'mask_ratio': 4,
            'dropout': 0.0,
            # 'val_period': 1,  # YOLOì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ
            # 2ë‹¨ê³„ ì¶”ë¡  ì„¤ì • (ì •ë°€ ê²€ì¦, BOM í™˜ê²½ ìµœì í™”)
            'conf': 0.25,  # 0.5 â†’ 0.25 (1ì°¨ ê²°ê³¼ ì¬ê²€ì¦)
            'iou': 0.60,  # ë” ì—„ê²©í•œ IoU (ì •ë°€ ê²€ì¦)
            'max_det': 100,  # 20 â†’ 100 (1ì°¨ì—ì„œ íƒì§€ëœ ì˜ì—­ë§Œ ê²€ì¦)
        }
    
    print(f"[STATS] í•™ìŠµ ì„¤ì •: {training_args}")
    
    # í•™ìŠµ ì‹œì‘
    start_time = time.time()
    
    # ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜
    def on_train_epoch_end(trainer):
        if supabase and job_id:
            epoch = trainer.epoch + 1
            total_epochs = training_args['epochs']
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            metrics = {}
            if hasattr(trainer, 'metrics'):
                metrics = {
                    'box_loss': float(trainer.metrics.get('box_loss', 0)),
                    'seg_loss': float(trainer.metrics.get('seg_loss', 0)),
                    'cls_loss': float(trainer.metrics.get('cls_loss', 0)),
                    'dfl_loss': float(trainer.metrics.get('dfl_loss', 0)),
                    'map50': float(trainer.metrics.get('map50', 0)),
                    'map50_95': float(trainer.metrics.get('map50_95', 0))
                }
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            update_training_progress(supabase, job_id, epoch, total_epochs, metrics)
    
    # YOLO ëª¨ë¸ì— ì½œë°± ì¶”ê°€
    model.add_callback('on_train_epoch_end', on_train_epoch_end)
    
    results = model.train(**training_args)
    end_time = time.time()
    
    training_time = end_time - start_time
    print(f"[TIME] í•™ìŠµ ì™„ë£Œ ì‹œê°„: {training_time:.2f}ì´ˆ")
    
    # í•™ìŠµ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
    if supabase and job_id:
        try:
            supabase.table('training_jobs').update({
                'status': 'completed',
                'progress': {
                    'current_epoch': training_args['epochs'],
                    'total_epochs': training_args['epochs'],
                    'percent': 100,
                    'status': 'completed'
                },
                'completed_at': datetime.now().isoformat(),
                'updated_at': datetime.now().isoformat()
            }).eq('id', job_id).execute()
            print("[OK] í•™ìŠµ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸ë¨")
        except Exception as e:
            print(f"[WARN] í•™ìŠµ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    return results, model

def save_model(model, config):
    """í•™ìŠµëœ ëª¨ë¸ ì €ì¥"""
    print("[SAVE] í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ì¤‘...")
    
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    model_dir = Path("public/models")
    model_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # PyTorch ëª¨ë¸ë¡œ ì €ì¥ (ê¸°ë³¸)
        model_path = model_dir / f"lego_yolo_set_{config.get('set_num', 'latest')}.pt"
        model.save(model_path)
        print(f"[OK] PyTorch ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        
        # ONNX ë³€í™˜ ì‹œë„ (í•„ìˆ˜) // ğŸ”§ ìˆ˜ì •ë¨
        onnx_path = None
        try:
            onnx_path = model_dir / f"lego_yolo_set_{config.get('set_num', 'latest')}.onnx"
            model.export(format='onnx', imgsz=config.get('imgsz', 640))
            
            # ë‚´ë³´ë‚¸ ëª¨ë¸ì„ ëª©ì ì§€ë¡œ ì´ë™
            exported_path = model_dir / "yolo11n.onnx"
            if exported_path.exists():
                exported_path.rename(onnx_path)
                print(f"[OK] ONNX ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {onnx_path}")
            else:
                # ë‹¤ë¥¸ ê²½ë¡œì—ì„œ ì°¾ê¸° ì‹œë„
                possible_paths = [
                    model_dir / "yolo11s-seg.onnx",
                    model_dir / "yolo11n-seg.onnx",
                    model_dir / "best.onnx"
                ]
                for p in possible_paths:
                    if p.exists():
                        p.rename(onnx_path)
                        print(f"[OK] ONNX ëª¨ë¸ ì´ë™ ì™„ë£Œ: {p} â†’ {onnx_path}")
                        break
                else:
                    raise FileNotFoundError("ONNX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        except Exception as onnx_error:
            print(f"[ERROR] ONNX ë³€í™˜ ì‹¤íŒ¨: {onnx_error}")
            print("[WARN] ONNX íŒŒì¼ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì €ì—ì„œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            onnx_path = None
        
        # ìë™ ì—…ë¡œë“œ ë° ë“±ë¡
        upload_and_register_model(model_path, onnx_path, config)
            
    except Exception as e:
        print(f"[ERROR] ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

def upload_and_register_model(model_path, onnx_path, config):
    """í•™ìŠµëœ ëª¨ë¸ì„ Supabase ìŠ¤í† ë¦¬ì§€ì— ì—…ë¡œë“œí•˜ê³  model_registryì— ë“±ë¡ // ğŸ”§ ìˆ˜ì •ë¨"""
    try:
        from supabase import create_client
        import os
        from datetime import datetime
        
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Service Role ìš°ì„ )
        supabase_url = os.getenv('SUPABASE_URL') or os.getenv('VITE_SUPABASE_URL')
        supabase_key = (
            os.getenv('SUPABASE_SERVICE_ROLE_KEY')
            or os.getenv('VITE_SUPABASE_SERVICE_ROLE')
            or os.getenv('SUPABASE_ANON_KEY')
            or os.getenv('VITE_SUPABASE_ANON_KEY')
        )
        
        if not supabase_url or not supabase_key:
            print("[WARN] Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ëª¨ë¸ ì—…ë¡œë“œ ìŠ¤í‚µ")
            return
            
        supabase = create_client(supabase_url, supabase_key)
        
        # ëª¨ë¸ íŒŒì¼ ì½ê¸°
        with open(model_path, 'rb') as f:
            model_data = f.read()
        
        # íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ + ìŠ¤í…Œì´ì§€ í¬í•¨) // ğŸ”§ ìˆ˜ì •ë¨
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        stage_label = config.get('model_stage', 'single')
        if stage_label not in ('stage1', 'stage2'):
            stage_label = 'single'
        model_name = f"brickbox_s_seg_{stage_label}_{timestamp}"
        pt_bucket_path = f"{model_name}.pt"
        
        # PyTorch ëª¨ë¸ ì—…ë¡œë“œ
        print(f"[UPLOAD] PyTorch ëª¨ë¸ ì—…ë¡œë“œ ì¤‘: {pt_bucket_path}")
        upload_result = supabase.storage.from_('models').upload(
            pt_bucket_path,
            model_data
        )
        
        # Supabase Storage ì—…ë¡œë“œ ì‘ë‹µ ì²˜ë¦¬
        if hasattr(upload_result, 'error') and upload_result.error:
            print(f"[ERROR] PyTorch ëª¨ë¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {upload_result.error}")
            pt_public_url = f"{supabase_url}/storage/v1/object/public/models/{pt_bucket_path}"
            print(f"[WARN] ëª¨ë¸ ì—…ë¡œë“œ ì‹¤íŒ¨ë¡œ ì¸í•´ ì˜ˆìƒ ê³µê°œ URLì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {pt_public_url}")
        else:
            pt_public_url = f"{supabase_url}/storage/v1/object/public/models/{pt_bucket_path}"
            print(f"[OK] PyTorch ëª¨ë¸ ì—…ë¡œë“œ ì„±ê³µ, ê³µê°œ URL: {pt_public_url}")
        
        # ONNX ëª¨ë¸ ì—…ë¡œë“œ (ìˆëŠ” ê²½ìš°) // ğŸ”§ ìˆ˜ì •ë¨
        onnx_bucket_path = None
        onnx_public_url = None
        # onnx_pathë¥¼ Path ê°ì²´ë¡œ ì •ê·œí™”
        onnx_path_obj = Path(onnx_path) if onnx_path else None
        if onnx_path_obj and onnx_path_obj.exists():
            onnx_bucket_path = f"{model_name}.onnx"
            print(f"[UPLOAD] ONNX ëª¨ë¸ ì—…ë¡œë“œ ì¤‘: {onnx_bucket_path}")
            with open(onnx_path_obj, 'rb') as f:
                onnx_data = f.read()
            
            onnx_upload_result = supabase.storage.from_('models').upload(
                onnx_bucket_path,
                onnx_data
            )
            
            if hasattr(onnx_upload_result, 'error') and onnx_upload_result.error:
                print(f"[ERROR] ONNX ëª¨ë¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {onnx_upload_result.error}")
            else:
                onnx_public_url = f"{supabase_url}/storage/v1/object/public/models/{onnx_bucket_path}"
                print(f"[OK] ONNX ëª¨ë¸ ì—…ë¡œë“œ ì„±ê³µ, ê³µê°œ URL: {onnx_public_url}")
        else:
            print("[WARN] ONNX íŒŒì¼ì´ ì—†ì–´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
        
        # model_registryì— ë“±ë¡
        print(f"[REGISTER] ëª¨ë¸ ë“±ë¡ ì¤‘: {model_name}")
        model_size_mb = len(model_data) / (1024 * 1024)
        
        # ë™ì¼ stageì˜ í™œì„± ëª¨ë¸ë§Œ ë¹„í™œì„±í™” (stage1ê³¼ stage2ëŠ” ë³„ë„ ê´€ë¦¬) // ğŸ”§ ìˆ˜ì •ë¨
        current_stage = config.get('model_stage', 'single')
        if current_stage in ('stage1', 'stage2'):
            # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ: ë™ì¼ stageë§Œ ë¹„í™œì„±í™”
        supabase.table('model_registry').update({
            'is_active': False,
            'status': 'inactive'
        }).eq('is_active', True).eq('model_stage', current_stage).execute()
            print(f"[INFO] {current_stage} ê¸°ì¡´ í™œì„± ëª¨ë¸ ë¹„í™œì„±í™”")
        else:
            # ë‹¨ì¼ ëª¨ë“œ: ëª¨ë“  í™œì„± ëª¨ë¸ ë¹„í™œì„±í™”
            supabase.table('model_registry').update({
                'is_active': False,
                'status': 'inactive'
            }).eq('is_active', True).execute()
        
        # ìƒˆ ëª¨ë¸ ë“±ë¡ (ONNX URL ìš°ì„  ì‚¬ìš©, í•­ìƒ activeë¡œ ì„¤ì •) // ğŸ”§ ìˆ˜ì •ë¨
        from datetime import datetime as _dt
        version = f"1.0.{_dt.now().strftime('%Y%m%d%H%M%S')}"
        
        # model_urlì€ ë¸Œë¼ìš°ì €ìš© ONNX íŒŒì¼ì„ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ .pt ì‚¬ìš©
        final_model_url = onnx_public_url if onnx_public_url else pt_public_url
        
        registry_data = {
            'model_name': model_name,
            'version': version,
            'model_url': final_model_url,  # ONNX ìš°ì„ , ì—†ìœ¼ë©´ .pt
            'model_path': onnx_bucket_path if onnx_bucket_path else pt_bucket_path,  # ONNX ìš°ì„ 
            'pt_model_path': pt_bucket_path,
            'is_active': True,  # í•­ìƒ activeë¡œ ì„¤ì •
            'status': 'active',  # í•­ìƒ activeë¡œ ì„¤ì • (stage1, stage2 ëª¨ë‘)
            'model_type': 'yolo',
            'model_size_mb': round(model_size_mb, 2),
            'segmentation_support': True,
            'model_stage': current_stage,
            'created_by': 'system',
            'training_metadata': {
                'set_num': config.get('set_num', 'latest'),
                'part_id': config.get('part_id'),
                'epochs': config.get('epochs', 100),
                'batch_size': config.get('batch_size', 16),
                'imgsz': config.get('imgsz', 640),
                'device': config.get('device', 'cuda')
            }
        }
        
        registry_result = supabase.table('model_registry').insert(registry_data).execute()
        
        # Supabase ì‘ë‹µ ì²˜ë¦¬
        if hasattr(registry_result, 'error') and registry_result.error:
            print(f"[ERROR] ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {registry_result.error}")
        else:
            print(f"[OK] ëª¨ë¸ ì—…ë¡œë“œ ë° ë“±ë¡ ì™„ë£Œ: {model_name}")
            print(f"[URL] PyTorch URL: {pt_public_url}")
            if onnx_public_url:
                print(f"[URL] ONNX URL: {onnx_public_url}")
            else:
                print("[WARN] ONNX URLì´ ì—†ìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì €ì—ì„œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"[ERROR] ëª¨ë¸ ì—…ë¡œë“œ ë° ë“±ë¡ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='BrickBox YOLO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--set_num', default='latest', help='ë ˆê³  ì„¸íŠ¸ ë²ˆí˜¸')
    parser.add_argument('--part_id', help='ë¶€í’ˆ ID ë˜ëŠ” ì—˜ë¦¬ë¨¼íŠ¸ ID (ë¶€í’ˆ ë‹¨ìœ„ í•™ìŠµìš©)')
    parser.add_argument('--epochs', type=int, default=100, help='í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ìˆ ë¬¸ì„œ ê¶Œì¥: 100)')
    parser.add_argument('--batch_size', type=int, default=24, help='ë°°ì¹˜ í¬ê¸° (16~32 ê¶Œì¥)')
    parser.add_argument('--imgsz', type=int, default=768, help='ì´ë¯¸ì§€ í¬ê¸° (768 ê¶Œì¥, 960 ê³ ì„±ëŠ¥)')
    parser.add_argument('--device', default='cuda', help='ì‚¬ìš© ë””ë°”ì´ìŠ¤')
    parser.add_argument('--job_id', help='í•™ìŠµ ì‘ì—… ID')
    parser.add_argument('--model_stage', choices=['stage1', 'stage2', 'hybrid'], default='hybrid', 
                       help='í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë‹¨ê³„ (hybrid: 1ë‹¨ê³„+2ë‹¨ê³„, stage1: YOLO11n-seg, stage2: YOLO11s-seg)')
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logger = setup_logging()
    
    # ì„¤ì • ì •ë³´
    config = {
        'set_num': args.set_num,
        'part_id': args.part_id,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'imgsz': args.imgsz,
        'device': args.device,
        'job_id': args.job_id,
        'model_stage': args.model_stage
    }
    
    print("[START] BrickBox YOLO í•™ìŠµ ì‹œì‘")
    print(f"[STATS] ì„¤ì •: {config}")
    
    try:
        # 1. í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì‹œì‘)
        if args.job_id:
            update_training_status(args.job_id, 'running', {'status': 'í•™ìŠµ ì‹œì‘'})
        
        # 2. ID ë§¤í•‘ ì œê±°: ì „ë‹¬ë°›ì€ IDë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (element_id ë””ë ‰í† ë¦¬ ìš°ì„ )
        actual_part_id = args.part_id
        
        # 3. ë°ì´í„°ì…‹ ì¤€ë¹„ (ì¤‘ë³µ ë¶€í’ˆ ì œê±° í¬í•¨)
        dataset_yaml = prepare_dataset(args.set_num, actual_part_id)
        if not dataset_yaml:
            print("[ERROR] ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨")
            if args.job_id:
                update_training_status(args.job_id, 'failed', {'error': 'ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨'})
            sys.exit(1)
        
        # 4. YOLO ëª¨ë¸ í•™ìŠµ
        print("[START] YOLO ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        results, model = train_yolo_model(dataset_yaml, config, args.job_id)
        
        # 5. í•™ìŠµ ê²°ê³¼ ì²˜ë¦¬
        if results:
            print("[OK] í•™ìŠµ ì™„ë£Œ!")
            
            # ìµœì¢… ë©”íŠ¸ë¦­ ì¶”ì¶œ (ì•ˆì „í•œ ë°©ì‹)
            final_metrics = {
                'mAP50': 0.0,
                'mAP50_95': 0.0,
                'precision': 0.0,
                'recall': 0.0
            }
            
            try:
                if hasattr(results, 'box') and results.box:
                    final_metrics['mAP50'] = getattr(results.box, 'map50', 0.0)
                    final_metrics['mAP50_95'] = getattr(results.box, 'map', 0.0)
                    final_metrics['precision'] = getattr(results.box, 'mp', 0.0)
                    final_metrics['recall'] = getattr(results.box, 'mr', 0.0)
            except Exception as e:
                print(f"[WARN] ë©”íŠ¸ë¦­ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                # ê¸°ë³¸ê°’ ì‚¬ìš©
            
            print(f"[STATS] ìµœì¢… ì„±ëŠ¥:")
            for metric, value in final_metrics.items():
                print(f"  {metric}: {value:.4f}")
            
            # 6. ëª¨ë¸ ì €ì¥ (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì²˜ë¦¬)
            if isinstance(model, dict):
                print("[HYBRID] í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì €ì¥ ì¤‘...")
                for stage, stage_model in model.items():
                    if stage_model and hasattr(stage_model, 'save'):
                        stage_path = Path(f"public/models/lego_yolo_{stage}_latest.pt")
                        stage_model.save(stage_path)
                        print(f"[OK] {stage} ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {stage_path}")
            else:
                save_model(model, config)
            
            # 7. í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì™„ë£Œ)
            if args.job_id:
                update_training_status(args.job_id, 'completed', {
                    'status': 'í•™ìŠµ ì™„ë£Œ',
                    'final_metrics': final_metrics
                })
            
            # 8. ë¶€í’ˆ í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´)
            update_part_training_status(args.set_num, actual_part_id, final_metrics)
            
            print("[SUCCESS] BrickBox YOLO í•™ìŠµ ì™„ë£Œ!")
            
        else:
            print("[ERROR] í•™ìŠµ ì‹¤íŒ¨")
            if args.job_id:
                update_training_status(args.job_id, 'failed', {'error': 'í•™ìŠµ ì‹¤íŒ¨'})
            sys.exit(1)
            
    except Exception as e:
        print(f"[ERROR] í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if args.job_id:
            update_training_status(args.job_id, 'failed', {'error': str(e)})
        sys.exit(1)

if __name__ == "__main__":
    main()