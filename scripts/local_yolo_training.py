#!/usr/bin/env python3
"""
ğŸ§± BrickBox ë¡œì»¬ YOLO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ì›¹ UIì—ì„œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” YOLO ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime

# YOLO ê´€ë ¨ ì„í¬íŠ¸
try:
    from ultralytics import YOLO
    import torch
except ImportError as e:
    print(f"âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install ultralytics torch")
    sys.exit(1)

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    import logging
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path("output/training/logs")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    log_file = log_dir / f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger(__name__)

def update_training_status(job_id, status, progress=None, metrics=None):
    """í•™ìŠµ ìƒíƒœë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë°ì´íŠ¸"""
    try:
        import requests
        
        # Supabase API í˜¸ì¶œ (ì‹¤ì œ êµ¬í˜„ ì‹œ í™˜ê²½ë³€ìˆ˜ì—ì„œ URL ê°€ì ¸ì˜¤ê¸°)
        supabase_url = os.getenv('VITE_SUPABASE_URL', 'https://npferbxuxocbfnfbpcnz.supabase.co')
        supabase_key = os.getenv('VITE_SUPABASE_ANON_KEY', 'your-anon-key')
        
        # í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸
        update_data = {
            'status': status,
            'updated_at': datetime.now().isoformat()
        }
        
        if progress:
            update_data['progress'] = progress
            
        if metrics:
            update_data['metrics'] = metrics
        
        # ì‹¤ì œ êµ¬í˜„ ì‹œ Supabase REST API í˜¸ì¶œ
        print(f"ğŸ“Š í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸: {status}")
        if progress:
            print(f"ğŸ“ˆ ì§„í–‰ë¥ : {progress}")
        if metrics:
            print(f"ğŸ“Š ë©”íŠ¸ë¦­: {metrics}")
            
    except Exception as e:
        print(f"âš ï¸ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def prepare_dataset(set_num, part_id=None):
    """ë°ì´í„°ì…‹ ì¤€ë¹„ - ì¤‘ë³µ ë¶€í’ˆ ì œê±° í¬í•¨"""
    print(f"ğŸ“‹ ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹œì‘: ì„¸íŠ¸ {set_num}, ë¶€í’ˆ {part_id}")

    # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •
    if set_num == 'latest':
        # ë¶€í’ˆ ë‹¨ìœ„ í•™ìŠµì˜ ê²½ìš° í•´ë‹¹ ë¶€í’ˆ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        if part_id:
            # ë¨¼ì € part_idë¡œ ì‹œë„
            dataset_path = Path(f"output/synthetic/{part_id}")
            
            # part_idë¡œ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì—˜ë¦¬ë¨¼íŠ¸ IDë¡œ ì‹œë„
            if not dataset_path.exists():
                print(f"âš ï¸ ë¶€í’ˆ ID {part_id} ë””ë ‰í† ë¦¬ê°€ ì—†ìŒ. ì—˜ë¦¬ë¨¼íŠ¸ IDë¡œ ê²€ìƒ‰ ì¤‘...")
                
                # parts_masterì—ì„œ ì—˜ë¦¬ë¨¼íŠ¸ ID ì¡°íšŒ
                try:
                    import requests
                    import os
                    
                    supabase_url = os.getenv('VITE_SUPABASE_URL', 'https://npferbxuxocbfnfbpcnz.supabase.co')
                    supabase_key = os.getenv('VITE_SUPABASE_ANON_KEY', 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im5wZmVyYnh1eG9jYmZuZmJwY256Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTk0NzQ5ODUsImV4cCI6MjA3NTA1MDk4NX0.eqKQh_o1k2VmP-_v__gUMHVOgvdIzml-zDhZyzfxUmk')
                    
                    # // ğŸ”§ ìˆ˜ì •ë¨: REST ë©”ì„œë“œë¥¼ GETìœ¼ë¡œ ë³€ê²½í•˜ê³  limit ì¶”ê°€
                    response = requests.get(
                        f"{supabase_url}/rest/v1/parts_master",
                        headers={
                            "apikey": supabase_key,
                            "Authorization": f"Bearer {supabase_key}",
                            "Content-Type": "application/json"
                        },
                        params={
                            "part_id": f"eq.{part_id}",
                            "select": "element_id",
                            "limit": 1
                        }
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        if data and len(data) > 0:
                            element_id = data[0].get('element_id')
                            if element_id:
                                dataset_path = Path(f"output/synthetic/{element_id}")
                                print(f"ğŸ”„ ì—˜ë¦¬ë¨¼íŠ¸ ID {element_id} ë””ë ‰í† ë¦¬ë¡œ ì‹œë„: {dataset_path}")
                except Exception as e:
                    print(f"âš ï¸ ì—˜ë¦¬ë¨¼íŠ¸ ID ì¡°íšŒ ì‹¤íŒ¨: {e}")
        else:
            dataset_path = Path("output/synthetic/prepared")
    else:
        dataset_path = Path(f"output/synthetic/set_{set_num}")

    if not dataset_path.exists():
        print(f"âŒ ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dataset_path}")
        return None
    
    # dataset.yaml íŒŒì¼ í™•ì¸
    yaml_file = dataset_path / "dataset.yaml"
    if not yaml_file.exists():
        print(f"âŒ dataset.yaml íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {yaml_file}")
        return None
    
    # ì¤‘ë³µ ë¶€í’ˆ ì œê±° ì²˜ë¦¬
    if set_num != 'latest':
        filtered_yaml = remove_duplicate_parts(yaml_file, set_num, part_id)
        if filtered_yaml:
            yaml_file = filtered_yaml
    
    print(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {dataset_path}")
    return str(yaml_file)

def remove_duplicate_parts(yaml_file, set_num, part_id=None):
    """ì´ë¯¸ í•™ìŠµëœ ë¶€í’ˆì„ ì œê±°í•˜ì—¬ ì¤‘ë³µ í•™ìŠµ ë°©ì§€"""
    try:
        import yaml
        from supabase import create_client
        import os
        
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        supabase_url = os.getenv('SUPABASE_URL') or os.getenv('VITE_SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_ANON_KEY') or os.getenv('VITE_SUPABASE_ANON_KEY')
        
        if not supabase_url or not supabase_key:
            print("âš ï¸ Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ì¤‘ë³µ ì œê±° ìŠ¤í‚µ")
            print("ğŸ’¡ SUPABASE_URL, SUPABASE_ANON_KEY ë˜ëŠ” VITE_SUPABASE_URL, VITE_SUPABASE_ANON_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”")
            return None
            
        supabase = create_client(supabase_url, supabase_key)
        
        # dataset.yaml ì½ê¸°
        with open(yaml_file, 'r', encoding='utf-8') as f:
            dataset_config = yaml.safe_load(f)
        
        # ì„¸íŠ¸ ë¶€í’ˆ ëª©ë¡ ì¡°íšŒ
        if part_id:
            # ë¶€í’ˆ ë‹¨ìœ„ í•™ìŠµ: íŠ¹ì • ë¶€í’ˆë§Œ í•™ìŠµ
            target_parts = [part_id]
            print(f"ğŸ§© ë¶€í’ˆ ë‹¨ìœ„ í•™ìŠµ: {part_id}")
        else:
            # ì„¸íŠ¸ ë‹¨ìœ„ í•™ìŠµ: ì„¸íŠ¸ ë‚´ ëª¨ë“  ë¶€í’ˆ
            try:
                # ì„¸íŠ¸ ì •ë³´ ì¡°íšŒ
                set_response = supabase.table('lego_sets').select('id').eq('set_num', set_num).single().execute()
                if not set_response.data:
                    print(f"âŒ ì„¸íŠ¸ {set_num}ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return None
                
                # ì„¸íŠ¸ ë¶€í’ˆ ì¡°íšŒ
                parts_response = supabase.table('set_parts').select('part_id').eq('set_id', set_response.data['id']).execute()
                target_parts = [p['part_id'] for p in parts_response.data] if parts_response.data else []
                print(f"ğŸ“¦ ì„¸íŠ¸ {set_num} ë¶€í’ˆ: {len(target_parts)}ê°œ")
            except Exception as e:
                print(f"âš ï¸ ì„¸íŠ¸ ë¶€í’ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                return None
        
        if not target_parts:
            print("âŒ í•™ìŠµí•  ë¶€í’ˆì´ ì—†ìŒ")
            return None
        
        # ì´ë¯¸ í•™ìŠµëœ ë¶€í’ˆ í™•ì¸
        try:
            # ì—¬ëŸ¬ ë¶€í’ˆ IDë¥¼ í•œ ë²ˆì— ì¡°íšŒ (ì„±ëŠ¥ ìµœì í™”)
            trained_parts = []
            try:
                # Supabase Python í´ë¼ì´ì–¸íŠ¸ì—ì„œ in_() ë©”ì„œë“œ ì‚¬ìš©
                response = supabase.table('part_training_status').select('part_id').in_('part_id', target_parts).eq('status', 'completed').execute()
                if response.data:
                    trained_parts = [p['part_id'] for p in response.data]
                    print(f"ğŸ” ì¼ê´„ ì¡°íšŒë¡œ {len(trained_parts)}ê°œ ë¶€í’ˆ ìƒíƒœ í™•ì¸")
            except Exception as e:
                print(f"âš ï¸ ì¼ê´„ ì¡°íšŒ ì‹¤íŒ¨, ê°œë³„ ì¡°íšŒë¡œ ì „í™˜: {e}")
                # ì¼ê´„ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ê°œë³„ ì¡°íšŒë¡œ fallback
                for part_id in target_parts:
                    try:
                        response = supabase.table('part_training_status').select('part_id').eq('part_id', part_id).eq('status', 'completed').execute()
                        if response.data:
                            trained_parts.extend([p['part_id'] for p in response.data])
                    except Exception as e:
                        print(f"âš ï¸ ë¶€í’ˆ {part_id} ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                        continue
            
            if trained_parts:
                print(f"â­ï¸ ì´ë¯¸ í•™ìŠµëœ ë¶€í’ˆ {len(trained_parts)}ê°œ ìŠ¤í‚µ: {trained_parts[:5]}{'...' if len(trained_parts) > 5 else ''}")
                
                # ìƒˆë¡œ í•™ìŠµí•  ë¶€í’ˆë§Œ í•„í„°ë§
                new_parts = [p for p in target_parts if p not in trained_parts]
                
                if not new_parts:
                    print("âŒ ëª¨ë“  ë¶€í’ˆì´ ì´ë¯¸ í•™ìŠµë¨ - í•™ìŠµ ì¤‘ë‹¨")
                    return None
                
                print(f"âœ… ìƒˆë¡œ í•™ìŠµí•  ë¶€í’ˆ {len(new_parts)}ê°œ: {new_parts[:5]}{'...' if len(new_parts) > 5 else ''}")
                
                # í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„±
                filtered_yaml = create_filtered_dataset(yaml_file, new_parts)
                return filtered_yaml
            else:
                print("âœ… ëª¨ë“  ë¶€í’ˆì´ ìƒˆë¡œ í•™ìŠµ ëŒ€ìƒ")
                return None
                
        except Exception as e:
            print(f"âš ï¸ ì¤‘ë³µ ë¶€í’ˆ í™•ì¸ ì‹¤íŒ¨: {e}")
            return None
            
    except Exception as e:
        print(f"âŒ ì¤‘ë³µ ë¶€í’ˆ ì œê±° ì‹¤íŒ¨: {e}")
        return None

def create_filtered_dataset(original_yaml, target_parts):
    """íŠ¹ì • ë¶€í’ˆë§Œ í¬í•¨í•˜ëŠ” í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„±"""
    try:
        import yaml
        import shutil
        from pathlib import Path
        import json
        
        print(f"ğŸ” í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘: {len(target_parts)}ê°œ ë¶€í’ˆ")
        print(f"ğŸ“¦ ëŒ€ìƒ ë¶€í’ˆ: {target_parts}")
        
        # ì›ë³¸ íŒŒì¼ ë°±ì—…
        backup_yaml = original_yaml.replace('.yaml', '_backup.yaml')
        shutil.copy2(original_yaml, backup_yaml)
        print(f"ğŸ’¾ ì›ë³¸ ë°±ì—…: {backup_yaml}")
        
        # dataset.yaml ì½ê¸°
        with open(original_yaml, 'r', encoding='utf-8') as f:
            dataset_config = yaml.safe_load(f)
        
        # YOLO ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
        train_path = Path(dataset_config.get('train', ''))
        val_path = Path(dataset_config.get('val', ''))
        test_path = Path(dataset_config.get('test', ''))
        
        print(f"ğŸ“ ë°ì´í„°ì…‹ ê²½ë¡œ:")
        print(f"  - Train: {train_path}")
        print(f"  - Val: {val_path}")
        print(f"  - Test: {test_path}")
        
        # ë¶€í’ˆë³„ í´ë˜ìŠ¤ ID ë§¤í•‘ ìƒì„±
        part_to_class_id = {part_id: idx for idx, part_id in enumerate(target_parts)}
        print(f"ğŸ·ï¸ í´ë˜ìŠ¤ ë§¤í•‘: {part_to_class_id}")
        
        # í•„í„°ë§ëœ íŒŒì¼ë“¤ ìˆ˜ì§‘
        filtered_files = {'train': [], 'val': [], 'test': []}
        total_images = 0
        
        for split_name, split_path in [('train', train_path), ('val', val_path), ('test', test_path)]:
            if not split_path.exists():
                print(f"âš ï¸ {split_name} ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {split_path}")
                continue
                
            print(f"ğŸ” {split_name} í´ë” ìŠ¤ìº” ì¤‘...")
            split_images = []
            
            # ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸° (webp í˜•ì‹)
            for image_file in split_path.glob('*.webp'):
                # ë¶€í’ˆ IDê°€ íŒŒì¼ëª…ì— í¬í•¨ëœ ì´ë¯¸ì§€ë§Œ í•„í„°ë§
                # 1. element_id íŒ¨í„´: {element_id}_{sequence}.webp
                # 2. part_id íŒ¨í„´: {part_id}_{sequence}.webp (ì—˜ë¦¬ë¨¼íŠ¸ IDê°€ ì—†ëŠ” ê²½ìš°)
                for part_id in target_parts:
                    # íŒŒì¼ëª…ì´ í•´ë‹¹ ë¶€í’ˆìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                    if image_file.name.startswith(f"{part_id}_"):
                        # ë¼ë²¨ íŒŒì¼ ê²½ë¡œ (labels í´ë”ì—ì„œ)
                        label_path = split_path.parent.parent / "labels" / split_path.name / image_file.with_suffix('.txt').name
                        if label_path.exists():
                            # ë¼ë²¨ íŒŒì¼ì—ì„œ í•´ë‹¹ ë¶€í’ˆì˜ ë¼ë²¨ë§Œ ì¶”ì¶œ
                            filtered_labels = filter_labels_for_parts(label_path, target_parts, part_to_class_id)
                            if filtered_labels:
                                split_images.append(str(image_file))
                                total_images += 1
                                print(f"  âœ… {image_file.name} (ë¶€í’ˆ: {part_id})")
                        break
            
            filtered_files[split_name] = split_images
            print(f"ğŸ“Š {split_name}: {len(split_images)}ê°œ ì´ë¯¸ì§€")
        
        if total_images == 0:
            print("âŒ í•„í„°ë§ëœ ì´ë¯¸ì§€ê°€ ì—†ìŒ")
            return None
        
        # í•„í„°ë§ëœ dataset.yaml ìƒì„±
        filtered_config = dataset_config.copy()
        filtered_config['nc'] = len(target_parts)  # í´ë˜ìŠ¤ ìˆ˜
        filtered_config['names'] = target_parts     # í´ë˜ìŠ¤ ì´ë¦„
        
        # í•„í„°ë§ëœ íŒŒì¼ì„ ì„ì‹œ í´ë”ì— ë³µì‚¬
        temp_dir = Path(original_yaml).parent / "filtered_dataset"
        temp_dir.mkdir(exist_ok=True)
        
        # train/val/test í´ë” ìƒì„±
        for split_name in ['train', 'val', 'test']:
            split_dir = temp_dir / split_name
            split_dir.mkdir(exist_ok=True)
            
            for image_path in filtered_files[split_name]:
                src_path = Path(image_path)
                dst_path = split_dir / src_path.name
                shutil.copy2(src_path, dst_path)
                
                # ë¼ë²¨ íŒŒì¼ë„ ë³µì‚¬ (labels í´ë”ì—ì„œ)
                label_src = Path(image_path).parent.parent / "labels" / Path(image_path).parent.name / Path(image_path).with_suffix('.txt').name
                label_dst = split_dir / Path(image_path).with_suffix('.txt').name
                if label_src.exists():
                    # í•„í„°ë§ëœ ë¼ë²¨ ë‚´ìš©ìœ¼ë¡œ ìƒˆ íŒŒì¼ ìƒì„±
                    filtered_labels = filter_labels_for_parts(label_src, target_parts, part_to_class_id)
                    if filtered_labels:
                        with open(label_dst, 'w', encoding='utf-8') as f:
                            for line in filtered_labels:
                                f.write(line + '\n')
        
        # í•„í„°ë§ëœ dataset.yaml ì €ì¥
        temp_yaml = temp_dir / "dataset.yaml"
        with open(temp_yaml, 'w', encoding='utf-8') as f:
            yaml.dump(filtered_config, f, default_flow_style=False)
        
        print(f"âœ… í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {total_images}ê°œ ì´ë¯¸ì§€")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {temp_yaml}")
        return str(temp_yaml)
        
    except Exception as e:
        print(f"âŒ í•„í„°ë§ëœ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def filter_labels_for_parts(label_file, target_parts, part_to_class_id):
    """ë¼ë²¨ íŒŒì¼ì—ì„œ íŠ¹ì • ë¶€í’ˆì˜ ë¼ë²¨ë§Œ í•„í„°ë§"""
    try:
        filtered_lines = []
        
        # íŒŒì¼ëª…ì—ì„œ ë¶€í’ˆ ID ì¶”ì¶œ
        file_name = Path(label_file).stem
        print(f"ğŸ” ë¼ë²¨ íŒŒì¼ ì²˜ë¦¬: {file_name}")
        
        with open(label_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                    
                parts = line.split()
                if len(parts) < 5:
                    continue
                
                # í˜„ì¬ ëª¨ë“  ë¼ë²¨ì´ í´ë˜ìŠ¤ 0ìœ¼ë¡œ í†µí•©ë˜ì–´ ìˆìŒ
                # íŒŒì¼ëª…ì´ target_partsì— í¬í•¨ëœ ë¶€í’ˆìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                for part_id, new_class_id in part_to_class_id.items():
                    if file_name.startswith(f"{part_id}_"):
                        # ìƒˆë¡œìš´ í´ë˜ìŠ¤ IDë¡œ ë³€ê²½
                        parts[0] = str(new_class_id)
                        filtered_lines.append(' '.join(parts))
                        print(f"  âœ… ë¼ë²¨ ë³€í™˜: í´ë˜ìŠ¤ 0 â†’ {new_class_id} (ë¶€í’ˆ: {part_id})")
                        break
        
        return filtered_lines
        
    except Exception as e:
        print(f"âš ï¸ ë¼ë²¨ í•„í„°ë§ ì‹¤íŒ¨ {label_file}: {e}")
        return []

def update_part_training_status(set_num, part_id, metrics):
    """í•™ìŠµ ì™„ë£Œëœ ë¶€í’ˆë“¤ì˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ì¤‘ë³µ í•™ìŠµ ë°©ì§€"""
    try:
        from supabase import create_client
        import os
        
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        supabase_url = os.getenv('SUPABASE_URL') or os.getenv('VITE_SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_ANON_KEY') or os.getenv('VITE_SUPABASE_ANON_KEY')
        
        if not supabase_url or not supabase_key:
            print("âš ï¸ Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ë¶€í’ˆ ìƒíƒœ ì—…ë°ì´íŠ¸ ìŠ¤í‚µ")
            print("ğŸ’¡ SUPABASE_URL, SUPABASE_ANON_KEY ë˜ëŠ” VITE_SUPABASE_URL, VITE_SUPABASE_ANON_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”")
            return
            
        supabase = create_client(supabase_url, supabase_key)
        
        if part_id:
            # ë¶€í’ˆ ë‹¨ìœ„ í•™ìŠµ: íŠ¹ì • ë¶€í’ˆë§Œ ì—…ë°ì´íŠ¸
            trained_parts = [part_id]
            print(f"ğŸ§© ë¶€í’ˆ ë‹¨ìœ„ í•™ìŠµ ì™„ë£Œ: {part_id}")
        else:
            # ì„¸íŠ¸ ë‹¨ìœ„ í•™ìŠµ: ì„¸íŠ¸ ë‚´ ëª¨ë“  ë¶€í’ˆ ì—…ë°ì´íŠ¸
            try:
                # ì„¸íŠ¸ ì •ë³´ ì¡°íšŒ
                set_response = supabase.table('lego_sets').select('id').eq('set_num', set_num).single().execute()
                if not set_response.data:
                    print(f"âŒ ì„¸íŠ¸ {set_num}ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return
                
                # ì„¸íŠ¸ ë¶€í’ˆ ì¡°íšŒ
                parts_response = supabase.table('set_parts').select('part_id').eq('set_id', set_response.data['id']).execute()
                trained_parts = [p['part_id'] for p in parts_response.data] if parts_response.data else []
                print(f"ğŸ“¦ ì„¸íŠ¸ ë‹¨ìœ„ í•™ìŠµ ì™„ë£Œ: {len(trained_parts)}ê°œ ë¶€í’ˆ")
            except Exception as e:
                print(f"âš ï¸ ì„¸íŠ¸ ë¶€í’ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                return
        
        # ê° ë¶€í’ˆì˜ í•™ìŠµ ìƒíƒœë¥¼ 'completed'ë¡œ ì—…ë°ì´íŠ¸
        for part_id in trained_parts:
            try:
                supabase.table('part_training_status').upsert({
                    'part_id': part_id,
                    'status': 'completed',
                    'map50': metrics.get('mAP50', 0.0),  # ì‹¤ì œ DB í•„ë“œëª…: map50
                    'precision': metrics.get('precision', 0.0),
                    'recall': metrics.get('recall', 0.0),
                    'last_trained_at': datetime.now().isoformat()
                }, {
                    'onConflict': 'part_id'
                }).execute()
                print(f"âœ… ë¶€í’ˆ {part_id} í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ë¶€í’ˆ {part_id} ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        
        print(f"ğŸ‰ ì´ {len(trained_parts)}ê°œ ë¶€í’ˆ í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ë¶€í’ˆ í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def train_yolo_model(dataset_yaml, config):
    """YOLO ëª¨ë¸ í•™ìŠµ"""
    print("ğŸš€ YOLO ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = 'cuda' if torch.cuda.is_available() and config.get('device') == 'cuda' else 'cpu'
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # GPU ì‚¬ìš© ë¶ˆê°€ ì‹œ ê²½ê³  ë° CPU ì„¤ì • ì¡°ì •
    if device == 'cpu':
        print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")
        print("ğŸ’¡ GPU ê°€ì†ì„ ì›í•œë‹¤ë©´ PyTorch CUDA ë²„ì „ì„ ì„¤ì¹˜í•˜ì„¸ìš”.")
        print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
    
    # í•˜ì´ë¸Œë¦¬ë“œ YOLO ëª¨ë¸ ì´ˆê¸°í™” (2ë‹¨ê³„ ì‹œìŠ¤í…œ)
    # 1ë‹¨ê³„: YOLO11n-seg (ë¹ ë¥¸ ìŠ¤ìº”)
    # 2ë‹¨ê³„: YOLO11s-seg (ì •ë°€ ê²€ì¦)
    model_stage = config.get('model_stage', 'stage1')  # stage1 ë˜ëŠ” stage2
    
    if model_stage == 'stage1':
        model = YOLO('yolo11n-seg.pt')  # 1ë‹¨ê³„: ë¹ ë¥¸ ìŠ¤ìº”ìš©
        print("ğŸš€ 1ë‹¨ê³„ ëª¨ë¸ (YOLO11n-seg): ë¹ ë¥¸ ì „ì²´ ìŠ¤ìº”")
    else:
        model = YOLO('yolo11s-seg.pt')  # 2ë‹¨ê³„: ì •ë°€ ê²€ì¦ìš©
        print("ğŸ¯ 2ë‹¨ê³„ ëª¨ë¸ (YOLO11s-seg): ì •ë°€ ê²€ì¦")
    
    # ë‹¨ê³„ë³„ í•™ìŠµ ì„¤ì •
    if model_stage == 'stage1':
        # 1ë‹¨ê³„: ë¹ ë¥¸ ìŠ¤ìº”ìš© ì„¤ì •
        # CPU ì‚¬ìš© ì‹œ ë°°ì¹˜ í¬ê¸°ì™€ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
        batch_size = 4 if device == 'cpu' else config.get('batch_size', 32)
        imgsz = 416 if device == 'cpu' else config.get('imgsz', 640)
        
        training_args = {
            'data': dataset_yaml,
            'epochs': config.get('epochs', 80),  # 1ë‹¨ê³„ëŠ” ë” ì ì€ ì—í­
            'batch': batch_size,  # CPU ì‹œ ì‘ì€ ë°°ì¹˜
            'imgsz': imgsz,  # CPU ì‹œ ì‘ì€ ì´ë¯¸ì§€
            'device': device,
            'project': 'output/training',
            'name': f'brickbox_stage1_{config.get("set_num", "latest")}_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            'save': True,
            'plots': True,
            'val': True,
            'patience': 15,  # Early stopping
            'save_period': 10,
            'cache': True,
            'workers': 4,
            'optimizer': 'AdamW',
            'lr0': 0.01,
            'lrf': 0.01,
            'momentum': 0.937,
            'weight_decay': 0.0005,
            'warmup_epochs': 3,
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1,
            'box': 7.5,
            'cls': 0.5,
            'dfl': 1.5,
            'pose': 12.0,
            'kobj': 2.0,
            'label_smoothing': 0.0,
            # 1ë‹¨ê³„ ë°ì´í„° ì¦ê°• (ì†ë„ ìš°ì„ )
            'copy_paste': 0.4,  # ì ì€ ì¦ê°•
            'mosaic': 0.3,
            'fliplr': 0.5,
            'hsv_h': 0.5,
            'hsv_s': 0.3,
            'hsv_v': 0.3,
            'perspective': 0.0005,
            'erasing': 0.1,
            'nbs': 64,
            'overlap_mask': True,
            'mask_ratio': 4,
            'dropout': 0.0,
            # 'val_period': 1,  # YOLOì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ
            # 1ë‹¨ê³„ ì¶”ë¡  ì„¤ì • (ë‚®ì€ ì„ê³„ê°’)
            'conf': 0.3,  # ë‚®ì€ ì‹ ë¢°ë„ë¡œ í›„ë³´ ìˆ˜ì§‘
            'iou': 0.50,
            'max_det': 50,  # ì ì€ íƒì§€ ìˆ˜
        }
    else:
        # 2ë‹¨ê³„: ì •ë°€ ê²€ì¦ìš© ì„¤ì •
        # CPU ì‚¬ìš© ì‹œ ë°°ì¹˜ í¬ê¸°ì™€ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
        batch_size = 2 if device == 'cpu' else config.get('batch_size', 16)
        imgsz = 512 if device == 'cpu' else config.get('imgsz', 768)
        
        training_args = {
            'data': dataset_yaml,
            'epochs': config.get('epochs', 100),  # 2ë‹¨ê³„ëŠ” ë” ë§ì€ ì—í­
            'batch': batch_size,  # CPU ì‹œ ì‘ì€ ë°°ì¹˜
            'imgsz': imgsz,  # CPU ì‹œ ì‘ì€ ì´ë¯¸ì§€
            'device': device,
            'project': 'output/training',
            'name': f'brickbox_stage2_{config.get("set_num", "latest")}_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            'save': True,
            'plots': True,
            'val': True,
            'patience': 15,  # Early stopping
            'save_period': 10,
            'cache': True,
            'workers': 4,
            'optimizer': 'AdamW',
            'lr0': 0.01,
            'lrf': 0.01,
            'momentum': 0.937,
            'weight_decay': 0.0005,
            'warmup_epochs': 3,
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1,
            'box': 7.5,
            'cls': 0.5,
            'dfl': 1.5,
            'pose': 12.0,
            'kobj': 2.0,
            'label_smoothing': 0.0,
            # 2ë‹¨ê³„ ë°ì´í„° ì¦ê°• (ì •í™•ë„ ìš°ì„ )
            'copy_paste': 0.6,  # ë§ì€ ì¦ê°•
            'mosaic': 0.5,
            'fliplr': 0.5,
            'hsv_h': 0.7,
            'hsv_s': 0.4,
            'hsv_v': 0.4,
            'perspective': 0.001,
            'erasing': 0.2,
            'nbs': 64,
            'overlap_mask': True,
            'mask_ratio': 4,
            'dropout': 0.0,
            # 'val_period': 1,  # YOLOì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ
            # 2ë‹¨ê³„ ì¶”ë¡  ì„¤ì • (ë†’ì€ ì„ê³„ê°’)
            'conf': 0.5,  # ë†’ì€ ì‹ ë¢°ë„ë¡œ ì •í™•í•œ íŒì •
            'iou': 0.60,
            'max_det': 20,  # ì ì€ íƒì§€ ìˆ˜
        }
    
    print(f"ğŸ“Š í•™ìŠµ ì„¤ì •: {training_args}")
    
    # í•™ìŠµ ì‹œì‘
    start_time = time.time()
    results = model.train(**training_args)
    end_time = time.time()
    
    training_time = end_time - start_time
    print(f"â±ï¸ í•™ìŠµ ì™„ë£Œ ì‹œê°„: {training_time:.2f}ì´ˆ")
    
    return results, model

def save_model(model, config):
    """í•™ìŠµëœ ëª¨ë¸ ì €ì¥"""
    print("ğŸ’¾ í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ì¤‘...")
    
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    model_dir = Path("public/models")
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # ONNX í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
    model_path = model_dir / f"lego_yolo_set_{config.get('set_num', 'latest')}.onnx"
    
    try:
        # ONNX í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
        model.export(format='onnx', imgsz=config.get('imgsz', 640))
        
        # ë‚´ë³´ë‚¸ ëª¨ë¸ì„ ëª©ì ì§€ë¡œ ì´ë™
        exported_path = model_dir / "yolo11n.onnx"
        if exported_path.exists():
            exported_path.rename(model_path)
            print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
            
            # ìë™ ì—…ë¡œë“œ ë° ë“±ë¡
            upload_and_register_model(model_path, config)
        else:
            print("âš ï¸ ONNX ë³€í™˜ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

def upload_and_register_model(model_path, config):
    """í•™ìŠµëœ ëª¨ë¸ì„ Supabase ìŠ¤í† ë¦¬ì§€ì— ì—…ë¡œë“œí•˜ê³  model_registryì— ë“±ë¡"""
    try:
        from supabase import create_client
        import os
        from datetime import datetime
        
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        supabase_url = os.getenv('SUPABASE_URL') or os.getenv('VITE_SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_ANON_KEY') or os.getenv('VITE_SUPABASE_ANON_KEY')
        
        if not supabase_url or not supabase_key:
            print("âš ï¸ Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ëª¨ë¸ ì—…ë¡œë“œ ìŠ¤í‚µ")
            return
            
        supabase = create_client(supabase_url, supabase_key)
        
        # ëª¨ë¸ íŒŒì¼ ì½ê¸°
        with open(model_path, 'rb') as f:
            model_data = f.read()
        
        # íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = f"brickbox_s_seg_{timestamp}"
        bucket_path = f"{model_name}.pt"
        
        # Supabase ìŠ¤í† ë¦¬ì§€ì— ì—…ë¡œë“œ
        print(f"ğŸ“¤ ëª¨ë¸ ì—…ë¡œë“œ ì¤‘: {bucket_path}")
        upload_result = supabase.storage.from_('models').upload(
            bucket_path,
            model_data,
            {
                'content-type': 'application/octet-stream',
                'upsert': True
            }
        )
        
        if upload_result.error:
            print(f"âŒ ëª¨ë¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {upload_result.error}")
            return
        
        # ê³µê°œ URL ìƒì„±
        public_url = supabase.storage.from_('models').get_public_url(bucket_path)
        
        # model_registryì— ë“±ë¡
        print(f"ğŸ“ ëª¨ë¸ ë“±ë¡ ì¤‘: {model_name}")
        model_size_mb = len(model_data) / (1024 * 1024)
        
        # ê¸°ì¡´ ëª¨ë¸ë“¤ì„ ë¹„í™œì„±í™”
        supabase.table('model_registry').update({
            'is_active': False,
            'status': 'inactive'
        }).execute()
        
        # ìƒˆ ëª¨ë¸ ë“±ë¡
        registry_result = supabase.table('model_registry').insert({
            'model_name': model_name,
            'version': '1.0.0',
            'model_url': public_url.public_url,
            'model_path': bucket_path,
            'pt_model_path': bucket_path,
            'is_active': True,
            'status': 'active',
            'model_type': 'yolo',
            'model_size_mb': round(model_size_mb, 2),
            'segmentation_support': True,
            'model_stage': 'single',
            'created_by': 'system',
            'training_metadata': {
                'set_num': config.get('set_num', 'latest'),
                'part_id': config.get('part_id'),
                'epochs': config.get('epochs', 100),
                'batch_size': config.get('batch_size', 16),
                'imgsz': config.get('imgsz', 640),
                'device': config.get('device', 'cuda')
            }
        }).execute()
        
        if registry_result.error:
            print(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {registry_result.error}")
        else:
            print(f"âœ… ëª¨ë¸ ì—…ë¡œë“œ ë° ë“±ë¡ ì™„ë£Œ: {model_name}")
            print(f"ğŸ”— ê³µê°œ URL: {public_url.public_url}")
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì—…ë¡œë“œ ë° ë“±ë¡ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='BrickBox YOLO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--set_num', default='latest', help='ë ˆê³  ì„¸íŠ¸ ë²ˆí˜¸')
    parser.add_argument('--part_id', help='ë¶€í’ˆ ID ë˜ëŠ” ì—˜ë¦¬ë¨¼íŠ¸ ID (ë¶€í’ˆ ë‹¨ìœ„ í•™ìŠµìš©)')
    parser.add_argument('--epochs', type=int, default=100, help='í•™ìŠµ ì—í­ ìˆ˜')
    parser.add_argument('--batch_size', type=int, default=24, help='ë°°ì¹˜ í¬ê¸° (16~32 ê¶Œì¥)')
    parser.add_argument('--imgsz', type=int, default=768, help='ì´ë¯¸ì§€ í¬ê¸° (768 ê¶Œì¥, 960 ê³ ì„±ëŠ¥)')
    parser.add_argument('--device', default='cuda', help='ì‚¬ìš© ë””ë°”ì´ìŠ¤')
    parser.add_argument('--job_id', help='í•™ìŠµ ì‘ì—… ID')
    parser.add_argument('--model_stage', choices=['stage1', 'stage2'], default='stage1', 
                       help='í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë‹¨ê³„ (stage1: YOLO11n-seg, stage2: YOLO11s-seg)')
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logger = setup_logging()
    
    # ì„¤ì • ì •ë³´
    config = {
        'set_num': args.set_num,
        'part_id': args.part_id,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'imgsz': args.imgsz,
        'device': args.device,
        'job_id': args.job_id,
        'model_stage': args.model_stage
    }
    
    print("ğŸ§± BrickBox YOLO í•™ìŠµ ì‹œì‘")
    print(f"ğŸ“Š ì„¤ì •: {config}")
    
    try:
        # 1. í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì‹œì‘)
        if args.job_id:
            update_training_status(args.job_id, 'running', {'status': 'í•™ìŠµ ì‹œì‘'})
        
        # 2. ID ë§¤í•‘ ì œê±°: ì „ë‹¬ë°›ì€ IDë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (element_id ë””ë ‰í† ë¦¬ ìš°ì„ )
        actual_part_id = args.part_id
        
        # 3. ë°ì´í„°ì…‹ ì¤€ë¹„ (ì¤‘ë³µ ë¶€í’ˆ ì œê±° í¬í•¨)
        dataset_yaml = prepare_dataset(args.set_num, actual_part_id)
        if not dataset_yaml:
            print("âŒ ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨")
            if args.job_id:
                update_training_status(args.job_id, 'failed', {'error': 'ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨'})
            sys.exit(1)
        
        # 4. YOLO ëª¨ë¸ í•™ìŠµ
        print("ğŸš€ YOLO ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        results, model = train_yolo_model(dataset_yaml, config)
        
        # 5. í•™ìŠµ ê²°ê³¼ ì²˜ë¦¬
        if results:
            print("âœ… í•™ìŠµ ì™„ë£Œ!")
            
            # ìµœì¢… ë©”íŠ¸ë¦­ ì¶”ì¶œ
            final_metrics = {
                'mAP50': getattr(results, 'box', {}).get('mAP50', 0.0),
                'mAP50_95': getattr(results, 'box', {}).get('mAP50_95', 0.0),
                'precision': getattr(results, 'box', {}).get('precision', 0.0),
                'recall': getattr(results, 'box', {}).get('recall', 0.0)
            }
            
            print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥:")
            for metric, value in final_metrics.items():
                print(f"  {metric}: {value:.4f}")
            
            # 6. ëª¨ë¸ ì €ì¥
            save_model(model, config)
            
            # 7. í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì™„ë£Œ)
            if args.job_id:
                update_training_status(args.job_id, 'completed', {
                    'status': 'í•™ìŠµ ì™„ë£Œ',
                    'final_metrics': final_metrics
                })
            
            # 8. ë¶€í’ˆ í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´)
            update_part_training_status(args.set_num, actual_part_id, final_metrics)
            
            print("ğŸ‰ BrickBox YOLO í•™ìŠµ ì™„ë£Œ!")
            
        else:
            print("âŒ í•™ìŠµ ì‹¤íŒ¨")
            if args.job_id:
                update_training_status(args.job_id, 'failed', {'error': 'í•™ìŠµ ì‹¤íŒ¨'})
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if args.job_id:
            update_training_status(args.job_id, 'failed', {'error': str(e)})
        sys.exit(1)

if __name__ == "__main__":
    main()