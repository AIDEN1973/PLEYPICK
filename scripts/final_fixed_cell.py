# YOLO11s-seg ëª¨ë¸ ì´ˆê¸°í™” (BrickBox íŠ¹ì„±ì— ìµœì í™”)
from ultralytics import YOLO
import torch
import datetime
import logging
import os

# ë¡œê¹… ì„¤ì • (plMdX3AQRAq2 ì…€ì—ì„œ ì´ë¯¸ ì„¤ì •ë˜ì—ˆì„ ìˆ˜ ìˆì§€ë§Œ, ì•ˆì „í•˜ê²Œ ë‹¤ì‹œ ì„¤ì •)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

device = 'cuda' if torch.cuda.is_available() else 'cpu'
logging.info(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

# YOLO11s-seg ëª¨ë¸ ì´ˆê¸°í™” (ê°ì²´ íƒì§€ + ê²½ëŸ‰í™” -> Segmentation + í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ)
model = YOLO('yolo11s-seg.pt')  # BrickBox ìµœì  ëª¨ë¸!

# í•™ìŠµ ì„¤ì • (BrickBox íŠ¹ì„± ê³ ë ¤)
epochs = 100
batch_size = 16
imgsz = 640

# ê³ ìœ í•œ í•™ìŠµ ì´ë¦„ ìƒì„±
training_name = f'brickbox_s_seg_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}'

logging.info(f"ğŸš€ YOLO11s-seg í•™ìŠµ ì‹œì‘:")
logging.info(f"  - Epochs: {epochs}")
logging.info(f"  - Batch Size: {batch_size}")
logging.info(f"  - Image Size: {imgsz}")
logging.info(f"  - Device: {device}")
logging.info(f"  - Training Name: {training_name}")
logging.info(f"  - ëª¨ë¸: YOLO11s-seg (Segmentation + í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ)")
logging.info(f"  - í•™ìŠµ-ì¶”ë¡  ì¼ì¹˜: âœ… ë™ì¼í•œ ì•„í‚¤í…ì²˜")
logging.info(f"  - í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ: 2ì°¨ ì •ë°€ ê²€ì¦ìš©")

# ì—í­ë³„ ë©”íŠ¸ë¦­ ì €ì¥ í•¨ìˆ˜ (í•™ìŠµ ì‹œì‘ ì „ì— ì •ì˜)
def save_epoch_metrics(trainer, job_id):
    if not job_id:
        return

    try:
        metrics = trainer.metrics
        epoch = trainer.epoch

        # training_metrics í…Œì´ë¸”ì— ì €ì¥
        # Note: Segmentation metrics keys might be different (e.g., metrics/mAP50(M), metrics/mAP50-95(M))
        # Need to confirm actual keys from Ultralytics segmentation training results.
        # Using detection keys for now, will need adjustment if segmentation keys are used.
        supabase.table('training_metrics').insert({
            'training_job_id': job_id,
            'epoch': epoch,
            'metrics': {
                'loss': metrics.get('train/box_loss', 0.0), # Use train/box_loss etc. from results_dict keys
                'cls_loss': metrics.get('train/cls_loss', 0.0),
                'dfl_loss': metrics.get('train/dfl_loss', 0.0),
                'mAP50_B': metrics.get('metrics/mAP50(B)', 0.0), # Keep detection keys for compatibility/comparison
                'mAP50_95_B': metrics.get('metrics/mAP50-95(B)', 0.0),
                'precision_B': metrics.get('metrics/precision(B)', 0.0),
                'recall_B': metrics.get('metrics/recall(B)', 0.0),
                # Add segmentation metrics keys if available and needed
                'mAP50_M': metrics.get('metrics/mAP50(M)', 0.0), # Segmentation mAP50
                'mAP50_95_M': metrics.get('metrics/mAP50-95(M)', 0.0), # Segmentation mAP50-95
            },
            'created_at': datetime.datetime.now().isoformat()
        }).execute()

        logging.info(f"ğŸ“Š ì—í­ {epoch} ë©”íŠ¸ë¦­ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        logging.warning(f"âš ï¸ ì—í­ ë©”íŠ¸ë¦­ ì €ì¥ ì‹¤íŒ¨: {e}")

# ì‹¤ì œ í•™ìŠµ ì‹¤í–‰
logging.info("ğŸ“Š í•™ìŠµ ì‹œì‘...")

# ğŸ”§ ìˆ˜ì •: job_id ì´ˆê¸°í™” ë° ì•ˆì „í•œ ì²˜ë¦¬
job_id = None
job_config_from_db = {}
set_num_from_db = None

# í•™ìŠµ ì‹œì‘ ì‹œ training_jobs ìƒíƒœ ì—…ë°ì´íŠ¸
try:
    # ìµœê·¼ ìƒì„±ëœ pending ì‘ì—…ì„ runningìœ¼ë¡œ ë³€ê²½
    pending_jobs = supabase.table('training_jobs').select('id, config').eq('status', 'pending').order('created_at', desc=True).limit(1).execute()
    if pending_jobs.data and len(pending_jobs.data) > 0:
        job_id = pending_jobs.data[0]['id']
        job_config_from_db = pending_jobs.data[0].get('config', {})
        set_num_from_db = job_config_from_db.get('set_num') # Get set_num from job config

        supabase.table('training_jobs').update({
            'status': 'running',
            'started_at': datetime.datetime.now().isoformat(),
            'colab_session_id': f'colab_session_{int(datetime.datetime.now().timestamp() * 1000)}',
            'config': {**job_config_from_db, 'training_name': training_name, 'model_type': 'yolo11s-seg'} # Add training_name and model_type to config
        }).eq('id', job_id).execute()
        logging.info(f"âœ… í•™ìŠµ ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸: {job_id} â†’ running")
    else:
        logging.warning("âš ï¸ ìƒíƒœë¥¼ 'running'ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•  pending í•™ìŠµ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
        logging.info("ğŸ’¡ ìƒˆ í•™ìŠµ ì‘ì—… ìƒì„± ë˜ëŠ” ìˆ˜ë™ ì‹œì‘ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # ğŸ”§ ìˆ˜ì •: job_idê°€ ì—†ì„ ë•Œ ê¸°ë³¸ê°’ ì„¤ì •
        job_id = None

except Exception as e:
    logging.error(f"âŒ í•™ìŠµ ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    job_id = None # Ensure job_id is None if update fails

# Attach callback for epoch metrics saving if job_id is available
if job_id:
    logging.info("ğŸ“Š ì—í­ë³„ ë©”íŠ¸ë¦­ ì €ì¥ì€ í˜„ì¬ ì½”ë“œ êµ¬ì¡°ì—ì„œ ì§ì ‘ ì½œë°±ìœ¼ë¡œ êµ¬í˜„í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ìµœì¢… ê²°ê³¼ë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
else:
    logging.info("ğŸ“Š job_idê°€ ì—†ì–´ ì—í­ë³„ ë©”íŠ¸ë¦­ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

# Initialize final_metrics before the training call
final_metrics = {}

# í•™ìŠµ ì‹œì‘
results = model.train(
    data='/content/brickbox_dataset/dataset.yaml',
    epochs=epochs,
    batch=batch_size,
    imgsz=imgsz,
    device=device,
    project='brickbox_yolo',
    name=training_name,
    save=True,
    plots=True,
    val=True,
)

logging.info("âœ… í•™ìŠµ ì™„ë£Œ!")

# í•™ìŠµ ì™„ë£Œ ì‹œ training_jobs ìƒíƒœ ì—…ë°ì´íŠ¸
try:
    if job_id: # Use the job_id obtained before training
        # Get final metrics from results object
        # Segmentation metrics keys might be different (e.g., metrics/mAP50(M), metrics/mAP50-95(M))
        final_metrics = {
            'mAP50_B': float(results.results_dict.get('metrics/mAP50(B)', 0.0)),
            'mAP50_95_B': float(results.results_dict.get('metrics/mAP50-95(B)', 0.0)),
            'precision_B': float(results.results_dict.get('metrics/precision(B)', 0.0)),
            'recall_B': float(results.results_dict.get('metrics/recall(B)', 0.0)),
            'mAP50_M': float(results.results_dict.get('metrics/mAP50(M)', 0.0)), # Segmentation mAP50
            'mAP50_95_M': float(results.results_dict.get('metrics/mAP50-95(M)', 0.0)), # Segmentation mAP50-95
        }

        # Get trained parts list from the 'data' variable if it exists
        trained_parts_list = []
        if 'data' in globals() and isinstance(globals()['data'], list):
             trained_parts_list = [item['part_id'] for item in globals()['data']]
             logging.info(f"ğŸ“Š í•™ìŠµëœ ê³ ìœ  ë¶€í’ˆ ìˆ˜ (ë°ì´í„° ë³€ìˆ˜ì—ì„œ ì¶”ì¶œ): {len(set(trained_parts_list))}")
        else:
             logging.warning("âš ï¸ 'data' ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í˜•ì‹ì´ ë‹¤ë¦…ë‹ˆë‹¤. í•™ìŠµëœ ë¶€í’ˆ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        update_data = {
            'status': 'completed',
            'completed_at': datetime.datetime.now().isoformat(),
            'progress': {
                'final_epoch': epochs,
                'final_metrics': final_metrics
            },
            'config': {**job_config_from_db, 'training_name': training_name, 'model_type': 'yolo11s-seg'} # Update config again on completion
        }

        # Conditionally add trained_parts based on training mode (set_num_from_db from job config)
        if set_num_from_db:
             logging.info(f"ğŸ¯ ì„¸íŠ¸ ë‹¨ìœ„ í•™ìŠµ ì™„ë£Œ: trained_parts ëª©ë¡ì„ ì €ì¥í•©ë‹ˆë‹¤.")
             update_data['config'] = {**update_data['config'], 'trained_parts': list(set(trained_parts_list))} # Save unique parts
        else:
             logging.info("ğŸ“Š ì „ì²´ ë°ì´í„° í•™ìŠµ ì™„ë£Œ: trained_parts ëª©ë¡ì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
             # Optionally save count for full dataset training
             update_data['config'] = {**update_data['config'], 'trained_parts_count': len(set(trained_parts_list))}

        supabase.table('training_jobs').update(update_data).eq('id', job_id).execute()
        logging.info(f"âœ… í•™ìŠµ ì‘ì—… ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸: {job_id} â†’ completed")

        # ì„¸íŠ¸ ë‹¨ìœ„ í•™ìŠµ ì™„ë£Œ ì‹œ set_training_status ì—…ë°ì´íŠ¸
        if set_num_from_db:
            logging.info(f"ğŸ¯ ì„¸íŠ¸ {set_num_from_db} í•™ìŠµ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤‘...")

            set_status_response = supabase.table('set_training_status').update({
                'status': 'completed',
                'trained_at': datetime.datetime.now().isoformat(),
                'unique_parts_trained': len(set(trained_parts_list)),
                'is_available_for_inspection': True
            }).eq('set_num', set_num_from_db).execute()

            if set_status_response.data:
                logging.info(f"âœ… ì„¸íŠ¸ {set_num_from_db} í•™ìŠµ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì„±ê³µ")
                logging.info(f"   - í•™ìŠµëœ ê³ ìœ  ë¶€í’ˆ ìˆ˜: {len(set(trained_parts_list))}ê°œ")
                logging.info(f"   - ê²€ìˆ˜ ê°€ëŠ¥ ìƒíƒœ: í™œì„±í™”")
            else:
                logging.warning(f"âš ï¸ ì„¸íŠ¸ {set_num_from_db} ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")

except Exception as e:
    logging.error(f"âŒ í•™ìŠµ ì‘ì—… ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

# ğŸš€ ìë™ ì‹¤í–‰: Cell 5, 6 ì‹¤í–‰
logging.info("ğŸ”„ ìë™ ì‹¤í–‰: Cell 5 (í•™ìŠµ ê²°ê³¼ ë¶„ì„) ì‹œì‘...")

# Cell 5: í•™ìŠµ ê²°ê³¼ ë¶„ì„
logging.info("ğŸ“Š í•™ìŠµ ê²°ê³¼ ë¶„ì„:")
logging.info(f"  - ìµœì¢… mAP50(Box): {final_metrics.get('mAP50_B', 'N/A')}")
logging.info(f"  - ìµœì¢… mAP50-95(Box): {final_metrics.get('mAP50_95_B', 'N/A')}")
logging.info(f"  - ìµœì¢… Precision(Box): {final_metrics.get('precision_B', 'N/A')}")
logging.info(f"  - ìµœì¢… Recall(Box): {final_metrics.get('recall_B', 'N/A')}")
logging.info(f"  - ìµœì¢… mAP50(Mask): {final_metrics.get('mAP50_M', 'N/A')}")
logging.info(f"  - ìµœì¢… mAP50-95(Mask): {final_metrics.get('mAP50_95_M', 'N/A')}")

# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ í™•ì¸
best_model_path = f'/content/brickbox_yolo/{training_name}/weights/best.pt'
logging.info(f"âœ… ìµœì  ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {best_model_path}")

# ëª¨ë¸ ê²€ì¦
logging.info("ğŸ” ëª¨ë¸ ê²€ì¦ ì¤‘...")
try:
    validation_results = model.val(data='/content/brickbox_dataset/dataset.yaml')
    logging.info("âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ!")
    # Optionally log validation metrics as well
    logging.info(f"  - ê²€ì¦ mAP50(Box): {validation_results.results_dict.get('metrics/mAP50(B)', 'N/A')}")
    logging.info(f"  - ê²€ì¦ mAP50-95(Box): {validation_results.results_dict.get('metrics/mAP50-95(B)', 'N/A')}")
    logging.info(f"  - ê²€ì¦ mAP50(Mask): {validation_results.results_dict.get('metrics/mAP50(M)', 'N/A')}")
    logging.info(f"  - ê²€ì¦ mAP50-95(Mask): {validation_results.results_dict.get('metrics/mAP50-95(M)', 'N/A')}")

except Exception as e:
    logging.error(f"âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")

# ğŸš€ ìë™ ì‹¤í–‰: Cell 6 (ONNX ë³€í™˜ ë° ì—…ë¡œë“œ) ì‹œì‘
logging.info("ğŸ”„ ìë™ ì‹¤í–‰: Cell 6 (ONNX ë³€í™˜ ë° ì—…ë¡œë“œ) ì‹œì‘...")

# Cell 6: ONNX ë³€í™˜ ë° Supabase ì—…ë¡œë“œ
logging.info("ğŸš€ ONNX ë³€í™˜ ì¤‘...")
onnx_model_path = f'/content/brickbox_yolo/{training_name}/weights/best.onnx'
try:
    # Export format for segmentation models might be different or have specific requirements
    # Using the general 'onnx' format which usually works for detection+segmentation
    model.export(format='onnx', imgsz=640, optimize=True)
    logging.info(f"âœ… ONNX ë³€í™˜ ì™„ë£Œ: {onnx_model_path}")
except Exception as e:
    logging.error(f"âŒ ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
    onnx_model_path = None # Ensure onnx_model_path is None if conversion fails

# Supabaseì— ëª¨ë¸ ì—…ë¡œë“œ
logging.info("ğŸ“¤ Supabaseì— ëª¨ë¸ ì—…ë¡œë“œ ì¤‘...")

# ëª¨ë¸ íŒŒì¼ì„ Supabase Storageì— ì—…ë¡œë“œ
pt_upload_success = False
onnx_upload_success = False

try:
    # PyTorch ëª¨ë¸ ì—…ë¡œë“œ
    if os.path.exists(best_model_path):
        with open(best_model_path, 'rb') as f:
            pt_model_data = f.read()

        # ğŸ”§ ìˆ˜ì •: models ë²„í‚· ì‚¬ìš©, upsert ì˜µì…˜ì„ ë¬¸ìì—´ë¡œ ë³€ê²½
        pt_response = supabase.storage.from_('models').upload(
            f'{training_name}/best.pt',
            pt_model_data,
            {'content-type': 'application/octet-stream', 'upsert': 'true'}
        )
        logging.info("âœ… PyTorch ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ!")
        pt_upload_success = True
    else:
        logging.warning(f"âš ï¸ PyTorch ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {best_model_path}")

    # ONNX ëª¨ë¸ ì—…ë¡œë“œ
    if onnx_model_path and os.path.exists(onnx_model_path):
        with open(onnx_model_path, 'rb') as f:
            onnx_model_data = f.read()

        # ğŸ”§ ìˆ˜ì •: models ë²„í‚· ì‚¬ìš©, upsert ì˜µì…˜ì„ ë¬¸ìì—´ë¡œ ë³€ê²½
        onnx_response = supabase.storage.from_('models').upload(
            f'{training_name}/best.onnx',
            onnx_model_data,
            {'content-type': 'application/octet-stream', 'upsert': 'true'}
        )
        logging.info("âœ… ONNX ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ!")
        onnx_upload_success = True
    else:
         logging.warning(f"âš ï¸ ONNX ëª¨ë¸ íŒŒì¼ì´ ì—†ê±°ë‚˜ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    logging.error(f"âŒ ëª¨ë¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

# ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸
logging.info("ğŸ“‹ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸ ì¤‘...")
try:
    # Model type should reflect segmentation
    model_type = 'yolo11s-seg'
    # Use final_metrics for registration
    final_metrics_for_registry = {
        'mAP50_B': final_metrics.get('mAP50_B', 0.0),
        'mAP50_95_B': final_metrics.get('mAP50_95_B', 0.0),
        'precision_B': final_metrics.get('precision_B', 0.0),
        'recall_B': final_metrics.get('recall_B', 0.0),
        'mAP50_M': final_metrics.get('mAP50_M', 0.0),
        'mAP50_95_M': final_metrics.get('mAP50_95_M', 0.0),
    }

    # Model versioning could be improved, using training_name for uniqueness now
    model_version = training_name # Using training name as version for now

    # ğŸ”§ ìˆ˜ì •: job_idê°€ Noneì¼ ë•Œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    set_num_from_db_reg = None
    trained_parts_list_reg = []
    
    if job_id:  # job_idê°€ ìˆì„ ë•Œë§Œ ì¿¼ë¦¬ ì‹¤í–‰
        try:
            jobs_response_reg = supabase.table('training_jobs').select('config').eq('id', job_id).single().execute()
            if jobs_response_reg.data and 'config' in jobs_response_reg.data:
                 set_num_from_db_reg = jobs_response_reg.data['config'].get('set_num')
                 # If trained_parts were saved in config on completion, retrieve them
                 trained_parts_list_reg = jobs_response_reg.data['config'].get('trained_parts', [])
                 if not trained_parts_list_reg and 'trained_parts_count' in jobs_response_reg.data['config']:
                      # If count was saved for full dataset, indicate it
                      trained_parts_list_reg = f"Full dataset ({jobs_response_reg.data['config']['trained_parts_count']} parts)"

        except Exception as e:
            logging.warning(f"âš ï¸ Failed to retrieve job config for model registry: {e}")
    else:
        logging.info("ğŸ“Š job_idê°€ ì—†ì–´ í•™ìŠµ ì‘ì—… ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ğŸ”§ ìˆ˜ì •: ëª¨ë¸ ê²½ë¡œì—ì„œ models/ ì ‘ë‘ì‚¬ ì œê±°, ì¶”ê°€ í•„ë“œ í¬í•¨
    model_registry_data = {
        'model_name': f'brickbox_yolo_{model_type}', # Use model_type in name
        'model_version': model_version, # Use dynamic version
        'model_type': model_type,
        'model_path': f'{training_name}/best.onnx' if onnx_upload_success else None, # ğŸ”§ ìˆ˜ì •: models/ ì ‘ë‘ì‚¬ ì œê±°
        'pt_model_path': f'{training_name}/best.pt' if pt_upload_success else None, # ğŸ”§ ìˆ˜ì •: models/ ì ‘ë‘ì‚¬ ì œê±°
        'training_job_id': job_id if job_id else None, # ğŸ”§ ìˆ˜ì •: job_idê°€ Noneì¼ ë•Œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        'performance_metrics': final_metrics_for_registry, # Use final_metrics_for_registry
        'is_active': True, # Consider logic for setting active model
        'created_at': datetime.datetime.now().isoformat(),
        # ì„¸íŠ¸ ë‹¨ìœ„ í•™ìŠµ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        'training_metadata': {
            'set_num': set_num_from_db_reg, # Use retrieved set_num
            'training_mode': 'set_based' if set_num_from_db_reg else 'full_dataset',
             # Store list for set_based, count/indicator for full dataset
            'trained_parts': trained_parts_list_reg # Use retrieved trained_parts info
        },
        # ğŸ”§ ì¶”ê°€ í•„ë“œë“¤ (ìŠ¤í‚¤ë§ˆì— ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•¨)
        'model_size_mb': round(os.path.getsize(best_model_path) / (1024*1024), 2) if os.path.exists(best_model_path) else 0.0,
        'segmentation_support': True,
        'model_stage': 'single'
    }

    # ğŸ”§ ìˆ˜ì •: RLS ì •ì±… ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ ì‚½ì…
    try:
        registry_response = supabase.table('model_registry').insert(model_registry_data).execute()
        logging.info("âœ… ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
    except Exception as e:
        logging.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        logging.info("ğŸ’¡ ì´ ì˜¤ë¥˜ëŠ” RLS ì •ì±… ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

except Exception as e:
    logging.error(f"âŒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

logging.info("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
logging.info("âœ… Cell 4: YOLO11s-seg í•™ìŠµ ì™„ë£Œ")
logging.info("âœ… Cell 5: í•™ìŠµ ê²°ê³¼ ë¶„ì„ ì™„ë£Œ")
logging.info("âœ… Cell 6: ONNX ë³€í™˜ ë° ì—…ë¡œë“œ ì™„ë£Œ")
