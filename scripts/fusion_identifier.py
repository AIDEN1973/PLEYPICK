#!/usr/bin/env python3 """ BrickBox Fusion Identifier Two-Stage FAISS + Fusion + Hungarian + BOM 매칭 """ import os import sys import json import time import logging from datetime import datetime from typing import Dict, List, Optional, Tuple import numpy as np from scipy.optimize import linear_sum_assignment # 로깅 설정 logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') logger = logging.getLogger(__name__) class FusionIdentifier: """Fusion 식별 워커""" def __init__(self, supabase_client=None, faiss_index_path=None): self.supabase = supabase_client self.faiss_index_path = faiss_index_path self.faiss_index = None self.clip_embeddings = {} self.fgc_embeddings = {} def initialize_faiss_index(self): """FAISS 인덱스 초기화 (Manifest 우선)""" try: import faiss # Manifest 기반 로드 시도 if self.faiss_index_path and os.path.exists(self.faiss_index_path): return self.load_faiss_index() else: # 빈 인덱스 생성 return self._build_empty_indexes() except ImportError: logger.error(" FAISS 없음 - pip install faiss-cpu") return False except Exception as e: logger.error(f" FAISS 인덱스 초기화 실패: {e}") return False def _build_empty_indexes(self): """빈 FAISS 인덱스 생성""" try: import faiss # 새 인덱스 생성 (CLIP: 512차원, FGC: 2048차원) self.clip_index = faiss.IndexFlatIP(512) # Inner Product self.fgc_index = faiss.IndexFlatIP(2048) # Inner Product # part_id 배열 초기화 self.part_ids_clip = np.array([]) self.part_ids_fgc = np.array([]) logger.info(" 빈 FAISS 인덱스 생성") return True except Exception as e: logger.error(f" 빈 인덱스 생성 실패: {e}") return False def load_embeddings_from_db(self): """데이터베이스에서 임베딩 로드""" try: if not self.supabase: logger.error(" Supabase 클라이언트 없음") return False # 임베딩 데이터 조회 result = self.supabase.table('parts_master_features').select('*').not_.is_('clip_vector_id', 'null').execute() if not result.data: logger.warning(" 임베딩 데이터 없음") return False logger.info(f" {len(result.data)}개 임베딩 로드 중...") for row in result.data: part_id = row['part_id'] clip_vector_id = row['clip_vector_id'] fgc_vector_id = row['fgc_vector_id'] # 실제 벡터 로드 (외부 저장소에서) try: # CLIP 벡터 로드 clip_path = f"vectors/clip/{clip_vector_id}.npy" if os.path.exists(clip_path): clip_vector = np.load(clip_path).astype(np.float32) # L2 정규화 clip_vector = clip_vector / np.linalg.norm(clip_vector) self.clip_embeddings[part_id] = clip_vector else: logger.warning(f" CLIP 벡터 파일 없음: {clip_path}") continue # FGC 벡터 로드 fgc_path = f"vectors/fgc/{fgc_vector_id}.npy" if os.path.exists(fgc_path): fgc_vector = np.load(fgc_path).astype(np.float32) # L2 정규화 fgc_vector = fgc_vector / np.linalg.norm(fgc_vector) self.fgc_embeddings[part_id] = fgc_vector else: logger.warning(f" FGC 벡터 파일 없음: {fgc_path}") continue except Exception as e: logger.error(f" 벡터 로드 실패 {part_id}: {e}") continue logger.info(f" 임베딩 로드 완료: {len(self.clip_embeddings)}개") return True except Exception as e: logger.error(f" 임베딩 로드 실패: {e}") return False def build_faiss_index(self): """FAISS 인덱스 구축 및 지속화""" try: if not self.clip_embeddings or not self.fgc_embeddings: logger.error(" 임베딩 데이터 없음") return False # CLIP 인덱스 구축 clip_vectors = np.array(list(self.clip_embeddings.values())) self.clip_index.add(clip_vectors) # FGC 인덱스 구축 fgc_vectors = np.array(list(self.fgc_embeddings.values())) self.fgc_index.add(fgc_vectors) # 인덱스 지속화 (저장) index_version = self._save_faiss_indexes() # 매니페스트 업데이트 self._update_index_manifest(index_version) logger.info(f" FAISS 인덱스 구축 완료: CLIP={len(clip_vectors)}, FGC={len(fgc_vectors)}, 버전={index_version}") return True except Exception as e: logger.error(f" FAISS 인덱스 구축 실패: {e}") return False def _save_faiss_indexes(self) -> str: """FAISS 인덱스 저장""" try: import faiss import hashlib import time # 인덱스 디렉토리 생성 index_dir = "faiss_index" os.makedirs(index_dir, exist_ok=True) # 버전 생성 timestamp = int(time.time()) version = f"v{timestamp}" # CLIP 인덱스 저장 clip_path = f"{index_dir}/clip_index_{version}.faiss" faiss.write_index(self.clip_index, clip_path) # FGC 인덱스 저장 fgc_path = f"{index_dir}/fgc_index_{version}.faiss" faiss.write_index(self.fgc_index, fgc_path) # 해시 계산 clip_hash = self._calculate_file_hash(clip_path) fgc_hash = self._calculate_file_hash(fgc_path) logger.info(f" 인덱스 저장 완료: {version}") logger.info(f" CLIP: {clip_path} (hash: {clip_hash[:8]}...)") logger.info(f" FGC: {fgc_path} (hash: {fgc_hash[:8]}...)") return version except Exception as e: logger.error(f" 인덱스 저장 실패: {e}") return None def _calculate_file_hash(self, file_path: str) -> str: """파일 해시 계산""" try: with open(file_path, 'rb') as f: content = f.read() return hashlib.sha256(content).hexdigest() except Exception as e: logger.error(f" 파일 해시 계산 실패: {e}") return None def _update_index_manifest(self, version: str): """인덱스 매니페스트 업데이트""" try: if not self.supabase: logger.warning(" Supabase 클라이언트 없음 - 매니페스트 업데이트 건너뜀") return # model_registry에 인덱스 정보 저장 manifest_data = { 'model_type': 'faiss_index', 'model_version': version, 'index_manifest': { 'clip_index_path': f"faiss_index/clip_index_{version}.faiss", 'fgc_index_path': f"faiss_index/fgc_index_{version}.faiss", 'clip_hash': self._calculate_file_hash(f"faiss_index/clip_index_{version}.faiss"), 'fgc_hash': self._calculate_file_hash(f"faiss_index/fgc_index_{version}.faiss"), 'created_at': datetime.now().isoformat(), 'vector_count': len(self.clip_embeddings) }, 'is_active': True } result = self.supabase.table('model_registry').upsert(manifest_data).execute() logger.info(f" 인덱스 매니페스트 업데이트: {version}") except Exception as e: logger.error(f" 매니페스트 업데이트 실패: {e}") def load_faiss_index(self, version: str = None): """저장된 FAISS 인덱스 로드""" try: import faiss if not version: # 최신 버전 로드 if self.supabase: result = self.supabase.table('model_registry').select('*').eq('model_type', 'faiss_index').eq('is_active', True).order('created_at', desc=True).limit(1).execute() if result.data: version = result.data[0]['model_version'] manifest = result.data[0]['index_manifest'] clip_path = manifest['clip_index_path'] fgc_path = manifest['fgc_index_path'] # 인덱스 로드 self.clip_index = faiss.read_index(clip_path) self.fgc_index = faiss.read_index(fgc_path) logger.info(f" 인덱스 로드 완료: {version}") return True logger.warning(" 저장된 인덱스 없음 - 새로 구축 필요") return False except Exception as e: logger.error(f" 인덱스 로드 실패: {e}") return False def two_stage_search(self, query_clip: np.ndarray, query_fgc: np.ndarray, k: int = 10) -> List[Tuple[str, float]]: """Two-Stage FAISS + Adaptive Fusion + Confusion Gate (수정됨)""" try: # Stage 1: CLIP 검색 (Top-5) clip_scores, clip_indices = self.clip_index.search(query_clip.reshape(1, -1), 5) top5_parts = [self.part_ids_clip[i] for i in clip_indices[0] if i < len(self.part_ids_clip)] # Confusion Gate: confusions가 Top-5에 없으면 Stage-2 진입 needs_stage2 = self._check_confusion_gate(query_clip, top5_parts) if needs_stage2: # Stage 2: 확장 검색 (Top-10) clip_scores, clip_indices = self.clip_index.search(query_clip.reshape(1, -1), 10) fgc_scores, fgc_indices = self.fgc_index.search(query_fgc.reshape(1, -1), 10) else: # Stage 1만 사용 fgc_scores, fgc_indices = self.fgc_index.search(query_fgc.reshape(1, -1), 5) # 인덱스→part_id 역매핑 후 part_id 단위로 통합 candidates = {} # CLIP 결과 처리 for score, idx in zip(clip_scores[0], clip_indices[0]): if idx < len(self.part_ids_clip): part_id = self.part_ids_clip[idx] candidates.setdefault(part_id, {})["clip"] = float(score) # FGC 결과 처리 for score, idx in zip(fgc_scores[0], fgc_indices[0]): if idx < len(self.part_ids_fgc): part_id = self.part_ids_fgc[idx] candidates.setdefault(part_id, {})["fgc"] = float(score) # Adaptive Fusion 계산 results = [] for part_id, scores in candidates.items(): clip_score = scores.get("clip", 0.0) fgc_score = scores.get("fgc", 0.0) # 누락된 modality는 0 또는 평균으로 보정 if "clip" not in scores: clip_score = 0.0 if "fgc" not in scores: fgc_score = 0.0 # Margin 기반 가중치 계산 margin = abs(clip_score - fgc_score) w_fgc = self._calculate_adaptive_weight(margin) w_clip = 1.0 - w_fgc # Adaptive Fusion 점수 fusion_score = w_clip * clip_score + w_fgc * fgc_score # Topo penalty & Area bonus 적용 topo_penalty = self._calculate_topo_penalty(part_id, query_clip) area_bonus = self._calculate_area_bonus(part_id, query_clip) final_score = fusion_score - topo_penalty + area_bonus results.append((part_id, final_score)) # 점수순 정렬 results.sort(key=lambda x: x[1], reverse=True) return results[:k] except Exception as e: logger.error(f" Two-Stage 검색 실패: {e}") return [] def _check_confusion_gate(self, query_embedding: np.ndarray, top5_parts: List[str]) -> bool: """Confusion Gate: confusions가 Top-5에 없으면 Stage-2 진입""" try: if not self.supabase: return False # 실제 confusion 클래스 조회 confusion_classes = self._get_confusion_classes(query_embedding) # Top-5에 confusion 클래스가 포함되어 있는지 확인 has_confusion = any(part in confusion_classes for part in top5_parts) # confusion이 없으면 Stage-2 진입 (확장 검색) return not has_confusion except Exception as e: logger.error(f" Confusion Gate 체크 실패: {e}") return False def _get_confusion_classes(self, query_embedding: np.ndarray) -> List[str]: """쿼리 임베딩의 confusion 클래스 조회""" try: if not self.supabase: return [] # 실제로는 confusion 테이블에서 조회 # 임시로 시뮬레이션 confusion_threshold = 0.8 confusion_classes = [] # CLIP 점수가 높지만 FGC 점수가 낮은 경우 confusion 가능성 for part_id, clip_vector in self.clip_embeddings.items(): clip_sim = np.dot(query_embedding, clip_vector) if clip_sim > confusion_threshold: confusion_classes.append(part_id) return confusion_classes[:3] # 최대 3개 confusion 클래스 except Exception as e: logger.error(f" Confusion 클래스 조회 실패: {e}") return [] def _calculate_adaptive_weight(self, margin: float) -> float: """Margin 기반 Adaptive Weight 계산""" # 문서의 FGC 마진 곡선: w_fgc = clamp(0.3 + (0.2 - margin)*1.2, 0.3, 0.7) w_fgc = 0.3 + (0.2 - margin) * 1.2 return np.clip(w_fgc, 0.3, 0.7) def _calculate_topo_penalty(self, part_id: str, query_embedding: np.ndarray) -> float: """Topology penalty 계산""" # 실제로는 part_id의 메타데이터에서 topo_applicable 확인 # 임시로 0 반환 return 0.0 def _calculate_area_bonus(self, part_id: str, query_embedding: np.ndarray) -> float: """Area consistency bonus 계산""" # 실제로는 예상 면적과 실제 면적 비교 # 임시로 0 반환 return 0.0 def hungarian_matching(self, detections: List[Dict], candidates: List[Tuple[str, float]]) -> List[Dict]: """계층/희소 헝가리안 알고리즘""" try: if not detections or not candidates: return [] # 희소화: Top-3 또는 sim_final≥0.50만 비용 행렬에 포함 sparse_candidates = [] for part_id, score in candidates: if score >= 0.50 or len(sparse_candidates) < 3: sparse_candidates.append((part_id, score)) if not sparse_candidates: logger.warning(" 희소화 후 후보 없음") return [] # 계층 처리 high_conf = [] # ≥0.90: 즉시 할당 mid_conf = [] # 0.70~0.90: 배치 헝가리안 low_conf = [] # ≤0.70: 보류 for part_id, score in sparse_candidates: if score >= 0.90: high_conf.append((part_id, score)) elif score >= 0.70: mid_conf.append((part_id, score)) else: low_conf.append((part_id, score)) matches = [] # 1. 고확신 즉시 할당 (greedy) for i, detection in enumerate(detections): if high_conf: part_id, score = high_conf.pop(0) matches.append({ 'detection_id': detection.get('id', f'det_{i}'), 'part_id': part_id, 'confidence': score, 'assignment_type': 'greedy_high_conf' }) # 2. 중간 확신 배치 헝가리안 if mid_conf and len(detections) > len([m for m in matches if m['assignment_type'] == 'greedy_high_conf']): remaining_detections = detections[len(matches):] if remaining_detections and mid_conf: hungarian_matches = self._batch_hungarian(remaining_detections, mid_conf) matches.extend(hungarian_matches) # 3. 저확신 보류 (휴리스틱) for part_id, score in low_conf: if score >= 0.60: # 최소 임계치 matches.append({ 'detection_id': f'holdout_{len(matches)}', 'part_id': part_id, 'confidence': score, 'assignment_type': 'holdout' }) logger.info(f" 계층 헝가리안 완료: {len(matches)}개 (고확신: {len([m for m in matches if m.get('assignment_type') == 'greedy_high_conf'])}, 중간: {len([m for m in matches if m.get('assignment_type') == 'hungarian'])}, 보류: {len([m for m in matches if m.get('assignment_type') == 'holdout'])})") return matches except Exception as e: logger.error(f" 계층 헝가리안 실패: {e}") return [] def _batch_hungarian(self, detections: List[Dict], candidates: List[Tuple[str, float]]) -> List[Dict]: """배치 헝가리안 (타임아웃 포함)""" try: if not detections or not candidates: return [] # 비용 행렬 생성 (희소화된 후보만) cost_matrix = np.zeros((len(detections), len(candidates))) for i, detection in enumerate(detections): for j, (part_id, score) in enumerate(candidates): cost_matrix[i, j] = 1.0 - score # Hungarian 알고리즘 적용 (타임아웃 500ms) import time start_time = time.time() try: row_indices, col_indices = linear_sum_assignment(cost_matrix) # 타임아웃 체크 if time.time() - start_time > 0.5: # 500ms logger.warning(" 헝가리안 타임아웃 → greedy 폴백") return self._greedy_fallback(detections, candidates) except Exception as e: logger.warning(f" 헝가리안 실패 → greedy 폴백: {e}") return self._greedy_fallback(detections, candidates) # 매칭 결과 생성 matches = [] for i, j in zip(row_indices, col_indices): if j < len(candidates): part_id, score = candidates[j] matches.append({ 'detection_id': detections[i].get('id', f'det_{i}'), 'part_id': part_id, 'confidence': score, 'cost': cost_matrix[i, j], 'assignment_type': 'hungarian' }) return matches except Exception as e: logger.error(f" 배치 헝가리안 실패: {e}") return self._greedy_fallback(detections, candidates) def _greedy_fallback(self, detections: List[Dict], candidates: List[Tuple[str, float]]) -> List[Dict]: """Greedy 폴백 (헝가리안 실패 시)""" matches = [] used_candidates = set() for i, detection in enumerate(detections): best_score = 0 best_candidate = None for j, (part_id, score) in enumerate(candidates): if j not in used_candidates and score > best_score: best_score = score best_candidate = (j, part_id, score) if best_candidate: j, part_id, score = best_candidate used_candidates.add(j) matches.append({ 'detection_id': detection.get('id', f'det_{i}'), 'part_id': part_id, 'confidence': score, 'assignment_type': 'greedy_fallback' }) return matches def apply_bom_constraints(self, matches: List[Dict], bom_data: Dict) -> List[Dict]: """BOM 제약 조건 적용 (계층적 처리)""" try: if not bom_data: logger.warning(" BOM 데이터 없음") return matches # BOM 제약 조건 적용 constrained_matches = [] bom_counts = bom_data.get('expected_counts', {}) used_parts = set() # 1단계: BOM 필수 부품 우선 할당 for match in matches: part_id = match['part_id'] expected_count = bom_counts.get(part_id, 0) if expected_count > 0 and part_id not in used_parts: match['bom_expected'] = expected_count match['bom_valid'] = True match['bom_priority'] = 'required' constrained_matches.append(match) used_parts.add(part_id) # 2단계: BOM 선택적 부품 할당 for match in matches: part_id = match['part_id'] expected_count = bom_counts.get(part_id, 0) if expected_count == 0 and part_id not in used_parts: # BOM에 없는 부품이지만 높은 신뢰도면 경고로 포함 if match['confidence'] > 0.8: match['bom_expected'] = 0 match['bom_valid'] = False match['bom_warning'] = True match['bom_priority'] = 'optional' constrained_matches.append(match) used_parts.add(part_id) # 3단계: BOM 제약 위반 체크 bom_violations = self._check_bom_violations(constrained_matches, bom_counts) logger.info(f" BOM 제약 적용: {len(constrained_matches)}개 (위반: {len(bom_violations)}개)") return constrained_matches except Exception as e: logger.error(f" BOM 제약 적용 실패: {e}") return matches def _check_bom_violations(self, matches: List[Dict], bom_counts: Dict) -> List[Dict]: """BOM 제약 위반 체크""" try: violations = [] part_usage = {} # 부품 사용량 계산 for match in matches: part_id = match['part_id'] part_usage[part_id] = part_usage.get(part_id, 0) + 1 # BOM 제약 위반 확인 for part_id, expected_count in bom_counts.items(): actual_count = part_usage.get(part_id, 0) if actual_count > expected_count: violations.append({ 'part_id': part_id, 'expected': expected_count, 'actual': actual_count, 'violation_type': 'over_usage' }) elif actual_count < expected_count: violations.append({ 'part_id': part_id, 'expected': expected_count, 'actual': actual_count, 'violation_type': 'under_usage' }) return violations except Exception as e: logger.error(f" BOM 위반 체크 실패: {e}") return [] def process_identification(self, detections: List[Dict], bom_data: Dict = None) -> List[Dict]: """식별 처리""" try: logger.info(f" 식별 처리 시작: {len(detections)}개 탐지") results = [] for detection in detections: # 쿼리 임베딩 추출 (실제 구현에서는 detection에서 추출) query_clip = np.random.rand(512).astype(np.float32) # 임시 query_fgc = np.random.rand(2048).astype(np.float32) # 임시 # Two-Stage 검색 candidates = self.two_stage_search(query_clip, query_fgc, k=10) if not candidates: logger.warning(f" 후보 없음: {detection.get('id', 'unknown')}") continue # Hungarian 매칭 matches = self.hungarian_matching([detection], candidates) # BOM 제약 조건 적용 if bom_data: matches = self.apply_bom_constraints(matches, bom_data) results.extend(matches) logger.info(f" 식별 처리 완료: {len(results)}개 매칭") return results except Exception as e: logger.error(f" 식별 처리 실패: {e}") return [] def save_results(self, results: List[Dict], output_path: str): """결과 저장""" try: os.makedirs(os.path.dirname(output_path), exist_ok=True) with open(output_path, 'w', encoding='utf-8') as f: json.dump(results, f, ensure_ascii=False, indent=2) logger.info(f" 결과 저장: {output_path}") return True except Exception as e: logger.error(f" 결과 저장 실패: {e}") return False def main(): """메인 함수""" try: # 식별자 초기화 identifier = FusionIdentifier() # FAISS 인덱스 초기화 if not identifier.initialize_faiss_index(): logger.error(" FAISS 인덱스 초기화 실패") return False # 임베딩 로드 if not identifier.load_embeddings_from_db(): logger.error(" 임베딩 로드 실패") return False # FAISS 인덱스 구축 if not identifier.build_faiss_index(): logger.error(" FAISS 인덱스 구축 실패") return False # 테스트 탐지 데이터 test_detections = [ {'id': 'det_1', 'bbox': [100, 100, 200, 200], 'confidence': 0.9}, {'id': 'det_2', 'bbox': [300, 300, 400, 400], 'confidence': 0.8} ] # 식별 처리 results = identifier.process_identification(test_detections) # 결과 저장 output_path = "results/identification_results.json" identifier.save_results(results, output_path) logger.info(" Fusion Identifier 완료") return True except Exception as e: logger.error(f" 식별자 실행 실패: {e}") return False if __name__ == "__main__": main() 