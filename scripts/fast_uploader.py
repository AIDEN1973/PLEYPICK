#!/usr/bin/env python3 """ BrickBox 고속 업로더 (병렬 처리) - 동시 업로드 24개 (CPU/네트워크 최적화) - 청크 업로드 8MB - 중복 확인 최적화 - 배치 DB 업데이트 """ import os import sys import asyncio import aiohttp import aiofiles import time import hashlib from pathlib import Path from typing import List, Dict, Set from concurrent.futures import ThreadPoolExecutor import logging from supabase import create_client, Client # 로깅 설정 (최소화) logging.basicConfig(level=logging.WARNING) logger = logging.getLogger(__name__) class FastUploader: def __init__(self, supabase_url: str, service_key: str): self.supabase = create_client(supabase_url, service_key) self.session = None self.uploaded_files: Set[str] = set() self.failed_files: List[str] = [] # 성능 설정 self.max_concurrent = 24 # 동시 업로드 수 self.chunk_size = 8 * 1024 * 1024 # 8MB 청크 self.retry_count = 3 self.retry_delay = 1.0 async def __aenter__(self): self.session = aiohttp.ClientSession( timeout=aiohttp.ClientTimeout(total=60), connector=aiohttp.TCPConnector(limit=50, limit_per_host=30) ) return self async def __aexit__(self, exc_type, exc_val, exc_tb): if self.session: await self.session.close() def scan_files(self, base_dir: Path) -> List[Dict]: """파일 스캔 (최적화)""" items = [] extensions = {'.webp', '.png', '.jpg', '.jpeg', '.txt', '.json'} for file_path in base_dir.rglob('*'): if file_path.is_file() and file_path.suffix.lower() in extensions: part_id = file_path.parent.name file_type = self._get_file_type(file_path.suffix) items.append({ 'file_path': str(file_path), 'part_id': part_id, 'file_type': file_type, 'size': file_path.stat().st_size }) # 크기순 정렬 (큰 파일 먼저) items.sort(key=lambda x: x['size'], reverse=True) return items def _get_file_type(self, suffix: str) -> str: if suffix.lower() in {'.webp', '.png', '.jpg', '.jpeg'}: return 'image' elif suffix.lower() == '.txt': return 'annotation' elif suffix.lower() == '.json': return 'json' return 'other' async def check_existing_files(self, part_ids: Set[str]) -> Set[str]: """기존 파일 일괄 확인 (최적화)""" existing = set() try: for part_id in part_ids: try: files = self.supabase.storage.from_("lego-synthetic").list(f"synthetic/{part_id}") if files: for f in files: if f.get('name'): existing.add(f"synthetic/{part_id}/{f['name']}") except Exception as e: logger.warning(f"Part {part_id} 확인 실패: {e}") continue except Exception as e: logger.error(f"기존 파일 확인 실패: {e}") return existing async def upload_file(self, item: Dict, existing_files: Set[str]) -> bool: """단일 파일 업로드 (최적화)""" try: file_path = Path(item['file_path']) filename = file_path.name part_id = item['part_id'] supa_path = f"synthetic/{part_id}/{filename}" # 중복 확인 if supa_path in existing_files: logger.info(f"[SKIP] {filename} (중복)") return True # 파일 읽기 (청크 단위) data = await self._read_file_chunked(file_path) if not data: return False # Content-Type 결정 content_type = self._get_content_type(filename) # 업로드 실행 for attempt in range(self.retry_count): try: response = self.supabase.storage.from_("lego-synthetic").upload( supa_path, data, file_options={ "content-type": content_type, "upsert": True, "cache-control": "public, max-age=31536000" } ) if hasattr(response, 'error') and response.error: raise Exception(f"Upload error: {response.error}") logger.info(f"[PASS] {filename}") self.uploaded_files.add(supa_path) return True except Exception as e: if attempt < self.retry_count - 1: await asyncio.sleep(self.retry_delay * (2 ** attempt)) continue else: logger.error(f"[FAIL] {filename}: {e}") self.failed_files.append(filename) return False except Exception as e: logger.error(f"[ERROR] {item['file_path']}: {e}") self.failed_files.append(item['file_path']) return False async def _read_file_chunked(self, file_path: Path) -> bytes: """청크 단위 파일 읽기""" try: data = b'' async with aiofiles.open(file_path, 'rb') as f: while chunk := await f.read(self.chunk_size): data += chunk return data except Exception as e: logger.error(f"파일 읽기 실패 {file_path}: {e}") return b'' def _get_content_type(self, filename: str) -> str: """Content-Type 결정""" lower = filename.lower() if lower.endswith('.webp'): return 'image/webp' elif lower.endswith('.png'): return 'image/png' elif lower.endswith(('.jpg', '.jpeg')): return 'image/jpeg' elif lower.endswith('.json'): return 'application/json' else: return 'text/plain' async def upload_batch(self, items: List[Dict], existing_files: Set[str]) -> Dict: """배치 업로드 (병렬 처리)""" semaphore = asyncio.Semaphore(self.max_concurrent) async def upload_with_semaphore(item): async with semaphore: return await self.upload_file(item, existing_files) # 병렬 업로드 실행 tasks = [upload_with_semaphore(item) for item in items] results = await asyncio.gather(*tasks, return_exceptions=True) # 결과 집계 success_count = sum(1 for r in results if r is True) fail_count = len(results) - success_count return { 'success': success_count, 'failed': fail_count, 'total': len(items) } async def upload_directory(self, base_dir: Path) -> Dict: """디렉토리 전체 업로드""" print(f" 파일 스캔 중: {base_dir}") items = self.scan_files(base_dir) if not items: print(" 업로드할 파일이 없습니다") return {'success': 0, 'failed': 0, 'total': 0} print(f" 업로드 대상: {len(items)}개 파일") # Part ID 수집 part_ids = {item['part_id'] for item in items} print(f" 기존 파일 확인 중: {len(part_ids)}개 Part ID") # 기존 파일 확인 existing_files = await self.check_existing_files(part_ids) print(f" 기존 파일: {len(existing_files)}개") # 배치별 업로드 batch_size = 100 total_success = 0 total_failed = 0 for i in range(0, len(items), batch_size): batch = items[i:i + batch_size] batch_num = (i // batch_size) + 1 total_batches = (len(items) + batch_size - 1) // batch_size print(f" 배치 {batch_num}/{total_batches} 업로드 중 ({len(batch)}개)") start_time = time.time() result = await self.upload_batch(batch, existing_files) elapsed = time.time() - start_time total_success += result['success'] total_failed += result['failed'] print(f" 배치 {batch_num} 완료: {result['success']}개 성공, {result['failed']}개 실패 ({elapsed:.1f}초)") return { 'success': total_success, 'failed': total_failed, 'total': len(items) } async def main(): """메인 실행 함수""" if len(sys.argv) < 2: print("사용법: python fast_uploader.py <디렉토리경로>") sys.exit(1) base_dir = Path(sys.argv[1]) if not base_dir.exists(): print(f" 디렉토리가 존재하지 않습니다: {base_dir}") sys.exit(1) # Supabase 설정 supabase_url = os.getenv("VITE_SUPABASE_URL", "https://npferbxuxocbfnfbpcnz.supabase.co") service_key = os.getenv("VITE_SUPABASE_SERVICE_ROLE", "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im5wZmVyYnh1eG9jYmZuZmJwY256Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1OTQ3NDk4NSwiZXhwIjoyMDc1MDUwOTg1fQ.pPWhWrb4QBC-DT4dd6Y1p-LlHNd9UTKef3SHEXUDp00") print(" BrickBox 고속 업로더 시작") print(f"[DIR] 대상 디렉토리: {base_dir}") print(f" 동시 업로드: 24개") print(f" 청크 크기: 8MB") start_time = time.time() async with FastUploader(supabase_url, service_key) as uploader: result = await uploader.upload_directory(base_dir) elapsed = time.time() - start_time print(f"\n 업로드 완료!") print(f" 성공: {result['success']}개") print(f" 실패: {result['failed']}개") print(f" 총 파일: {result['total']}개") print(f"⏱️ 소요 시간: {elapsed:.1f}초") print(f" 평균 속도: {result['total']/elapsed:.1f} 파일/초") if result['failed'] > 0: print(f"\n 실패한 파일들:") for failed_file in uploader.failed_files[:10]: # 처음 10개만 표시 print(f" - {failed_file}") if len(uploader.failed_files) > 10: print(f" ... 및 {len(uploader.failed_files)-10}개 더") if __name__ == "__main__": asyncio.run(main()) 