import { ref } from 'vue'
import { supabase } from './useSupabase'
import { useEnhancedRecognition } from './useEnhancedRecognition'
import { usePartClassification } from './usePartClassification'

// LLM API ì„¤ì • (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµìš©)
const LLM_CONFIG = {
  apiKey: import.meta.env.VITE_OPENAI_API_KEY,
  baseUrl: 'https://api.openai.com/v1',
  model: 'gpt-4o-mini',
  maxTokens: 1000,
  temperature: 0.1
}

// í™˜ê²½ ë³€ìˆ˜ ë””ë²„ê¹… (í”„ë¡œë•ì…˜ì—ì„œë„ í‘œì‹œ)
console.log('ğŸ” Environment Debug:', {
  VITE_OPENAI_API_KEY: import.meta.env.VITE_OPENAI_API_KEY ? 'Present' : 'Missing',
  apiKey: LLM_CONFIG.apiKey ? 'Present' : 'Missing',
  allEnv: Object.keys(import.meta.env).filter(key => key.startsWith('VITE_')),
  // ì¶”ê°€ ë””ë²„ê¹… ì •ë³´
  importMetaEnv: import.meta.env,
  nodeEnv: import.meta.env.MODE,
  dev: import.meta.env.DEV,
  prod: import.meta.env.PROD
})

// í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì •: 1ì°¨(4o-mini) ê²°ê³¼ê°€ ëª¨í˜¸í•˜ë©´ 2ì°¨(4.1-mini)ë¡œ ë³´ê°•
const HYBRID_CONFIG = {
  enabled: false,
  secondaryModel: 'gpt-4.1-mini'
}

// OpenAI í…ìŠ¤íŠ¸ ì„ë² ë”© ì„¤ì • (ì‚¬ì „ ë¶„ì„ëœ feature_textìš©)
const CLIP_CONFIG = {
  apiKey: import.meta.env.VITE_OPENAI_API_KEY,
  baseUrl: 'https://api.openai.com/v1',
  model: 'text-embedding-3-small',
  dimensions: 768
}

// ê°œë³„ í•¨ìˆ˜ë“¤ì„ exportí•˜ê¸° ìœ„í•´ í•¨ìˆ˜ë“¤ì„ ë°–ìœ¼ë¡œ ì´ë™
// ì¬ì‹œë„ íšŸìˆ˜ ì¶”ì ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
let analysisRetryCount = new Map()

export async function analyzePartWithLLM(part, retryCount = 0) {
  try {
    // API í‚¤ ê²€ì¦
    if (!LLM_CONFIG.apiKey || LLM_CONFIG.apiKey === 'undefined') {
      console.warn('âš ï¸ OpenAI API key is missing, skipping LLM analysis')
      console.warn('ğŸ” Environment check:', {
        VITE_OPENAI_API_KEY: import.meta.env.VITE_OPENAI_API_KEY ? 'Present' : 'Missing',
        allEnv: Object.keys(import.meta.env).filter(key => key.startsWith('VITE_'))
      })
      return null // LLM ë¶„ì„ ìŠ¤í‚µ
    }
    
    // ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ì´ë¯¸ì§€ ë¶„ì„ ê°•ì œ)
    const MAX_RETRIES = 3
    const partKey = `${part.part_num || part.part?.part_num}_${part.color?.id || part.color_id}`
    
    if (retryCount >= MAX_RETRIES) {
      console.error(`âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ (${MAX_RETRIES}íšŒ): ${partKey}`)
      console.log(`ğŸ”„ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.`)
      return createTextOnlyAnalysis(part, part.part?.name || part.name, part.part_num || part.part?.part_num)
    }
    
    if (retryCount > 0) {
      console.log(`ğŸ”„ ì´ë¯¸ì§€ ë¶„ì„ ì¬ì‹œë„ ${retryCount}/${MAX_RETRIES}: ${partKey}`)
    }
    
    if (import.meta.env.DEV) {
      console.log('ë¶„ì„í•  ë¶€í’ˆ ì •ë³´:', part)
    }
    
    // ë¶€í’ˆ ì •ë³´ í™•ì¸ ë° ì •ë¦¬
    const partName = part.part?.name || part.name || 'Unknown'
    const partNum = part.part_num || part.part?.part_num || 'Unknown'
    const partImgUrl = part.part?.part_img_url || part.part_img_url || null
    const colorName = part.color?.name || part.color_name || 'Unknown'
    const colorId = part.color?.id ?? part.color_id ?? null
    const elementId = part.element_id || part.inv_part_id || null
    
    // ë ˆê³  ê³µì‹ ë¶€í’ˆë²ˆí˜¸ í™•ì¸ (external_idsì—ì„œ ì¶”ì¶œ)
    const externalIds = part.part?.external_ids || part.external_ids || {}
    const legoPartNumber = externalIds.lego || externalIds.Lego || null
    
    if (import.meta.env.DEV) {
      console.log('ì •ë¦¬ëœ ë¶€í’ˆ ì •ë³´:', { partName, partNum, partImgUrl, legoPartNumber })
    }
    
    // ì´ë¯¸ì§€ URLì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰
    if (!partImgUrl) {
      console.warn(`ë¶€í’ˆ ${partNum}ì˜ ì´ë¯¸ì§€ URLì´ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.`)
      return createTextOnlyAnalysis(part, partName, partNum)
    }
    
    // ì´ë¯¸ì§€ URL ê²€ì¦ ë° ìš°ì„ ìˆœìœ„ ì„¤ì •
    let finalImageUrl = partImgUrl
    
    // Supabase Storage ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸ (ìš°ì„ ìˆœìœ„ 1)
    if (part.supabase_image_url) {
      finalImageUrl = part.supabase_image_url
      console.log(`âœ… Supabase Storage ì´ë¯¸ì§€ ì‚¬ìš©: ${finalImageUrl}`)
    } else if (partImgUrl.includes('cdn.rebrickable.com')) {
      console.warn(`âš ï¸ Rebrickable CDN ì´ë¯¸ì§€ ì‚¬ìš©: ${partImgUrl}`)
      console.warn(`ì´ë¯¸ì§€ ë¶„ì„ì„ ê°•ì œë¡œ ì‹œë„í•©ë‹ˆë‹¤.`)
      // CDN ì´ë¯¸ì§€ë„ ë¶„ì„ ì‹œë„
    } else {
      console.log(`ğŸ“· ë‹¤ë¥¸ ì†ŒìŠ¤ ì´ë¯¸ì§€ ì‚¬ìš©: ${partImgUrl}`)
    }
    
    const prompt = `Analyze this LEGO part image carefully. Part: ${partName} (${partNum}). 

Focus on visual characteristics:
- Shape and geometry
- Stud pattern and connection points
- Unique visual features
- Size category (Duplo/System/Minifig/Technic)
- Color and surface details

Return JSON with detailed analysis:

{
  "shape": "detailed shape description",
  "center_stud": true/false,
  "groove": true/false,
  "connection": "connection type",
  "function": "main function",
  "feature_text": "comprehensive visual description",
  "recognition_hints": {
    "top_view": "detailed top view description",
    "side_view": "detailed side view description", 
    "unique_features": ["specific visual features"]
  },
  "similar_parts": ["similar part numbers"],
  "distinguishing_features": ["distinguishing visual features"],
  "stud_count_top": 0,
  "tube_count_bottom": 0,
  "size_category": "duplo|system|minifig|technic",
  "keypoints": ["important visual shape points"],
  "confusions": ["visually confusing similar parts"],
  "color_expectation": "observed color and surface details",
  "confidence": 0.95
}`

    const requestBody = {
      model: LLM_CONFIG.model,
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: prompt
            },
            {
              type: 'image_url',
              image_url: {
                url: partImgUrl,
                detail: 'high'
              }
            }
          ]
        }
      ],
      max_tokens: LLM_CONFIG.maxTokens,
      temperature: LLM_CONFIG.temperature,
      response_format: { type: 'json_object' }
    }

    if (import.meta.env.DEV) {
      console.log('API ìš”ì²­ ì •ë³´:', {
        model: LLM_CONFIG.model,
        apiKey: LLM_CONFIG.apiKey ? 'ì„¤ì •ë¨' : 'ì—†ìŒ',
        imageUrl: partImgUrl,
        promptLength: prompt.length
      })
    }

    const response = await fetch(`${LLM_CONFIG.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${LLM_CONFIG.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(requestBody)
    })

    if (!response.ok) {
      const errorText = await response.text()
      console.error('API ì˜¤ë¥˜ ì‘ë‹µ:', errorText)
      
      // ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ íƒ€ì„ì•„ì›ƒ ë¬¸ì œ í•´ê²°
      if (errorText.includes('Timeout while downloading') || errorText.includes('invalid_image_url')) {
        console.warn(`âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ íƒ€ì„ì•„ì›ƒ: ${finalImageUrl}`)
        console.warn(`ğŸ”„ ì´ë¯¸ì§€ URLì„ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤...`)
        
        // ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
        await new Promise(resolve => setTimeout(resolve, 2000))
        
        // ì¬ì‹œë„ ì‹œë„
        console.log(`ğŸ”„ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¬ì‹œë„ ì¤‘...`)
        return await analyzePartWithLLM(part, retryCount + 1) // ì¬ê·€ í˜¸ì¶œë¡œ ì¬ì‹œë„
      }
      
      // Rate limit ëŒ€ì‘ (ê°œì„ ëœ ë²„ì „)
      if (response.status === 429) {
        const errorData = JSON.parse(errorText)
        
        // retry_after í—¤ë” ìš°ì„  í™•ì¸, ì—†ìœ¼ë©´ ì‘ë‹µì—ì„œ ì¶”ì¶œ
        const retryAfterHeader = response.headers.get('retry-after')
        const retryAfterFromError = errorData.error?.retry_after
        const retryAfter = retryAfterHeader ? parseInt(retryAfterHeader) : (retryAfterFromError || 60)
        
        // ìµœì†Œ 60ì´ˆ, ìµœëŒ€ 300ì´ˆ ëŒ€ê¸°
        const waitTime = Math.min(Math.max(retryAfter, 60), 300)
        console.warn(`â³ Rate limit exceeded. Waiting ${waitTime} seconds...`)
        await new Promise(resolve => setTimeout(resolve, waitTime * 1000))
        
        // ì¬ì‹œë„ ì‹œë„
        console.log(`ğŸ”„ Rate limit ëŒ€ê¸° í›„ ì¬ì‹œë„ ì¤‘...`)
        return await analyzePartWithLLM(part, retryCount + 1) // ì¬ê·€ í˜¸ì¶œë¡œ ì¬ì‹œë„
        
        // ì¬ì‹œë„
        const retryResponse = await fetch(`${LLM_CONFIG.baseUrl}/chat/completions`, {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${LLM_CONFIG.apiKey}`,
            'Content-Type': 'application/json'
          },
          body: JSON.stringify(requestBody)
        })
        
        if (!retryResponse.ok) {
          const retryErrorText = await retryResponse.text()
          throw new Error(`LLM API Error (retry failed): ${retryResponse.status} - ${retryErrorText}`)
        }
        
        // ì¬ì‹œë„ ì„±ê³µ ì‹œ ì‘ë‹µ ì²˜ë¦¬
        const retryData = await retryResponse.json()
        if (!retryData.choices || !retryData.choices[0] || !retryData.choices[0].message) {
          console.error('ì¬ì‹œë„ ì‘ë‹µ êµ¬ì¡° ì˜¤ë¥˜:', retryData)
          console.log('ğŸ”„ ì¬ì‹œë„ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.')
          return createTextOnlyAnalysis(part, partName, partNum)
        }
        
        let retryParsed
        try {
          retryParsed = JSON.parse(retryData.choices[0].message.content)
        } catch (e) {
          const retryLlmResponse = retryData.choices[0].message.content || ''
          let retryJsonText = retryLlmResponse
          const retryJsonBlockMatch = retryLlmResponse.match(/```json\s*([\s\S]*?)\s*```/)
          if (retryJsonBlockMatch) {
            retryJsonText = retryJsonBlockMatch[1].trim()
          } else {
            const retryJsonObjectMatch = retryLlmResponse.match(/\{[\s\S]*\}/)
            if (retryJsonObjectMatch) retryJsonText = retryJsonObjectMatch[0]
          }
          try {
            retryParsed = JSON.parse(retryJsonText)
          } catch (err) {
            console.error('ì¬ì‹œë„ JSON íŒŒì‹± ì‹¤íŒ¨:', err)
            console.log('ğŸ”„ JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.')
            return createTextOnlyAnalysis(part, partName, partNum)
          }
        }
        
        retryParsed.part_num = partNum
        return retryParsed
      }
      
      throw new Error(`LLM API Error: ${response.status} - ${errorText}`)
    }

    const data = await response.json()
    if (import.meta.env.DEV) {
      console.log('LLM raw response:', data)
    }
    
    // ì‘ë‹µ êµ¬ì¡° í™•ì¸
    if (!data.choices || !data.choices[0] || !data.choices[0].message) {
      console.error('ì‘ë‹µ êµ¬ì¡° ì˜¤ë¥˜:', data)
      return null
    }
    
    // JSON ì‘ë‹µ ê°•ì œ ëª¨ë“œ: contentëŠ” JSON ë¬¸ìì—´ì´ì–´ì•¼ í•¨
    let parsed
    try {
      parsed = JSON.parse(data.choices[0].message.content)
    } catch (e) {
      // ì˜ˆì™¸ì ìœ¼ë¡œ í¬ë§·ì´ ì–´ê¸‹ë‚˜ëŠ” ê²½ìš° ê¸°ì¡´ íŒŒì„œë¡œ í´ë°±
      const llmResponse = data.choices[0].message.content || ''
      let jsonText = llmResponse
      const jsonBlockMatch = llmResponse.match(/```json\s*([\s\S]*?)\s*```/)
      if (jsonBlockMatch) {
        jsonText = jsonBlockMatch[1].trim()
      } else {
        const jsonObjectMatch = llmResponse.match(/\{[\s\S]*\}/)
        if (jsonObjectMatch) jsonText = jsonObjectMatch[0]
      }
      try {
        parsed = JSON.parse(jsonText)
      } catch (err) {
        console.error('JSON íŒŒì‹± ì‹¤íŒ¨:', err)
        return null
      }
    }

    // 1ì°¨ ê²°ê³¼
    parsed.part_num = partNum

    // ë©”íƒ€ë°ì´í„° ì •ê·œí™”(í˜•ì‹ ë³´ì • ë° í•„ìˆ˜ í•„ë“œ ë³´ì¡´)
    const normalizeArray = (v) => Array.isArray(v) ? v : (v ? [v] : [])
    const toBoolean = (v) => typeof v === 'boolean' ? v : (String(v).toLowerCase() === 'true')
    const toNumber = (v) => {
      const n = Number(v)
      return Number.isFinite(n) ? n : null
    }
    const normalizeAnalysis = (obj) => {
      const normalized = { ...obj }
      normalized.shape = typeof obj.shape === 'string' ? obj.shape : (obj.shape?.toString?.() || '')
      normalized.center_stud = toBoolean(obj.center_stud ?? false)
      normalized.groove = toBoolean(obj.groove ?? false)
      normalized.connection = typeof obj.connection === 'string' ? obj.connection : (obj.connection?.toString?.() || 'unknown')
      normalized.function = typeof obj.function === 'string' ? obj.function : (obj.function?.toString?.() || 'unknown')
      normalized.feature_text = typeof obj.feature_text === 'string' ? obj.feature_text : (obj.feature_text?.toString?.() || '')
      normalized.recognition_hints = {
        ...(obj.recognition_hints || {}),
        top_view: obj.recognition_hints?.top_view ?? '',
        side_view: obj.recognition_hints?.side_view ?? '',
        unique_features: normalizeArray(obj.recognition_hints?.unique_features)
      }
      normalized.similar_parts = normalizeArray(obj.similar_parts)
      normalized.distinguishing_features = normalizeArray(obj.distinguishing_features)
      normalized.keypoints = normalizeArray(obj.keypoints)
      normalized.confusions = normalizeArray(obj.confusions)
      normalized.color_expectation = (typeof obj.color_expectation === 'string' ? obj.color_expectation : (obj.color_expectation?.toString?.() || null))
      normalized.stud_count_top = toNumber(obj.stud_count_top)
      normalized.tube_count_bottom = toNumber(obj.tube_count_bottom)
      return normalized
    }
    parsed = normalizeAnalysis(parsed)

    // í•˜ì´ë¸Œë¦¬ë“œ ë³´ê°• íŠ¸ë¦¬ê±°: ë‚®ì€ confidence(<0.8), feature_text ì§§ìŒ(<40ì), key í•„ë“œ ëˆ„ë½ ì‹œ
    const needRefine = HYBRID_CONFIG.enabled && (
      (typeof parsed.confidence === 'number' && parsed.confidence < 0.8) ||
      (!parsed.feature_text || String(parsed.feature_text).length < 40) ||
      !parsed.recognition_hints || !parsed.distinguishing_features || !parsed.similar_parts ||
      (Array.isArray(parsed.keypoints) ? parsed.keypoints.length === 0 : true) ||
      (Array.isArray(parsed.confusions) ? parsed.confusions.length === 0 : true) ||
      (parsed.stud_count_top === null) || (parsed.tube_count_bottom === null) ||
      (!parsed.color_expectation)
    )

    if (!needRefine) return parsed

    // 2ì°¨ ëª¨ë¸(4.1-mini)ë¡œ ë³´ê°• ìš”ì²­
    const refinePrompt = `ë‹¤ìŒ JSONì„ ë³´ê°•í•˜ì„¸ìš”. ëˆ„ë½ëœ í•„ë“œë¥¼ ì±„ìš°ê³ , plate/brickì˜ stud/tube, ê°ë„(ìˆë‹¤ë©´), ìƒ‰ìƒëª…(í‘œì¤€ëª…), í˜¼ë™ë˜ëŠ” ìœ ì‚¬ë¶€í’ˆì„ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ í•˜ì„¸ìš”. JSONë§Œ ì‘ë‹µ.

ì›ë³¸ JSON:\n${JSON.stringify(parsed)}`

    const refineBody = {
      model: HYBRID_CONFIG.secondaryModel,
      messages: [ { role: 'user', content: [ { type: 'text', text: refinePrompt } ] } ],
      max_tokens: LLM_CONFIG.maxTokens,
      temperature: 0.2,
      response_format: { type: 'json_object' }
    }

    const refineResp = await fetch(`${LLM_CONFIG.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: { 'Authorization': `Bearer ${LLM_CONFIG.apiKey}`, 'Content-Type': 'application/json' },
      body: JSON.stringify(refineBody)
    })

    if (!refineResp.ok) return parsed
    let refined
    try {
      const refineData = await refineResp.json()
      refined = JSON.parse(refineData.choices[0].message.content)
    } catch {
      return parsed
    }

    // ë³‘í•©: 2ì°¨ ê°’ì´ ìˆìœ¼ë©´ ìš°ì„ , ì—†ìœ¼ë©´ 1ì°¨ ìœ ì§€ + ì •ê·œí™” ë³´ì¡´
    const mergedRaw = {
      ...parsed,
      ...refined,
      recognition_hints: { ...(parsed.recognition_hints || {}), ...(refined.recognition_hints || {}) },
      similar_parts: Array.isArray(refined?.similar_parts) && refined.similar_parts.length > 0 ? refined.similar_parts : parsed.similar_parts,
      distinguishing_features: Array.isArray(refined?.distinguishing_features) && refined.distinguishing_features.length > 0 ? refined.distinguishing_features : parsed.distinguishing_features,
      keypoints: Array.isArray(refined?.keypoints) && refined.keypoints.length > 0 ? refined.keypoints : parsed.keypoints,
      confusions: Array.isArray(refined?.confusions) && refined.confusions.length > 0 ? refined.confusions : parsed.confusions,
      stud_count_top: (refined?.stud_count_top ?? parsed.stud_count_top),
      tube_count_bottom: (refined?.tube_count_bottom ?? parsed.tube_count_bottom),
      color_expectation: (refined?.color_expectation ?? parsed.color_expectation)
    }

    const merged = normalizeAnalysis(mergedRaw)
    merged.part_num = partNum
    return merged
    
    } catch (error) {
      console.error('LLM ë¶„ì„ ì‹¤íŒ¨:', error)
      console.log('ğŸ”„ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.')
      return createTextOnlyAnalysis(part, partName, partNum)
    }
}

// ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ìƒì„± (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - LLM ì‹¤íŒ¨ ì‹œ null ë°˜í™˜)
// function createDefaultAnalysis(part, partName = null, partNum = null) {
//   const name = partName || part.part?.name || part.name || 'Unknown'
//   const num = partNum || part.part_num || part.part?.part_num || 'Unknown'
//   
//   return {
//     shape: `ë¶„ì„ ì‹¤íŒ¨: ${name}`,
//     center_stud: false,
//     groove: false,
//     connection: 'unknown',
//     function: 'unknown',
//     feature_text: `ë¶€í’ˆ ${num}ì˜ ìë™ ìƒì„±ëœ ê¸°ë³¸ ì„¤ëª…`,
//     recognition_hints: {
//       top_view: 'ê¸°ë³¸ í˜•íƒœ',
//       side_view: 'ê¸°ë³¸ í˜•íƒœ',
//       unique_features: ['ê¸°ë³¸ íŠ¹ì§•']
//     },
//     similar_parts: [],
//     distinguishing_features: ['ê¸°ë³¸ íŠ¹ì§•'],
//     confidence: 0.1,
//     part_num: num
//   }
// }

// í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ì„
function createTextOnlyAnalysis(part, partName, partNum) {
  return {
    shape: `í…ìŠ¤íŠ¸ ë¶„ì„: ${partName}`,
    center_stud: false,
    groove: false,
    connection: 'unknown',
    function: 'unknown',
    feature_text: `ë¶€í’ˆ ${partNum}ì˜ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê¸°ë³¸ ì„¤ëª…`,
    recognition_hints: {
      top_view: 'í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì •',
      side_view: 'í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì •',
      unique_features: ['í…ìŠ¤íŠ¸ ê¸°ë°˜ íŠ¹ì§•']
    },
    similar_parts: [],
    distinguishing_features: ['í…ìŠ¤íŠ¸ ê¸°ë°˜ íŠ¹ì§•'],
    confidence: 0.3,
    part_num: partNum
  }
}

// ì„ë² ë”© ìƒì„± í•¨ìˆ˜ export
// í…ìŠ¤íŠ¸ ì„ë² ë”© ë°°ì¹˜ + ìºì‹œ
export async function generateTextEmbeddingsBatch(analysisResults) {
  const results = []

  // 0) ì…ë ¥ ë°ì´í„° ì¤‘ë³µ ì œê±°: (part_num, color_id) ì¡°í•© ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
  const uniqueResults = []
  const seenEmbeddingKeys = new Set()
  
  for (const item of analysisResults) {
    const partNum = item.part_num || 'unknown'
    const colorId = item.color_id !== undefined ? item.color_id : (item.color?.id !== undefined ? item.color.id : null)
    const key = `${partNum}_${colorId}`
    
    if (!seenEmbeddingKeys.has(key)) {
      seenEmbeddingKeys.add(key)
      uniqueResults.push(item)
    } else {
      console.warn(`âš ï¸ Duplicate embedding input found for part_num=${partNum}, color_id=${colorId}, skipping`)
    }
  }
  
  console.log(`ğŸ“Š Embedding input deduplication: ${analysisResults.length} -> ${uniqueResults.length} results`)
  analysisResults = uniqueResults

  // 1) ê¸°ì¡´ ì„ë² ë”© ë³´ìœ /feature_text ëˆ„ë½ ì„ ë¶„ë¥˜
  const needsEmbedding = []
  for (const item of analysisResults) {
    const partNum = item.part_num || 'unknown'
    
    // part_idê°€ 'unknown'ì¸ ê²½ìš° ìŠ¤í‚µ
    if (partNum === 'unknown' || partNum === 'Unknown') {
      console.warn(`âš ï¸ Skipping embedding for unknown part_num: ${partNum}`)
      continue
    }
    
    if (item.has_embedding === true) {
      console.log(`â­ï¸ Skipping embedding for ${partNum} - already has embedding`)
      results.push({ part_num: partNum, embedding: item.existing_embedding || null, feature_text: item.feature_text })
      continue
    }
    if (!item.feature_text) {
      console.warn(`âš ï¸ No feature text for ${partNum}, skipping embedding`)
      results.push({ part_num: partNum, embedding: null, error: 'feature_text missing' })
      continue
    }
    needsEmbedding.push(item)
  }

  if (needsEmbedding.length === 0) return results

  // 2) í…ìŠ¤íŠ¸ í•´ì‹œ ìºì‹œë¡œ ì¤‘ë³µ ì œê±°
  const textToIndices = new Map()
  const uniqueTexts = []
  needsEmbedding.forEach((item, idx) => {
    const key = stableTextKey(item.feature_text)
    if (!textToIndices.has(key)) {
      textToIndices.set(key, [])
      uniqueTexts.push(item.feature_text)
    }
    textToIndices.get(key).push(idx)
  })

  // 3) OpenAI Embeddings API ë‹¤ì¤‘ ì…ë ¥ ë°°ì¹˜ í˜¸ì¶œ
  try {
    const response = await fetch(`${CLIP_CONFIG.baseUrl}/embeddings`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${CLIP_CONFIG.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: CLIP_CONFIG.model,
        input: uniqueTexts,
        dimensions: CLIP_CONFIG.dimensions
      })
    })

    if (!response.ok) throw new Error(`Embedding API Error: ${response.status}`)
    const data = await response.json()

    // 4) ê²°ê³¼ ë§¤í•‘: ë™ì¼ í…ìŠ¤íŠ¸ ê³µìœ  ì¸ë±ìŠ¤ì— ë™ì¼ ì„ë² ë”© ë³µì œ
    uniqueTexts.forEach((text, uIdx) => {
      const embedding = data.data[uIdx].embedding
      const targetIndices = textToIndices.get(stableTextKey(text))
      for (const idx of targetIndices) {
        const src = needsEmbedding[idx]
        const enhancedText = buildEnhancedEmbeddingText({
          partName: src.part?.name,
          partNum: src.part_num,
          colorName: src.color?.name,
          featureText: src.feature_text,
          keypoints: src.keypoints,
          distinguishing: src.distinguishing_features,
          legoPartNumber: src.part?.external_ids?.lego || src.part?.external_ids?.Lego || null
        })
        results.push({ part_num: src.part_num || 'unknown', embedding, feature_text: enhancedText })
      }
    })
  } catch (error) {
    console.error('âŒ Batch embeddings failed:', error)
    // ì‹¤íŒ¨ ì‹œ ê°œë³„ í˜¸ì¶œ í´ë°±(ìµœì†Œí•œì˜ í’ˆì§ˆ ìœ ì§€)
    for (const src of needsEmbedding) {
      try {
        const r = await fetch(`${CLIP_CONFIG.baseUrl}/embeddings`, {
          method: 'POST',
          headers: { 'Authorization': `Bearer ${CLIP_CONFIG.apiKey}`, 'Content-Type': 'application/json' },
          body: JSON.stringify({ model: CLIP_CONFIG.model, input: src.feature_text, dimensions: CLIP_CONFIG.dimensions })
        })
        if (!r.ok) throw new Error(`Embedding API Error: ${r.status}`)
        const j = await r.json()
        const enhancedText = buildEnhancedEmbeddingText({
          partName: src.part?.name,
          partNum: src.part_num,
          colorName: src.color?.name,
          featureText: src.feature_text,
          keypoints: src.keypoints,
          distinguishing: src.distinguishing_features,
          legoPartNumber: src.part?.external_ids?.lego || src.part?.external_ids?.Lego || null
        })
        results.push({ part_num: src.part_num || 'unknown', embedding: j.data[0].embedding, feature_text: enhancedText })
      } catch (e) {
        results.push({ part_num: src.part_num || 'unknown', embedding: null, error: e.message })
      }
    }
  }

  return results
}

function stableTextKey(text) {
  return String(text).trim().toLowerCase()
}

// í‘œì¤€ íƒœê·¸ ì •ê·œí™” (ê²€ìƒ‰Â·í›„ì²˜ë¦¬ ìµœì í™”)
function normalizeShapeTag(raw) {
  const t = String(raw || '').toLowerCase()
  if (/(plate|í”Œë ˆì´íŠ¸)/.test(t)) return 'plate'
  if (/(brick|ë¸Œë¦­)/.test(t)) return 'brick'
  if (/(slope|ê²½ì‚¬)/.test(t)) return 'slope'
  if (/(tile|íƒ€ì¼)/.test(t)) return 'tile'
  if (/(animal|ë™ë¬¼)/.test(t)) return 'animal_figure'
  if (/(leaf|plant|ì|ì‹ë¬¼)/.test(t)) return 'plant_leaf'
  if (/(technic|í…Œí¬ë‹‰)/.test(t)) return 'technic'
  return t || 'unknown'
}

function normalizeFunctionTag(raw) {
  const t = String(raw || '').toLowerCase()
  if (/(basic|ê¸°ë³¸|building|êµ¬ì„±)/.test(t)) return 'building'
  if (/(decoration|ì¥ì‹)/.test(t)) return 'decoration'
  if (/(figure|í”¼ê·œì–´)/.test(t)) return 'figure'
  if (/(slope|ê²½ì‚¬)/.test(t)) return 'slope'
  return t || 'unknown'
}

// CLIP ìŠ¤íƒ€ì¼ ë¬¸êµ¬(ì§§ê³  í•µì‹¬ì–´ ìœ„ì£¼)ë¡œ ë³€í™˜
function clipifyPhrases(arr) {
  if (!Array.isArray(arr)) return []
  return arr
    .map(s => String(s || '').toLowerCase().trim())
    .filter(Boolean)
    .map(s => s
      .replace(/\b(with|and|the|of|for|a|an|in|on|to|by|from)\b/g, '')
      .replace(/\s+/g, ' ')
      .trim()
    )
    .map(s => s.length > 40 ? s.slice(0, 40) : s)
}

function buildEnhancedEmbeddingText({ partName, partNum, colorName, featureText, keypoints, distinguishing, legoPartNumber }) {
  const header = [
    partName ? `name:${partName}` : null,
    partNum ? `part:${partNum}` : null,
    legoPartNumber ? `lego:${legoPartNumber}` : null,
    colorName ? `color:${colorName}` : null
  ].filter(Boolean).join(' ')

  const keypointsText = Array.isArray(keypoints) && keypoints.length > 0
    ? ` keypoints:${keypoints.join('|')}`
    : ''
  const distinguishingText = Array.isArray(distinguishing) && distinguishing.length > 0
    ? ` distinguishing:${distinguishing.join('|')}`
    : ''

  return `${header} features:${featureText || ''}${keypointsText}${distinguishingText}`.trim()
}

// ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í•¨ìˆ˜ export
export async function saveToMasterPartsDB(analysisResults) {
  try {
    // 0) ì…ë ¥ ë°ì´í„° ì¤‘ë³µ ì œê±°: (part_num, color_id) ì¡°í•© ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
    const uniqueResults = []
    const seenAnalysisKeys = new Set()
    
    for (const result of analysisResults) {
      const partNum = result.part_num || 'unknown'
      const colorId = result.color_id !== undefined ? result.color_id : (result.color?.id !== undefined ? result.color.id : null)
      
      // part_idê°€ nullì´ê±°ë‚˜ 'unknown'ì¸ ê²½ìš° ìŠ¤í‚µ
      if (!partNum || partNum === 'unknown' || partNum === 'Unknown') {
        console.warn(`âš ï¸ Skipping result with invalid part_num: ${partNum}`)
        continue
      }
      
      const key = `${partNum}_${colorId}`
      
      if (!seenAnalysisKeys.has(key)) {
        seenAnalysisKeys.add(key)
        uniqueResults.push(result)
      } else {
        console.warn(`âš ï¸ Duplicate analysis result found for part_num=${partNum}, color_id=${colorId}, skipping`)
      }
    }
    
    console.log(`ğŸ“Š Input deduplication: ${analysisResults.length} -> ${uniqueResults.length} results`)
    analysisResults = uniqueResults

    // 1) ëˆ„ë½ ì„ë² ë”© ë³´ì¶©: embedding ì—†ëŠ” ê²°ê³¼ë“¤ë§Œ ë°°ì¹˜ ìƒì„±
    const missingEmb = analysisResults.filter(r => !Array.isArray(r.embedding) || r.embedding.length === 0)
    if (missingEmb.length > 0) {
      try {
        const embResults = await generateTextEmbeddingsBatch(missingEmb)
        const embMap = new Map()
        for (const e of embResults) {
          if (Array.isArray(e.embedding)) embMap.set(e.part_num, e.embedding)
        }
        analysisResults.forEach(r => {
          if (!Array.isArray(r.embedding) || r.embedding.length === 0) {
            const emb = embMap.get(r.part_num)
            if (emb) r.embedding = emb
          }
        })
      } catch (e) {
        console.warn('âš ï¸ Failed to backfill embeddings; proceeding without some embeddings', e)
      }
    }

    // ë¶„ë¥˜ê¸° ì´ˆê¸°í™” (Tier/ë©”íƒ€ë°ì´í„° ì‚°ì¶œ)
    const classifier = usePartClassification()

    // color_id í™•ì •: result.color_id ë˜ëŠ” result.color?.idì—ì„œ ì¶”ì¶œ, ì—†ìœ¼ë©´ ì €ì¥ ìŠ¤í‚µ
    const mapped = analysisResults.map(result => {
      const resolvedColorId = (result.color_id !== undefined && result.color_id !== null)
        ? result.color_id
        : (result.color?.id !== undefined ? result.color.id : null)

      const partName = result.part?.name || result.name || ''
      const partNum = result.part_num || result.part?.part_num || ''

      // Tier ë¶„ë¥˜ ë° í–¥ìƒ ë©”íƒ€ë°ì´í„° ê³„ì‚°
      const tierClassification = classifier.classifyPartTier({ name: partName, part_num: partNum })
      const enhancedMetadata = classifier.generateEnhancedMetadata({ name: partName, part_num: partNum }, tierClassification)

      // íƒœê·¸ ì •ê·œí™” + CLIP ìŠ¤íƒ€ì¼ ë¬¸êµ¬ ë³€í™˜
      const normalizedShape = normalizeShapeTag(result.shape)
      const normalizedFunction = normalizeFunctionTag(result.function)
      const clipDistinguishing = clipifyPhrases(result.distinguishing_features)
      const clipHints = {
        ...result.recognition_hints,
        unique_features: clipifyPhrases(result.recognition_hints?.unique_features)
      }

      return {
        part_id: result.part_num,
        part_name: result.part?.name || 'Unknown',
        part_category: result.part?.part_cat_id || null,
        color_id: resolvedColorId,
        // 3-Tier ìš´ì˜ ì»¬ëŸ¼ ì €ì¥ (í†µê³„/ìš´ì˜ìš©)
        tier: tierClassification.tier,
        orientation_sensitive: tierClassification.orientation_sensitive,
        complexity_level: enhancedMetadata.complexity_level,
        feature_json: {
          shape: result.shape,
          center_stud: result.center_stud,
          groove: result.groove,
          connection: result.connection,
          function: result.function,
          recognition_hints: result.recognition_hints,
          similar_parts: result.similar_parts,
          distinguishing_features: result.distinguishing_features,
          keypoints: result.keypoints || [],
          confusions: result.confusions || [],
          stud_count_top: (typeof result.stud_count_top === 'number' ? result.stud_count_top : null),
          tube_count_bottom: (typeof result.tube_count_bottom === 'number' ? result.tube_count_bottom : null),
          color_expectation: result.color_expectation || null,
          shape_tag: normalizedShape,
          function_tag: normalizedFunction,
          clip_distinguishing: clipDistinguishing,
          clip_unique_features: clipHints.unique_features
        },
        feature_text: result.feature_text,
        clip_text_emb: Array.isArray(result.embedding) ? result.embedding : null,
        // ë³„ë„ ì»¬ëŸ¼ìœ¼ë¡œë„ ì €ì¥í•˜ì—¬ ê²€ìƒ‰ ìµœì í™”
        recognition_hints: result.recognition_hints || null,
        similar_parts: result.similar_parts || null,
        distinguishing_features: clipDistinguishing || null,
        confidence: result.confidence || 0.5
      }
    })

    // color_idê°€ ì—†ëŠ” ë ˆì½”ë“œëŠ” ì €ì¥ ìŠ¤í‚µ (ì¤‘ë³µ/ì¬ë¶„ì„ ìœ ë°œ ë°©ì§€)
    const validRecords = mapped.filter(r => r.color_id !== null && r.color_id !== undefined)
    const skipped = mapped.length - validRecords.length

    // ì¤‘ë³µ ì œê±°: (part_id, color_id) ì¡°í•©ì´ ì¤‘ë³µë˜ëŠ” ê²½ìš° ë§ˆì§€ë§‰ ê²ƒë§Œ ìœ ì§€
    const uniqueRecords = []
    const seenRecordKeys = new Set()
    
    // ì—­ìˆœìœ¼ë¡œ ìˆœíšŒí•˜ì—¬ ì¤‘ë³µëœ í‚¤ì˜ ê²½ìš° ë§ˆì§€ë§‰(ìµœì‹ ) ë ˆì½”ë“œë§Œ ìœ ì§€
    for (let i = validRecords.length - 1; i >= 0; i--) {
      const record = validRecords[i]
      const key = `${record.part_id}_${record.color_id}`
      
      if (!seenRecordKeys.has(key)) {
        seenRecordKeys.add(key)
        uniqueRecords.unshift(record) // ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•´ unshift ì‚¬ìš©
      } else {
        console.warn(`âš ï¸ Duplicate record found for part_id=${record.part_id}, color_id=${record.color_id}, skipping`)
      }
    }

    const records = uniqueRecords
    const duplicatesRemoved = validRecords.length - records.length

    console.log(`ğŸ’¾ Saving ${records.length} records to parts_master_features...`)
    if (skipped > 0) {
      console.warn(`âš ï¸ Skipping ${skipped} records without color_id to avoid null-color duplicates`)
    }
    if (duplicatesRemoved > 0) {
      console.warn(`âš ï¸ Removed ${duplicatesRemoved} duplicate records to avoid constraint violations`)
    }
    
    const { data, error } = await supabase
      .from('parts_master_features')
      .upsert(records, { 
        onConflict: 'part_id,color_id',
        ignoreDuplicates: false 
      })

    if (error) throw error

    console.log(`âœ… Successfully saved ${records.length} records to parts_master_features`)
    
    // ìºì‹œ ì—…ë°ì´íŠ¸
    records.forEach(record => {
      const cacheKey = `${record.part_id}_${record.color_id}`
      const result = {
        part_num: record.part_id,
        color_id: record.color_id,
        shape: record.feature_json?.shape || 'unknown',
        center_stud: record.feature_json?.center_stud || false,
        groove: record.feature_json?.groove || false,
        connection: record.feature_json?.connection || 'unknown',
        function: record.feature_json?.function || 'unknown',
        feature_text: record.feature_text,
        recognition_hints: record.feature_json?.recognition_hints || {},
        similar_parts: record.feature_json?.similar_parts || [],
        distinguishing_features: record.feature_json?.distinguishing_features || [],
        confidence: record.confidence || 0.5,
        embedding: record.clip_text_emb
      }
      analysisCache.set(cacheKey, result)
    })
    
    return { success: true, count: records.length }
    
  } catch (error) {
    console.error('âŒ Database save failed:', error)
    throw error
  }
}

// ê¸°ì¡´ ë¶„ì„ í™•ì¸ í•¨ìˆ˜ export
export async function checkExistingAnalysis(partNum, colorId) {
  try {
    const cacheKey = `${partNum}_${colorId}`
    console.log(`ğŸ” Checking existing analysis for ${partNum} (color: ${colorId})`)
    
    // colorIdê°€ ì—†ìœ¼ë©´, null-colorë¡œ ì €ì¥ëœ ê¸°ì¡´ ë ˆì½”ë¦¬ë„ ì¡´ì¬ë¡œ ê°„ì£¼í•˜ì—¬ ì¦‰ì‹œ ìŠ¤í‚µ
    if (colorId === null || colorId === undefined) {
      console.warn(`âš ï¸ colorId is missing for ${partNum}; treating as existing to avoid duplicate LLM runs`)
      return { part_num: partNum, color_id: null, feature_text: '', embedding: null }
    }

    // 1. ë¨¼ì € ìºì‹œì—ì„œ í™•ì¸
    if (analysisCache.has(cacheKey)) {
      console.log(`âœ… Found in cache for ${partNum} (color: ${colorId})`)
      return analysisCache.get(cacheKey)
    }
    
    // 2. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í™•ì¸
    const { data, error: dbError } = await supabase
      .from('parts_master_features')
      .select('part_id, color_id, feature_json, feature_text, clip_text_emb, confidence')
      .eq('part_id', partNum)
      .eq('color_id', parseInt(colorId))
      .maybeSingle()
    
    console.log(`ğŸ” Query result for ${partNum} (color: ${colorId}):`, { data, error: dbError })

    if (dbError) {
      console.warn(`âš ï¸ Database error checking existing analysis:`, dbError)
      return null // ì˜¤ë¥˜ ì‹œ ìƒˆë¡œ ë¶„ì„
    }
    
    // ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
    if (!data) {
      console.log(`ğŸ“ No existing analysis found for ${partNum} (color: ${colorId})`)
      return null
    }
    
    console.log(`âœ… Found existing analysis for ${partNum} (color: ${colorId})`)
    console.log(`ğŸ“Š Existing data: part_id=${data.part_id}, confidence=${data.confidence}, has_embedding=${!!data.clip_text_emb}`)
    
    // part_idê°€ nullì¸ ê²½ìš° ì²˜ë¦¬
    if (!data.part_id) {
      console.log(`âš ï¸ Found record but part_id is null for ${partNum} (color: ${colorId}), skipping`)
      return null
    }
    
    // ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ë¥¼ í˜„ì¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    const result = {
      part_num: data.part_id,
      color_id: data.color_id,
      shape: data.feature_json?.shape || 'unknown',
      center_stud: data.feature_json?.center_stud || false,
      groove: data.feature_json?.groove || false,
      connection: data.feature_json?.connection || 'unknown',
      function: data.feature_json?.function || 'unknown',
      feature_text: data.feature_text,
      recognition_hints: data.feature_json?.recognition_hints || {},
      similar_parts: data.feature_json?.similar_parts || [],
      distinguishing_features: data.feature_json?.distinguishing_features || [],
      confidence: data.confidence || 0.5,
      embedding: data.clip_text_emb,
      has_embedding: !!data.clip_text_emb,
      existing_embedding: data.clip_text_emb
    }
    
    // ìºì‹œì— ì €ì¥
    analysisCache.set(cacheKey, result)
    return result
    
  } catch (error) {
    console.error('âŒ Check existing analysis failed:', error)
    return null
  }
}

// ì „ì—­ ìºì‹œë¡œ ì¤‘ë³µ ì²´í¬
const analysisCache = new Map()

export function useMasterPartsPreprocessing() {
  const loading = ref(false)
  const error = ref(null)
  const processing = ref(false)
  
  // í–¥ìƒëœ ì¸ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
  const enhancedRecognition = useEnhancedRecognition()
  const progress = ref(0)

  // ëª¨ë“  Rebrickable ë¶€í’ˆ ìˆ˜ì§‘
  const collectAllRebrickableParts = async () => {
    loading.value = true
    error.value = null

    try {
      const allParts = []
      let page = 1
      const pageSize = 1000

      while (true) {
        const response = await fetch(`https://rebrickable.com/api/v3/lego/parts/?page=${page}&page_size=${pageSize}`, {
          headers: {
            'Authorization': `key ${import.meta.env.VITE_REBRICKABLE_API_KEY}`,
            'Content-Type': 'application/json'
          }
        })

        if (!response.ok) break

        const data = await response.json()
        if (!data.results || data.results.length === 0) break

        allParts.push(...data.results)
        page++

        // API ì œí•œ ê³ ë ¤
        await new Promise(resolve => setTimeout(resolve, 100))
      }

      console.log(`Collected ${allParts.length} parts from Rebrickable`)
      return allParts
    } catch (err) {
      error.value = err.message
      throw err
    } finally {
      loading.value = false
    }
  }

  // Rate Limit ìƒíƒœ ì¶”ì 
  let rateLimitCount = 0
  let lastRateLimitTime = 0
  
  // ë¶€í’ˆë³„ LLM ë¶„ì„ (ë°°ì¹˜ ì²˜ë¦¬ - Rate Limit ëŒ€ì‘)
  const analyzePartsBatch = async (parts, batchSize = 2) => {
    processing.value = true
    error.value = null
    progress.value = 0

    try {
      const results = []
      const errors = []
      
      // Rate Limit ìƒíƒœì— ë”°ë¥¸ ë™ì  ì¡°ì •
      const currentTime = Date.now()
      const timeSinceLastRateLimit = currentTime - lastRateLimitTime
      
      let DELAY_BETWEEN_BATCHES = 10000 // ê¸°ë³¸ 10ì´ˆ
      let DELAY_BETWEEN_REQUESTS = 2000  // ê¸°ë³¸ 2ì´ˆ
      
      // ìµœê·¼ Rate Limit ë°œìƒ ì‹œ ë” ê¸´ ì§€ì—°
      if (rateLimitCount > 0 && timeSinceLastRateLimit < 300000) { // 5ë¶„ ì´ë‚´
        DELAY_BETWEEN_BATCHES = 30000 // 30ì´ˆ
        DELAY_BETWEEN_REQUESTS = 5000  // 5ì´ˆ
        console.warn(`âš ï¸ Rate limit detected recently, using extended delays: ${DELAY_BETWEEN_BATCHES}ms batches, ${DELAY_BETWEEN_REQUESTS}ms requests`)
      }

      for (let i = 0; i < parts.length; i += batchSize) {
        const batch = parts.slice(i, i + batchSize)
        console.log(`Processing batch ${Math.floor(i / batchSize) + 1}/${Math.ceil(parts.length / batchSize)}`)

        const batchPromises = batch.map(async (part, index) => {
          // ìš”ì²­ ê°„ ì§€ì—°
          if (index > 0) {
            await new Promise(resolve => setTimeout(resolve, DELAY_BETWEEN_REQUESTS))
          }
          
          try {
            const analysis = await analyzePartWithLLM(part)
            if (analysis === null) {
              console.log(`â­ï¸ Skipping LLM analysis for ${part.part_num} - API key missing`)
              return { part, analysis: null, success: true, skipped: true }
            }
            return { part, analysis, success: true }
          } catch (err) {
            console.error(`Failed to analyze part ${part.part_num}:`, err)
            
            // Rate Limit ì—ëŸ¬ ì¶”ì 
            if (err.message.includes('429') || err.message.includes('rate_limit')) {
              rateLimitCount++
              lastRateLimitTime = Date.now()
              console.warn(`ğŸš¨ Rate limit error #${rateLimitCount} detected for part ${part.part_num}`)
            }
            
            return { part, error: err.message, success: false }
          }
        })

        const batchResults = await Promise.all(batchPromises)
        
        for (const result of batchResults) {
          if (result.success) {
            if (result.skipped) {
              console.log(`â­ï¸ Skipped LLM analysis for ${result.part.part_num} - using existing data only`)
            }
            results.push(result)
          } else {
            errors.push(result)
          }
        }

        // ë°°ì¹˜ ê°„ ì§€ì—° (ë§ˆì§€ë§‰ ë°°ì¹˜ ì œì™¸)
        if (i + batchSize < parts.length) {
          console.log(`â³ Waiting ${DELAY_BETWEEN_BATCHES/1000}s before next batch... (Rate limit count: ${rateLimitCount})`)
          await new Promise(resolve => setTimeout(resolve, DELAY_BETWEEN_BATCHES))
        }

        // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        progress.value = Math.round((i + batchSize) / parts.length * 100)

        // API ì œí•œì„ ìœ„í•œ ì§€ì—°
        await new Promise(resolve => setTimeout(resolve, 2000))
      }

      return { results, errors }
    } catch (err) {
      error.value = err.message
      throw err
    } finally {
      processing.value = false
    }
  }

  // ê°œë³„ ë¶€í’ˆ LLM ë¶„ì„ (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ Aë‹¨ê³„)
  const analyzePartWithLLM = async (part) => {
    try {
      if (import.meta.env.DEV) {
      console.log('ë¶„ì„í•  ë¶€í’ˆ ì •ë³´:', part)
    }
      
      // ë¶€í’ˆ ì •ë³´ í™•ì¸ ë° ì •ë¦¬
      const partName = part.part?.name || part.name || 'Unknown'
      const partNum = part.part_num || part.part?.part_num || 'Unknown'
      const partImgUrl = part.part?.part_img_url || part.part_img_url || null
      
      // part_idê°€ 'Unknown'ì¸ ê²½ìš° ìŠ¤í‚µ
      if (partNum === 'Unknown') {
        console.warn(`âš ï¸ Skipping part with unknown part_num: ${partName}`)
        return null
      }
      
      console.log('ì •ë¦¬ëœ ë¶€í’ˆ ì •ë³´:', { partName, partNum, partImgUrl })
      
      // ì´ë¯¸ì§€ URLì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰
      if (!partImgUrl) {
        console.warn(`ë¶€í’ˆ ${partNum}ì˜ ì´ë¯¸ì§€ URLì´ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.`)
        return createTextOnlyAnalysis(part, partName, partNum)
      }
      
      const prompt = `ë‹¤ìŒ ë ˆê³  ë¶€í’ˆì„ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.

ë¶€í’ˆ ì •ë³´:
- ë¶€í’ˆëª…: ${partName}
- ë¶€í’ˆ ë²ˆí˜¸: ${partNum}

ì‘ë‹µ í˜•ì‹:
{
  "shape": "ë¶€í’ˆì˜ ê¸°ë³¸ í˜•íƒœ",
  "center_stud": true/false,
  "groove": true/false,
  "connection": "ì—°ê²° ë°©ì‹",
  "function": "ì£¼ìš” ê¸°ëŠ¥",
  "feature_text": "ë¶€í’ˆ íŠ¹ì§•ì„ ì„¤ëª…í•˜ëŠ” í…ìŠ¤íŠ¸",
  "recognition_hints": {
    "top_view": "ìœ„ì—ì„œ ë³¸ ëª¨ìŠµ",
    "side_view": "ì˜†ì—ì„œ ë³¸ ëª¨ìŠµ",
    "unique_features": ["ê³ ìœ  íŠ¹ì§•ë“¤"]
  },
  "similar_parts": ["ìœ ì‚¬í•œ ë¶€í’ˆ ë²ˆí˜¸ë“¤"],
  "distinguishing_features": ["êµ¬ë³„ë˜ëŠ” íŠ¹ì§•ë“¤"],
  "confidence": 0.95
}

ì‹ ë¢°ë„(confidence) ê¸°ì¤€:
- 0.9-1.0: ë§¤ìš° ëª…í™•í•œ ë¶€í’ˆ (ê¸°ë³¸ ë¸”ë¡, í”Œë ˆì´íŠ¸ ë“±)
- 0.7-0.9: ë¹„êµì  ëª…í™•í•œ ë¶€í’ˆ (íŠ¹ìˆ˜ ë¶€í’ˆ, ì¥ì‹ ìš”ì†Œ)
- 0.5-0.7: ì• ë§¤í•œ ë¶€í’ˆ (ë³µì¡í•œ í˜•íƒœ, ì¸ì‡„ê°€ ìˆëŠ” ë¶€í’ˆ)
- 0.3-0.5: ë¶ˆí™•ì‹¤í•œ ë¶€í’ˆ (ì´ë¯¸ì§€ê°€ íë¦¬ê±°ë‚˜ ê°ë„ê°€ ë‚˜ì¨)
- 0.0-0.3: ë¶„ì„ ë¶ˆê°€ëŠ¥ (ì´ë¯¸ì§€ ì—†ìŒ ë˜ëŠ” ë„ˆë¬´ íë¦¼)`

      const requestBody = {
        model: LLM_CONFIG.model,
        messages: [
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: prompt
              },
              {
                type: 'image_url',
                image_url: {
                  url: partImgUrl,
                  detail: 'high'
                }
              }
            ]
          }
        ],
        max_tokens: LLM_CONFIG.maxTokens,
        temperature: LLM_CONFIG.temperature
      }

      console.log('API ìš”ì²­ ì •ë³´:', {
        model: LLM_CONFIG.model,
        apiKey: LLM_CONFIG.apiKey ? 'ì„¤ì •ë¨' : 'ì—†ìŒ',
        imageUrl: partImgUrl,
        promptLength: prompt.length
      })

      const response = await fetch(`${LLM_CONFIG.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${LLM_CONFIG.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(requestBody)
      })

      if (!response.ok) {
        const errorText = await response.text()
        console.error('API ì˜¤ë¥˜ ì‘ë‹µ:', errorText)
        throw new Error(`LLM API Error: ${response.status} - ${errorText}`)
      }

      const data = await response.json()
      if (import.meta.env.DEV) {
      console.log('LLM raw response:', data)
    }
      
      // ì‘ë‹µ êµ¬ì¡° í™•ì¸
      if (!data.choices || !data.choices[0] || !data.choices[0].message) {
        console.error('ì‘ë‹µ êµ¬ì¡° ì˜¤ë¥˜:', data)
        return null
      }
      
      const llmResponse = data.choices[0].message.content
      console.log('LLM ì‘ë‹µ ë‚´ìš©:', llmResponse)

      // JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json ... ``` ë˜ëŠ” { ... } íŒ¨í„´ ì°¾ê¸°)
      let jsonText = llmResponse
      
      // ```json ... ``` íŒ¨í„´ ì°¾ê¸°
      const jsonBlockMatch = llmResponse.match(/```json\s*([\s\S]*?)\s*```/)
      if (jsonBlockMatch) {
        jsonText = jsonBlockMatch[1].trim()
        console.log('ì¶”ì¶œëœ JSON ë¸”ë¡:', jsonText)
      } else {
        // ```json íŒ¨í„´ì´ ì—†ìœ¼ë©´ { ... } íŒ¨í„´ ì°¾ê¸°
        const jsonObjectMatch = llmResponse.match(/\{[\s\S]*\}/)
        if (jsonObjectMatch) {
          jsonText = jsonObjectMatch[0]
          console.log('ì¶”ì¶œëœ JSON ê°ì²´:', jsonText)
        }
      }

      // JSON íŒŒì‹±
      let analysisResult
      try {
        analysisResult = JSON.parse(jsonText)
        // part_num ì¶”ê°€ (LLM ì‘ë‹µì—ëŠ” ì—†ìœ¼ë¯€ë¡œ ìˆ˜ë™ ì¶”ê°€)
        analysisResult.part_num = partNum
        console.log('íŒŒì‹±ëœ ë¶„ì„ ê²°ê³¼:', analysisResult)
      } catch (parseError) {
        console.error('JSON íŒŒì‹± ì‹¤íŒ¨:', parseError)
        console.log('ì¶”ì¶œëœ JSON í…ìŠ¤íŠ¸:', jsonText)
        console.log('ì›ë³¸ ì‘ë‹µ:', llmResponse)
        // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ null ë°˜í™˜
        analysisResult = null
      }

      return analysisResult
    } catch (err) {
      console.error('LLM analysis failed:', err)
      return null
    }
  }

  // ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ìƒì„± (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - LLM ì‹¤íŒ¨ ì‹œ null ë°˜í™˜)
  // const createDefaultAnalysis = (part) => {
  //   const partNum = part.part_num || part.part?.part_num || 'unknown'
  //   return {
  //     part_num: partNum,
  //     shape: `ë¶„ì„ ì‹¤íŒ¨: ${part.name || part.part?.name}`,
  //     center_stud: false,
  //     groove: false,
  //     connection: 'unknown',
  //     function: 'unknown',
  //     feature_text: `ë¶„ì„ ì‹¤íŒ¨: ${part.name || part.part?.name}`,
  //     recognition_hints: {
  //       top_view: 'ë¶„ì„ ì‹¤íŒ¨',
  //       side_view: 'ë¶„ì„ ì‹¤íŒ¨',
  //       unique_features: []
  //     },
  //     similar_parts: [],
  //     distinguishing_features: [],
  //     confidence: 0.3
  //   }
  // }

  // í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ì„ (ì´ë¯¸ì§€ URLì´ ì—†ì„ ë•Œ)
  const createTextOnlyAnalysis = (part, partName, partNum) => {
    console.log(`ğŸ“ í…ìŠ¤íŠ¸ ì „ìš© ë¶„ì„ ìˆ˜í–‰: ${partName} (${partNum})`)
    console.log(`ğŸ” DEBUG: part object:`, part)
    console.log(`ğŸ” DEBUG: partNum value:`, partNum, typeof partNum)
    
    // ë¶€í’ˆëª…ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
    const isDuplo = partName.toLowerCase().includes('duplo')
    const isAnimal = partName.toLowerCase().includes('animal') || partName.toLowerCase().includes('lion') || partName.toLowerCase().includes('penguin')
    const isBrick = partName.toLowerCase().includes('brick')
    const hasPrint = partName.toLowerCase().includes('print')
    
    const result = {
      part_num: partNum,
      shape: isBrick ? 'rectangular_brick' : (isAnimal ? 'animal_figure' : 'unknown'),
      center_stud: isBrick,
      groove: false,
      connection: isBrick ? 'stud_connection' : 'unknown',
      function: isAnimal ? 'animal_figure' : (isBrick ? 'building_block' : 'unknown'),
      feature_text: `í…ìŠ¤íŠ¸ ë¶„ì„: ${partName}${isDuplo ? ' (Duplo)' : ''}${hasPrint ? ' (ì¸ì‡„ í¬í•¨)' : ''}`,
      recognition_hints: {
        top_view: isBrick ? '2x2 ë¸Œë¦­ í˜•íƒœ' : (isAnimal ? 'ë™ë¬¼ ëª¨ì–‘' : 'ë¯¸í™•ì¸'),
        side_view: isBrick ? 'ìŠ¤í„°ë“œ ì—°ê²°ë¶€' : (isAnimal ? 'ë™ë¬¼ íŠ¹ì§•' : 'ë¯¸í™•ì¸'),
        unique_features: hasPrint ? ['ì¸ì‡„ëœ ë””í…Œì¼'] : []
      },
      similar_parts: [],
      distinguishing_features: isDuplo ? ['Duplo í¬ê¸°'] : [],
      confidence: 0.4 // í…ìŠ¤íŠ¸ ë¶„ì„ì´ë¯€ë¡œ ë‚®ì€ ì‹ ë¢°ë„
    }
    
    console.log(`ğŸ” DEBUG: í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼:`, result)
    return result
  }

  // CLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµìš©)
  const generateClipTextEmbedding = async (featureText) => {
    try {
      if (!CLIP_CONFIG.apiKey) {
        throw new Error('Missing VITE_OPENAI_API_KEY')
      }
      const response = await fetch(`${CLIP_CONFIG.baseUrl}/embeddings`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${CLIP_CONFIG.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: CLIP_CONFIG.model,
          input: featureText,
          dimensions: CLIP_CONFIG.dimensions
        })
      })

      if (!response.ok) {
        throw new Error(`CLIP API Error: ${response.status}`)
      }

      const data = await response.json()
      return data.data[0].embedding
    } catch (err) {
      console.error('CLIP text embedding generation failed:', err)
      throw err
    }
  }

  // CLIP ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± (ì‹¤ì‹œê°„ ë§¤ì¹­ìš©)
  const generateClipImageEmbedding = async (imageUrl) => {
    try {
      // ì´ë¯¸ì§€ ì„ë² ë”©ì€ ì„œë²„/ì™¸ë¶€ APIì—ì„œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
      // í™˜ê²½ë³€ìˆ˜: VITE_CLIP_IMAGE_API_URL (POST base64 or URL)
      const endpoint = import.meta.env.VITE_CLIP_IMAGE_API_URL
      if (!endpoint) {
        throw new Error('Missing VITE_CLIP_IMAGE_API_URL')
      }
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ image_url: imageUrl, dimensions: CLIP_CONFIG.dimensions })
      })
      if (!response.ok) throw new Error(`Image embedding API Error: ${response.status}`)
      const data = await response.json()
      if (!data.embedding) throw new Error('Image embedding API response missing embedding')
      return data.embedding
    } catch (err) {
      console.error('CLIP image embedding generation failed:', err)
      throw err
    }
  }

  // ë§ˆìŠ¤í„° ë¶€í’ˆ DBì— ì €ì¥ (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµìš©)
  const saveToMasterPartsDB = async (analysisResults) => {
    try {
      const records = analysisResults.map(result => ({
        part_id: result.part_num,
        part_name: result.part?.name || 'Unknown',
        part_category: result.part?.part_cat_id || null,
        color_id: result.color?.id || null,
        feature_json: {
          shape: result.shape,
          center_stud: result.center_stud,
          groove: result.groove,
          connection: result.connection,
          function: result.function
        },
        feature_text: result.feature_text,
        clip_text_emb: result.clip_text_emb,
        recognition_hints: result.recognition_hints,
        similar_parts: result.similar_parts,
        distinguishing_features: result.distinguishing_features,
        confidence: result.confidence,
        usage_frequency: 0,
        detection_accuracy: 0.0,
        created_at: new Date().toISOString()
      }))

      const { data, error: dbError } = await supabase
        .from('parts_master_features')
        .upsert(records, {
          onConflict: 'part_id,color_id'
        })
        .select()

      if (dbError) throw dbError
      return data
    } catch (err) {
      error.value = err.message
      throw err
    }
  }

  // ì „ì²´ ë§ˆìŠ¤í„° DB êµ¬ì¶• í”„ë¡œì„¸ìŠ¤
  const buildMasterPartsDatabase = async () => {
    processing.value = true
    error.value = null
    progress.value = 0

    try {
      console.log('Starting master parts database construction...')

      // 1. ëª¨ë“  Rebrickable ë¶€í’ˆ ìˆ˜ì§‘
      console.log('Step 1: Collecting all Rebrickable parts...')
      const allParts = await collectAllRebrickableParts()
      console.log(`Collected ${allParts.length} parts`)

      // 2. ë¶€í’ˆë³„ LLM ë¶„ì„ (ë°°ì¹˜ ì²˜ë¦¬)
      console.log('Step 2: Analyzing parts with LLM...')
      const analysisResults = await analyzePartsBatch(allParts, 5) // ì‘ì€ ë°°ì¹˜ í¬ê¸°
      console.log(`Analyzed ${analysisResults.results.length} parts successfully`)
      console.log(`Failed to analyze ${analysisResults.errors.length} parts`)

      // 3. CLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
      console.log('Step 3: Generating CLIP text embeddings...')
      const embeddingResults = await generateTextEmbeddingsBatch(analysisResults.results)

      // 4. ë§ˆìŠ¤í„° DBì— ì €ì¥
      console.log('Step 4: Saving to master database...')
      const savedRecords = await saveToMasterPartsDB(embeddingResults)

      console.log(`Master parts database construction completed!`)
      console.log(`Total records saved: ${savedRecords.length}`)

      return {
        totalParts: allParts.length,
        analyzedParts: analysisResults.results.length,
        failedParts: analysisResults.errors.length,
        savedRecords: savedRecords.length
      }
    } catch (err) {
      error.value = err.message
      throw err
    } finally {
      processing.value = false
    }
  }

  // CLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ë°°ì¹˜ ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµìš©)
  const generateTextEmbeddingsBatch = async (analysisResults) => {
    const results = []

    for (const result of analysisResults) {
      try {
        // part_num í™•ì¸ ë° ì²˜ë¦¬
        const partNum = result.part_num || 'unknown'
        
        if (!result.feature_text) {
          console.warn(`ë¶€í’ˆ ${partNum}ì˜ feature_textê°€ ì—†ìŠµë‹ˆë‹¤.`)
          results.push({
            ...result,
            clip_text_emb: null
          })
          continue
        }

        const textEmbedding = await generateClipTextEmbedding(result.feature_text)
        results.push({
          ...result,
          clip_text_emb: textEmbedding
        })
      } catch (err) {
        const partNum = result.part_num || 'unknown'
        console.error(`Failed to generate text embedding for ${partNum}:`, err)
        results.push({
          ...result,
          clip_text_emb: null
        })
      }
    }

    return results
  }

  // ë§ˆìŠ¤í„° DB ìƒíƒœ í™•ì¸ (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµìš©)
  const checkMasterDBStatus = async () => {
    try {
      const { data, error: dbError } = await supabase
        .from('parts_master_features')
        .select('part_id, created_at, confidence, usage_frequency, detection_accuracy')
        .order('created_at', { ascending: false })
        .limit(1000)

      if (dbError) throw dbError

      const stats = {
        totalRecords: data.length,
        averageConfidence: data.reduce((sum, record) => sum + (record.confidence || 0), 0) / data.length,
        lastUpdated: data[0]?.created_at,
        highConfidence: data.filter(r => r.confidence > 0.8).length,
        lowConfidence: data.filter(r => r.confidence < 0.5).length,
        averageUsageFrequency: data.reduce((sum, record) => sum + (record.usage_frequency || 0), 0) / data.length,
        averageDetectionAccuracy: data.reduce((sum, record) => sum + (record.detection_accuracy || 0), 0) / data.length
      }

      return stats
    } catch (err) {
      error.value = err.message
      throw err
    }
  }

  // ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í™•ì¸ (ì¤‘ë³µ ë°©ì§€ìš©)
  const checkExistingAnalysis = async (partColorPairs) => {
    try {
      const { data, error: dbError } = await supabase
        .from('parts_master_features')
        .select('part_id, color_id, feature_json, feature_text, clip_text_emb, confidence')
        .in('part_id', partColorPairs.map(p => p.part_num))
        .in('color_id', partColorPairs.map(p => p.color_id))

      if (dbError) throw dbError
      
      // ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ë¥¼ í˜„ì¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
      const existingResults = data.map(record => ({
        part_num: record.part_id,
        color_id: record.color_id,
        shape: record.feature_json?.shape || 'unknown',
        center_stud: record.feature_json?.center_stud || false,
        groove: record.feature_json?.groove || false,
        connection: record.feature_json?.connection || 'unknown',
        function: record.feature_json?.function || 'unknown',
        feature_text: record.feature_text,
        recognition_hints: record.feature_json?.recognition_hints || {},
        similar_parts: record.feature_json?.similar_parts || [],
        distinguishing_features: record.feature_json?.distinguishing_features || [],
        confidence: record.confidence,
        clip_text_emb: record.clip_text_emb,
        is_existing: true // ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ì„ì„ í‘œì‹œ
      }))
      
      console.log(`ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ${existingResults.length}ê°œ ë°œê²¬`)
      return existingResults
    } catch (err) {
      console.error('ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í™•ì¸ ì‹¤íŒ¨:', err)
      return []
    }
  }

  return {
    loading,
    error,
    processing,
    progress,
    collectAllRebrickableParts,
    analyzePartsBatch,
    analyzePartWithLLM,
    generateClipTextEmbedding,
    generateClipImageEmbedding,
    saveToMasterPartsDB,
    buildMasterPartsDatabase,
    generateTextEmbeddingsBatch,
    checkMasterDBStatus,
    checkExistingAnalysis,
    // í–¥ìƒëœ ì¸ì‹ ì‹œìŠ¤í…œ
    enhancedRecognitionPipeline: enhancedRecognition.enhancedRecognitionPipeline,
    processBatchRecognition: enhancedRecognition.processBatchRecognition,
    filterByConfidence: enhancedRecognition.filterByConfidence,
    sortByConfidence: enhancedRecognition.sortByConfidence,
    generateStatistics: enhancedRecognition.generateStatistics
  }
}
