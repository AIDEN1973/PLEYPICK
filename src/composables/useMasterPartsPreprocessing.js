import { ref } from 'vue'
import { supabase } from './useSupabase'

// LLM API ì„¤ì • (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµìš©)
const LLM_CONFIG = {
  apiKey: import.meta.env.VITE_OPENAI_API_KEY || process.env.VITE_OPENAI_API_KEY || 'your-actual-api-key-here',
  baseUrl: 'https://api.openai.com/v1',
  model: 'gpt-4o-mini',
  maxTokens: 1000,
  temperature: 0.1
}

// í™˜ê²½ ë³€ìˆ˜ ë””ë²„ê¹… (í”„ë¡œë•ì…˜ì—ì„œë„ í‘œì‹œ)
console.log('ğŸ” Environment Debug:', {
  VITE_OPENAI_API_KEY: import.meta.env.VITE_OPENAI_API_KEY ? 'Present' : 'Missing',
  process_env: process.env.VITE_OPENAI_API_KEY ? 'Present' : 'Missing',
  apiKey: LLM_CONFIG.apiKey ? 'Present' : 'Missing',
  allEnv: Object.keys(import.meta.env).filter(key => key.startsWith('VITE_'))
})

// í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì •: 1ì°¨(4o-mini) ê²°ê³¼ê°€ ëª¨í˜¸í•˜ë©´ 2ì°¨(4.1-mini)ë¡œ ë³´ê°•
const HYBRID_CONFIG = {
  enabled: false,
  secondaryModel: 'gpt-4.1-mini'
}

// OpenAI í…ìŠ¤íŠ¸ ì„ë² ë”© ì„¤ì • (ì‚¬ì „ ë¶„ì„ëœ feature_textìš©)
const CLIP_CONFIG = {
  apiKey: import.meta.env.VITE_OPENAI_API_KEY,
  baseUrl: 'https://api.openai.com/v1',
  model: 'text-embedding-3-small',
  dimensions: 768
}

// ê°œë³„ í•¨ìˆ˜ë“¤ì„ exportí•˜ê¸° ìœ„í•´ í•¨ìˆ˜ë“¤ì„ ë°–ìœ¼ë¡œ ì´ë™
export async function analyzePartWithLLM(part) {
  try {
    // API í‚¤ ê²€ì¦
    if (!LLM_CONFIG.apiKey || LLM_CONFIG.apiKey === 'undefined') {
      console.warn('âš ï¸ OpenAI API key is missing, skipping LLM analysis')
      console.warn('ğŸ” Environment check:', {
        VITE_OPENAI_API_KEY: import.meta.env.VITE_OPENAI_API_KEY ? 'Present' : 'Missing',
        process_env: process.env.VITE_OPENAI_API_KEY ? 'Present' : 'Missing',
        allEnv: Object.keys(import.meta.env).filter(key => key.startsWith('VITE_'))
      })
      return createDefaultAnalysis(part)
    }
    
    if (import.meta.env.DEV) {
      console.log('ë¶„ì„í•  ë¶€í’ˆ ì •ë³´:', part)
    }
    
    // ë¶€í’ˆ ì •ë³´ í™•ì¸ ë° ì •ë¦¬
    const partName = part.part?.name || part.name || 'Unknown'
    const partNum = part.part_num || part.part?.part_num || 'Unknown'
    const partImgUrl = part.part?.part_img_url || part.part_img_url || null
    const colorName = part.color?.name || part.color_name || 'Unknown'
    const colorId = part.color?.id ?? part.color_id ?? null
    const elementId = part.element_id || part.inv_part_id || null
    
    // ë ˆê³  ê³µì‹ ë¶€í’ˆë²ˆí˜¸ í™•ì¸ (external_idsì—ì„œ ì¶”ì¶œ)
    const externalIds = part.part?.external_ids || part.external_ids || {}
    const legoPartNumber = externalIds.lego || externalIds.Lego || null
    
    if (import.meta.env.DEV) {
      console.log('ì •ë¦¬ëœ ë¶€í’ˆ ì •ë³´:', { partName, partNum, partImgUrl, legoPartNumber })
    }
    
    // ì´ë¯¸ì§€ URLì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰
    if (!partImgUrl) {
      console.warn(`ë¶€í’ˆ ${partNum}ì˜ ì´ë¯¸ì§€ URLì´ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.`)
      return createTextOnlyAnalysis(part, partName, partNum)
    }
    
    const prompt = `ë‹¤ìŒ ë ˆê³  ë¶€í’ˆì„ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.

ë¶€í’ˆ ì •ë³´:
- ë¶€í’ˆëª…: ${partName}
- ë¶€í’ˆ ë²ˆí˜¸: ${partNum}
${legoPartNumber ? `- ë ˆê³  ê³µì‹ ë¶€í’ˆë²ˆí˜¸: ${legoPartNumber}` : ''}
 - ìƒ‰ìƒ: ${colorName}${colorId !== null ? ` (ID: ${colorId})` : ''}
 ${elementId ? `- ì—˜ë¦¬ë¨¼íŠ¸ ID: ${elementId}` : ''}

ì‘ë‹µ í˜•ì‹:
{
  "shape": "ë¶€í’ˆì˜ ê¸°ë³¸ í˜•íƒœ",
  "center_stud": true/false,
  "groove": true/false,
  "connection": "ì—°ê²° ë°©ì‹",
  "function": "ì£¼ìš” ê¸°ëŠ¥",
  "feature_text": "ë¶€í’ˆ íŠ¹ì§•ì„ ì„¤ëª…í•˜ëŠ” í…ìŠ¤íŠ¸",
  "recognition_hints": {
    "top_view": "ìœ„ì—ì„œ ë³¸ ëª¨ìŠµ",
    "side_view": "ì˜†ì—ì„œ ë³¸ ëª¨ìŠµ",
    "unique_features": ["ê³ ìœ  íŠ¹ì§•ë“¤"]
  },
  "similar_parts": ["ìœ ì‚¬í•œ ë¶€í’ˆ ë²ˆí˜¸ë“¤"],
  "distinguishing_features": ["êµ¬ë³„ë˜ëŠ” íŠ¹ì§•ë“¤"],
  "stud_count_top": 0,
  "tube_count_bottom": 0,
  "size_category": "duplo|system|minifig|technic",
  "keypoints": ["ì‹ë³„ì— ì¤‘ìš”í•œ í˜•ìƒ í¬ì¸íŠ¸(ëª¨ì„œë¦¬, ìŠ¬ë¡¯, í™ˆ, í•€ ìœ„ì¹˜ ë“±)"],
  "confusions": ["í˜¼ë™ë˜ê¸° ì‰¬ìš´ ìœ ì‚¬ ë¶€í’ˆ ë²ˆí˜¸ë“¤"],
  "color_expectation": "ì´ë¯¸ì§€ìƒ ìƒ‰ìƒ ê´€ì°° ìš”ì•½ (ë¹›/ê°ë„ ì˜í–¥ ë°°ì œ)",
  "confidence": 0.95
}

ì‹ ë¢°ë„(confidence) ê¸°ì¤€:
- 0.9-1.0: ë§¤ìš° ëª…í™•í•œ ë¶€í’ˆ (ê¸°ë³¸ ë¸”ë¡, í”Œë ˆì´íŠ¸ ë“±)
- 0.7-0.9: ë¹„êµì  ëª…í™•í•œ ë¶€í’ˆ (íŠ¹ìˆ˜ ë¶€í’ˆ, ì¥ì‹ ìš”ì†Œ)
- 0.5-0.7: ì• ë§¤í•œ ë¶€í’ˆ (ë³µì¡í•œ í˜•íƒœ, ì¸ì‡„ê°€ ìˆëŠ” ë¶€í’ˆ)
- 0.3-0.5: ë¶ˆí™•ì‹¤í•œ ë¶€í’ˆ (ì´ë¯¸ì§€ê°€ íë¦¬ê±°ë‚˜ ê°ë„ê°€ ë‚˜ì¨)
- 0.0-0.3: ë¶„ì„ ë¶ˆê°€ëŠ¥ (ì´ë¯¸ì§€ ì—†ìŒ ë˜ëŠ” ë„ˆë¬´ íë¦¼)`

    const requestBody = {
      model: LLM_CONFIG.model,
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: prompt
            },
            {
              type: 'image_url',
              image_url: {
                url: partImgUrl,
                detail: 'high'
              }
            }
          ]
        }
      ],
      max_tokens: LLM_CONFIG.maxTokens,
      temperature: LLM_CONFIG.temperature,
      response_format: { type: 'json_object' }
    }

    if (import.meta.env.DEV) {
      console.log('API ìš”ì²­ ì •ë³´:', {
        model: LLM_CONFIG.model,
        apiKey: LLM_CONFIG.apiKey ? 'ì„¤ì •ë¨' : 'ì—†ìŒ',
        imageUrl: partImgUrl,
        promptLength: prompt.length
      })
    }

    const response = await fetch(`${LLM_CONFIG.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${LLM_CONFIG.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(requestBody)
    })

    if (!response.ok) {
      const errorText = await response.text()
      console.error('API ì˜¤ë¥˜ ì‘ë‹µ:', errorText)
      throw new Error(`LLM API Error: ${response.status} - ${errorText}`)
    }

    const data = await response.json()
    if (import.meta.env.DEV) {
      console.log('LLM raw response:', data)
    }
    
    // ì‘ë‹µ êµ¬ì¡° í™•ì¸
    if (!data.choices || !data.choices[0] || !data.choices[0].message) {
      console.error('ì‘ë‹µ êµ¬ì¡° ì˜¤ë¥˜:', data)
      return createDefaultAnalysis(part)
    }
    
    // JSON ì‘ë‹µ ê°•ì œ ëª¨ë“œ: contentëŠ” JSON ë¬¸ìì—´ì´ì–´ì•¼ í•¨
    let parsed
    try {
      parsed = JSON.parse(data.choices[0].message.content)
    } catch (e) {
      // ì˜ˆì™¸ì ìœ¼ë¡œ í¬ë§·ì´ ì–´ê¸‹ë‚˜ëŠ” ê²½ìš° ê¸°ì¡´ íŒŒì„œë¡œ í´ë°±
      const llmResponse = data.choices[0].message.content || ''
      let jsonText = llmResponse
      const jsonBlockMatch = llmResponse.match(/```json\s*([\s\S]*?)\s*```/)
      if (jsonBlockMatch) {
        jsonText = jsonBlockMatch[1].trim()
      } else {
        const jsonObjectMatch = llmResponse.match(/\{[\s\S]*\}/)
        if (jsonObjectMatch) jsonText = jsonObjectMatch[0]
      }
      try {
        parsed = JSON.parse(jsonText)
      } catch (err) {
        console.error('JSON íŒŒì‹± ì‹¤íŒ¨:', err)
        return createDefaultAnalysis(part, partName, partNum)
      }
    }

    // 1ì°¨ ê²°ê³¼
    parsed.part_num = partNum

    // í•˜ì´ë¸Œë¦¬ë“œ ë³´ê°• íŠ¸ë¦¬ê±°: ë‚®ì€ confidence(<0.8), feature_text ì§§ìŒ(<40ì), key í•„ë“œ ëˆ„ë½ ì‹œ
    const needRefine = HYBRID_CONFIG.enabled && (
      (typeof parsed.confidence === 'number' && parsed.confidence < 0.8) ||
      (!parsed.feature_text || String(parsed.feature_text).length < 40) ||
      !parsed.recognition_hints || !parsed.distinguishing_features || !parsed.similar_parts
    )

    if (!needRefine) return parsed

    // 2ì°¨ ëª¨ë¸(4.1-mini)ë¡œ ë³´ê°• ìš”ì²­
    const refinePrompt = `ë‹¤ìŒ JSONì„ ë³´ê°•í•˜ì„¸ìš”. ëˆ„ë½ëœ í•„ë“œë¥¼ ì±„ìš°ê³ , plate/brickì˜ stud/tube, ê°ë„(ìˆë‹¤ë©´), ìƒ‰ìƒëª…(í‘œì¤€ëª…), í˜¼ë™ë˜ëŠ” ìœ ì‚¬ë¶€í’ˆì„ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ í•˜ì„¸ìš”. JSONë§Œ ì‘ë‹µ.

ì›ë³¸ JSON:\n${JSON.stringify(parsed)}`

    const refineBody = {
      model: HYBRID_CONFIG.secondaryModel,
      messages: [ { role: 'user', content: [ { type: 'text', text: refinePrompt } ] } ],
      max_tokens: LLM_CONFIG.maxTokens,
      temperature: 0.2,
      response_format: { type: 'json_object' }
    }

    const refineResp = await fetch(`${LLM_CONFIG.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: { 'Authorization': `Bearer ${LLM_CONFIG.apiKey}`, 'Content-Type': 'application/json' },
      body: JSON.stringify(refineBody)
    })

    if (!refineResp.ok) return parsed
    let refined
    try {
      const refineData = await refineResp.json()
      refined = JSON.parse(refineData.choices[0].message.content)
    } catch {
      return parsed
    }

    // ë³‘í•©: 2ì°¨ ê°’ì´ ìˆìœ¼ë©´ ìš°ì„ , ì—†ìœ¼ë©´ 1ì°¨ ìœ ì§€
    const merged = {
      ...parsed,
      ...refined,
      recognition_hints: { ...(parsed.recognition_hints || {}), ...(refined.recognition_hints || {}) },
      similar_parts: Array.isArray(refined?.similar_parts) && refined.similar_parts.length > 0 ? refined.similar_parts : parsed.similar_parts,
      distinguishing_features: Array.isArray(refined?.distinguishing_features) && refined.distinguishing_features.length > 0 ? refined.distinguishing_features : parsed.distinguishing_features
    }
    merged.part_num = partNum
    return merged
    
  } catch (error) {
    console.error('LLM ë¶„ì„ ì‹¤íŒ¨:', error)
    return createDefaultAnalysis(part)
  }
}

// ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ìƒì„±
function createDefaultAnalysis(part, partName = null, partNum = null) {
  const name = partName || part.part?.name || part.name || 'Unknown'
  const num = partNum || part.part_num || part.part?.part_num || 'Unknown'
  
  return {
    shape: `ë¶„ì„ ì‹¤íŒ¨: ${name}`,
    center_stud: false,
    groove: false,
    connection: 'unknown',
    function: 'unknown',
    feature_text: `ë¶€í’ˆ ${num}ì˜ ìë™ ìƒì„±ëœ ê¸°ë³¸ ì„¤ëª…`,
    recognition_hints: {
      top_view: 'ê¸°ë³¸ í˜•íƒœ',
      side_view: 'ê¸°ë³¸ í˜•íƒœ',
      unique_features: ['ê¸°ë³¸ íŠ¹ì§•']
    },
    similar_parts: [],
    distinguishing_features: ['ê¸°ë³¸ íŠ¹ì§•'],
    confidence: 0.1,
    part_num: num
  }
}

// í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ì„
function createTextOnlyAnalysis(part, partName, partNum) {
  return {
    shape: `í…ìŠ¤íŠ¸ ë¶„ì„: ${partName}`,
    center_stud: false,
    groove: false,
    connection: 'unknown',
    function: 'unknown',
    feature_text: `ë¶€í’ˆ ${partNum}ì˜ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê¸°ë³¸ ì„¤ëª…`,
    recognition_hints: {
      top_view: 'í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì •',
      side_view: 'í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì •',
      unique_features: ['í…ìŠ¤íŠ¸ ê¸°ë°˜ íŠ¹ì§•']
    },
    similar_parts: [],
    distinguishing_features: ['í…ìŠ¤íŠ¸ ê¸°ë°˜ íŠ¹ì§•'],
    confidence: 0.3,
    part_num: partNum
  }
}

// ì„ë² ë”© ìƒì„± í•¨ìˆ˜ export
// í…ìŠ¤íŠ¸ ì„ë² ë”© ë°°ì¹˜ + ìºì‹œ
export async function generateTextEmbeddingsBatch(analysisResults) {
  const results = []

  // 0) ì…ë ¥ ë°ì´í„° ì¤‘ë³µ ì œê±°: (part_num, color_id) ì¡°í•© ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
  const uniqueResults = []
  const seenEmbeddingKeys = new Set()
  
  for (const item of analysisResults) {
    const partNum = item.part_num || 'unknown'
    const colorId = item.color_id !== undefined ? item.color_id : (item.color?.id !== undefined ? item.color.id : null)
    const key = `${partNum}_${colorId}`
    
    if (!seenEmbeddingKeys.has(key)) {
      seenEmbeddingKeys.add(key)
      uniqueResults.push(item)
    } else {
      console.warn(`âš ï¸ Duplicate embedding input found for part_num=${partNum}, color_id=${colorId}, skipping`)
    }
  }
  
  console.log(`ğŸ“Š Embedding input deduplication: ${analysisResults.length} -> ${uniqueResults.length} results`)
  analysisResults = uniqueResults

  // 1) ê¸°ì¡´ ì„ë² ë”© ë³´ìœ /feature_text ëˆ„ë½ ì„ ë¶„ë¥˜
  const needsEmbedding = []
  for (const item of analysisResults) {
    const partNum = item.part_num || 'unknown'
    if (item.has_embedding === true) {
      console.log(`â­ï¸ Skipping embedding for ${partNum} - already has embedding`)
      results.push({ part_num: partNum, embedding: item.existing_embedding || null, feature_text: item.feature_text })
      continue
    }
    if (!item.feature_text) {
      console.warn(`âš ï¸ No feature text for ${partNum}, skipping embedding`)
      results.push({ part_num: partNum, embedding: null, error: 'feature_text missing' })
      continue
    }
    needsEmbedding.push(item)
  }

  if (needsEmbedding.length === 0) return results

  // 2) í…ìŠ¤íŠ¸ í•´ì‹œ ìºì‹œë¡œ ì¤‘ë³µ ì œê±°
  const textToIndices = new Map()
  const uniqueTexts = []
  needsEmbedding.forEach((item, idx) => {
    const key = stableTextKey(item.feature_text)
    if (!textToIndices.has(key)) {
      textToIndices.set(key, [])
      uniqueTexts.push(item.feature_text)
    }
    textToIndices.get(key).push(idx)
  })

  // 3) OpenAI Embeddings API ë‹¤ì¤‘ ì…ë ¥ ë°°ì¹˜ í˜¸ì¶œ
  try {
    const response = await fetch(`${CLIP_CONFIG.baseUrl}/embeddings`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${CLIP_CONFIG.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: CLIP_CONFIG.model,
        input: uniqueTexts,
        dimensions: CLIP_CONFIG.dimensions
      })
    })

    if (!response.ok) throw new Error(`Embedding API Error: ${response.status}`)
    const data = await response.json()

    // 4) ê²°ê³¼ ë§¤í•‘: ë™ì¼ í…ìŠ¤íŠ¸ ê³µìœ  ì¸ë±ìŠ¤ì— ë™ì¼ ì„ë² ë”© ë³µì œ
    uniqueTexts.forEach((text, uIdx) => {
      const embedding = data.data[uIdx].embedding
      const targetIndices = textToIndices.get(stableTextKey(text))
      for (const idx of targetIndices) {
        const src = needsEmbedding[idx]
        const enhancedText = buildEnhancedEmbeddingText({
          partName: src.part?.name,
          partNum: src.part_num,
          colorName: src.color?.name,
          featureText: src.feature_text,
          keypoints: src.keypoints,
          distinguishing: src.distinguishing_features,
          legoPartNumber: src.part?.external_ids?.lego || src.part?.external_ids?.Lego || null
        })
        results.push({ part_num: src.part_num || 'unknown', embedding, feature_text: enhancedText })
      }
    })
  } catch (error) {
    console.error('âŒ Batch embeddings failed:', error)
    // ì‹¤íŒ¨ ì‹œ ê°œë³„ í˜¸ì¶œ í´ë°±(ìµœì†Œí•œì˜ í’ˆì§ˆ ìœ ì§€)
    for (const src of needsEmbedding) {
      try {
        const r = await fetch(`${CLIP_CONFIG.baseUrl}/embeddings`, {
          method: 'POST',
          headers: { 'Authorization': `Bearer ${CLIP_CONFIG.apiKey}`, 'Content-Type': 'application/json' },
          body: JSON.stringify({ model: CLIP_CONFIG.model, input: src.feature_text, dimensions: CLIP_CONFIG.dimensions })
        })
        if (!r.ok) throw new Error(`Embedding API Error: ${r.status}`)
        const j = await r.json()
        const enhancedText = buildEnhancedEmbeddingText({
          partName: src.part?.name,
          partNum: src.part_num,
          colorName: src.color?.name,
          featureText: src.feature_text,
          keypoints: src.keypoints,
          distinguishing: src.distinguishing_features,
          legoPartNumber: src.part?.external_ids?.lego || src.part?.external_ids?.Lego || null
        })
        results.push({ part_num: src.part_num || 'unknown', embedding: j.data[0].embedding, feature_text: enhancedText })
      } catch (e) {
        results.push({ part_num: src.part_num || 'unknown', embedding: null, error: e.message })
      }
    }
  }

  return results
}

function stableTextKey(text) {
  return String(text).trim().toLowerCase()
}

// í‘œì¤€ íƒœê·¸ ì •ê·œí™” (ê²€ìƒ‰Â·í›„ì²˜ë¦¬ ìµœì í™”)
function normalizeShapeTag(raw) {
  const t = String(raw || '').toLowerCase()
  if (/(plate|í”Œë ˆì´íŠ¸)/.test(t)) return 'plate'
  if (/(brick|ë¸Œë¦­)/.test(t)) return 'brick'
  if (/(slope|ê²½ì‚¬)/.test(t)) return 'slope'
  if (/(tile|íƒ€ì¼)/.test(t)) return 'tile'
  if (/(animal|ë™ë¬¼)/.test(t)) return 'animal_figure'
  if (/(leaf|plant|ì|ì‹ë¬¼)/.test(t)) return 'plant_leaf'
  if (/(technic|í…Œí¬ë‹‰)/.test(t)) return 'technic'
  return t || 'unknown'
}

function normalizeFunctionTag(raw) {
  const t = String(raw || '').toLowerCase()
  if (/(basic|ê¸°ë³¸|building|êµ¬ì„±)/.test(t)) return 'building'
  if (/(decoration|ì¥ì‹)/.test(t)) return 'decoration'
  if (/(figure|í”¼ê·œì–´)/.test(t)) return 'figure'
  if (/(slope|ê²½ì‚¬)/.test(t)) return 'slope'
  return t || 'unknown'
}

// CLIP ìŠ¤íƒ€ì¼ ë¬¸êµ¬(ì§§ê³  í•µì‹¬ì–´ ìœ„ì£¼)ë¡œ ë³€í™˜
function clipifyPhrases(arr) {
  if (!Array.isArray(arr)) return []
  return arr
    .map(s => String(s || '').toLowerCase().trim())
    .filter(Boolean)
    .map(s => s
      .replace(/\b(with|and|the|of|for|a|an|in|on|to|by|from)\b/g, '')
      .replace(/\s+/g, ' ')
      .trim()
    )
    .map(s => s.length > 40 ? s.slice(0, 40) : s)
}

function buildEnhancedEmbeddingText({ partName, partNum, colorName, featureText, keypoints, distinguishing, legoPartNumber }) {
  const header = [
    partName ? `name:${partName}` : null,
    partNum ? `part:${partNum}` : null,
    legoPartNumber ? `lego:${legoPartNumber}` : null,
    colorName ? `color:${colorName}` : null
  ].filter(Boolean).join(' ')

  const keypointsText = Array.isArray(keypoints) && keypoints.length > 0
    ? ` keypoints:${keypoints.join('|')}`
    : ''
  const distinguishingText = Array.isArray(distinguishing) && distinguishing.length > 0
    ? ` distinguishing:${distinguishing.join('|')}`
    : ''

  return `${header} features:${featureText || ''}${keypointsText}${distinguishingText}`.trim()
}

// ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í•¨ìˆ˜ export
export async function saveToMasterPartsDB(analysisResults) {
  try {
    // 0) ì…ë ¥ ë°ì´í„° ì¤‘ë³µ ì œê±°: (part_num, color_id) ì¡°í•© ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
    const uniqueResults = []
    const seenAnalysisKeys = new Set()
    
    for (const result of analysisResults) {
      const partNum = result.part_num || 'unknown'
      const colorId = result.color_id !== undefined ? result.color_id : (result.color?.id !== undefined ? result.color.id : null)
      const key = `${partNum}_${colorId}`
      
      if (!seenAnalysisKeys.has(key)) {
        seenAnalysisKeys.add(key)
        uniqueResults.push(result)
      } else {
        console.warn(`âš ï¸ Duplicate analysis result found for part_num=${partNum}, color_id=${colorId}, skipping`)
      }
    }
    
    console.log(`ğŸ“Š Input deduplication: ${analysisResults.length} -> ${uniqueResults.length} results`)
    analysisResults = uniqueResults

    // 1) ëˆ„ë½ ì„ë² ë”© ë³´ì¶©: embedding ì—†ëŠ” ê²°ê³¼ë“¤ë§Œ ë°°ì¹˜ ìƒì„±
    const missingEmb = analysisResults.filter(r => !Array.isArray(r.embedding) || r.embedding.length === 0)
    if (missingEmb.length > 0) {
      try {
        const embResults = await generateTextEmbeddingsBatch(missingEmb)
        const embMap = new Map()
        for (const e of embResults) {
          if (Array.isArray(e.embedding)) embMap.set(e.part_num, e.embedding)
        }
        analysisResults.forEach(r => {
          if (!Array.isArray(r.embedding) || r.embedding.length === 0) {
            const emb = embMap.get(r.part_num)
            if (emb) r.embedding = emb
          }
        })
      } catch (e) {
        console.warn('âš ï¸ Failed to backfill embeddings; proceeding without some embeddings', e)
      }
    }

    // color_id í™•ì •: result.color_id ë˜ëŠ” result.color?.idì—ì„œ ì¶”ì¶œ, ì—†ìœ¼ë©´ ì €ì¥ ìŠ¤í‚µ
    const mapped = analysisResults.map(result => {
      const resolvedColorId = (result.color_id !== undefined && result.color_id !== null)
        ? result.color_id
        : (result.color?.id !== undefined ? result.color.id : null)

      // íƒœê·¸ ì •ê·œí™” + CLIP ìŠ¤íƒ€ì¼ ë¬¸êµ¬ ë³€í™˜
      const normalizedShape = normalizeShapeTag(result.shape)
      const normalizedFunction = normalizeFunctionTag(result.function)
      const clipDistinguishing = clipifyPhrases(result.distinguishing_features)
      const clipHints = {
        ...result.recognition_hints,
        unique_features: clipifyPhrases(result.recognition_hints?.unique_features)
      }

      return {
        part_id: result.part_num,
        part_name: result.part?.name || 'Unknown',
        part_category: result.part?.part_cat_id || null,
        color_id: resolvedColorId,
        feature_json: {
          shape: result.shape,
          center_stud: result.center_stud,
          groove: result.groove,
          connection: result.connection,
          function: result.function,
          recognition_hints: result.recognition_hints,
          similar_parts: result.similar_parts,
          distinguishing_features: result.distinguishing_features,
          shape_tag: normalizedShape,
          function_tag: normalizedFunction,
          clip_distinguishing: clipDistinguishing,
          clip_unique_features: clipHints.unique_features
        },
        feature_text: result.feature_text,
        clip_text_emb: Array.isArray(result.embedding) ? result.embedding : null,
        // ë³„ë„ ì»¬ëŸ¼ìœ¼ë¡œë„ ì €ì¥í•˜ì—¬ ê²€ìƒ‰ ìµœì í™”
        recognition_hints: result.recognition_hints || null,
        similar_parts: result.similar_parts || null,
        distinguishing_features: clipDistinguishing || null,
        confidence: result.confidence || 0.5
      }
    })

    // color_idê°€ ì—†ëŠ” ë ˆì½”ë“œëŠ” ì €ì¥ ìŠ¤í‚µ (ì¤‘ë³µ/ì¬ë¶„ì„ ìœ ë°œ ë°©ì§€)
    const validRecords = mapped.filter(r => r.color_id !== null && r.color_id !== undefined)
    const skipped = mapped.length - validRecords.length

    // ì¤‘ë³µ ì œê±°: (part_id, color_id) ì¡°í•©ì´ ì¤‘ë³µë˜ëŠ” ê²½ìš° ë§ˆì§€ë§‰ ê²ƒë§Œ ìœ ì§€
    const uniqueRecords = []
    const seenRecordKeys = new Set()
    
    // ì—­ìˆœìœ¼ë¡œ ìˆœíšŒí•˜ì—¬ ì¤‘ë³µëœ í‚¤ì˜ ê²½ìš° ë§ˆì§€ë§‰(ìµœì‹ ) ë ˆì½”ë“œë§Œ ìœ ì§€
    for (let i = validRecords.length - 1; i >= 0; i--) {
      const record = validRecords[i]
      const key = `${record.part_id}_${record.color_id}`
      
      if (!seenRecordKeys.has(key)) {
        seenRecordKeys.add(key)
        uniqueRecords.unshift(record) // ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•´ unshift ì‚¬ìš©
      } else {
        console.warn(`âš ï¸ Duplicate record found for part_id=${record.part_id}, color_id=${record.color_id}, skipping`)
      }
    }

    const records = uniqueRecords
    const duplicatesRemoved = validRecords.length - records.length

    console.log(`ğŸ’¾ Saving ${records.length} records to parts_master_features...`)
    if (skipped > 0) {
      console.warn(`âš ï¸ Skipping ${skipped} records without color_id to avoid null-color duplicates`)
    }
    if (duplicatesRemoved > 0) {
      console.warn(`âš ï¸ Removed ${duplicatesRemoved} duplicate records to avoid constraint violations`)
    }
    
    const { data, error } = await supabase
      .from('parts_master_features')
      .upsert(records, { 
        onConflict: 'part_id,color_id',
        ignoreDuplicates: false 
      })

    if (error) throw error

    console.log(`âœ… Successfully saved ${records.length} records to parts_master_features`)
    
    // ìºì‹œ ì—…ë°ì´íŠ¸
    records.forEach(record => {
      const cacheKey = `${record.part_id}_${record.color_id}`
      const result = {
        part_num: record.part_id,
        color_id: record.color_id,
        shape: record.feature_json?.shape || 'unknown',
        center_stud: record.feature_json?.center_stud || false,
        groove: record.feature_json?.groove || false,
        connection: record.feature_json?.connection || 'unknown',
        function: record.feature_json?.function || 'unknown',
        feature_text: record.feature_text,
        recognition_hints: record.feature_json?.recognition_hints || {},
        similar_parts: record.feature_json?.similar_parts || [],
        distinguishing_features: record.feature_json?.distinguishing_features || [],
        confidence: record.confidence || 0.5,
        embedding: record.clip_text_emb
      }
      analysisCache.set(cacheKey, result)
    })
    
    return { success: true, count: records.length }
    
  } catch (error) {
    console.error('âŒ Database save failed:', error)
    throw error
  }
}

// ê¸°ì¡´ ë¶„ì„ í™•ì¸ í•¨ìˆ˜ export
export async function checkExistingAnalysis(partNum, colorId) {
  try {
    const cacheKey = `${partNum}_${colorId}`
    console.log(`ğŸ” Checking existing analysis for ${partNum} (color: ${colorId})`)
    
    // colorIdê°€ ì—†ìœ¼ë©´, null-colorë¡œ ì €ì¥ëœ ê¸°ì¡´ ë ˆì½”ë¦¬ë„ ì¡´ì¬ë¡œ ê°„ì£¼í•˜ì—¬ ì¦‰ì‹œ ìŠ¤í‚µ
    if (colorId === null || colorId === undefined) {
      console.warn(`âš ï¸ colorId is missing for ${partNum}; treating as existing to avoid duplicate LLM runs`)
      return { part_num: partNum, color_id: null, feature_text: '', embedding: null }
    }

    // 1. ë¨¼ì € ìºì‹œì—ì„œ í™•ì¸
    if (analysisCache.has(cacheKey)) {
      console.log(`âœ… Found in cache for ${partNum} (color: ${colorId})`)
      return analysisCache.get(cacheKey)
    }
    
    // 2. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í™•ì¸
    const { data, error: dbError } = await supabase
      .from('parts_master_features')
      .select('part_id, color_id, feature_json, feature_text, clip_text_emb, confidence')
      .eq('part_id', partNum)
      .eq('color_id', parseInt(colorId))
      .maybeSingle()
    
    console.log(`ğŸ” Query result for ${partNum} (color: ${colorId}):`, { data, error: dbError })

    if (dbError) {
      console.warn(`âš ï¸ Database error checking existing analysis:`, dbError)
      return null // ì˜¤ë¥˜ ì‹œ ìƒˆë¡œ ë¶„ì„
    }
    
    // ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
    if (!data) {
      console.log(`ğŸ“ No existing analysis found for ${partNum} (color: ${colorId})`)
      return null
    }
    
    console.log(`âœ… Found existing analysis for ${partNum} (color: ${colorId})`)
    console.log(`ğŸ“Š Existing data: part_id=${data.part_id}, confidence=${data.confidence}, has_embedding=${!!data.clip_text_emb}`)
    
    // part_idê°€ nullì¸ ê²½ìš° ì²˜ë¦¬
    if (!data.part_id) {
      console.log(`âš ï¸ Found record but part_id is null for ${partNum} (color: ${colorId}), skipping`)
      return null
    }
    
    // ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ë¥¼ í˜„ì¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    const result = {
      part_num: data.part_id,
      color_id: data.color_id,
      shape: data.feature_json?.shape || 'unknown',
      center_stud: data.feature_json?.center_stud || false,
      groove: data.feature_json?.groove || false,
      connection: data.feature_json?.connection || 'unknown',
      function: data.feature_json?.function || 'unknown',
      feature_text: data.feature_text,
      recognition_hints: data.feature_json?.recognition_hints || {},
      similar_parts: data.feature_json?.similar_parts || [],
      distinguishing_features: data.feature_json?.distinguishing_features || [],
      confidence: data.confidence || 0.5,
      embedding: data.clip_text_emb,
      has_embedding: !!data.clip_text_emb,
      existing_embedding: data.clip_text_emb
    }
    
    // ìºì‹œì— ì €ì¥
    analysisCache.set(cacheKey, result)
    return result
    
  } catch (error) {
    console.error('âŒ Check existing analysis failed:', error)
    return null
  }
}

// ì „ì—­ ìºì‹œë¡œ ì¤‘ë³µ ì²´í¬
const analysisCache = new Map()

export function useMasterPartsPreprocessing() {
  const loading = ref(false)
  const error = ref(null)
  const processing = ref(false)
  const progress = ref(0)

  // ëª¨ë“  Rebrickable ë¶€í’ˆ ìˆ˜ì§‘
  const collectAllRebrickableParts = async () => {
    loading.value = true
    error.value = null

    try {
      const allParts = []
      let page = 1
      const pageSize = 1000

      while (true) {
        const response = await fetch(`https://rebrickable.com/api/v3/lego/parts/?page=${page}&page_size=${pageSize}`, {
          headers: {
            'Authorization': `key ${import.meta.env.VITE_REBRICKABLE_API_KEY}`,
            'Content-Type': 'application/json'
          }
        })

        if (!response.ok) break

        const data = await response.json()
        if (!data.results || data.results.length === 0) break

        allParts.push(...data.results)
        page++

        // API ì œí•œ ê³ ë ¤
        await new Promise(resolve => setTimeout(resolve, 100))
      }

      console.log(`Collected ${allParts.length} parts from Rebrickable`)
      return allParts
    } catch (err) {
      error.value = err.message
      throw err
    } finally {
      loading.value = false
    }
  }

  // ë¶€í’ˆë³„ LLM ë¶„ì„ (ë°°ì¹˜ ì²˜ë¦¬)
  const analyzePartsBatch = async (parts, batchSize = 10) => {
    processing.value = true
    error.value = null
    progress.value = 0

    try {
      const results = []
      const errors = []

      for (let i = 0; i < parts.length; i += batchSize) {
        const batch = parts.slice(i, i + batchSize)
        console.log(`Processing batch ${Math.floor(i / batchSize) + 1}/${Math.ceil(parts.length / batchSize)}`)

        const batchPromises = batch.map(async (part, index) => {
          try {
            const analysis = await analyzePartWithLLM(part)
            return { part, analysis, success: true }
          } catch (err) {
            console.error(`Failed to analyze part ${part.part_num}:`, err)
            return { part, error: err.message, success: false }
          }
        })

        const batchResults = await Promise.all(batchPromises)
        
        for (const result of batchResults) {
          if (result.success) {
            results.push(result)
          } else {
            errors.push(result)
          }
        }

        // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        progress.value = Math.round((i + batchSize) / parts.length * 100)

        // API ì œí•œì„ ìœ„í•œ ì§€ì—°
        await new Promise(resolve => setTimeout(resolve, 2000))
      }

      return { results, errors }
    } catch (err) {
      error.value = err.message
      throw err
    } finally {
      processing.value = false
    }
  }

  // ê°œë³„ ë¶€í’ˆ LLM ë¶„ì„ (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ Aë‹¨ê³„)
  const analyzePartWithLLM = async (part) => {
    try {
      if (import.meta.env.DEV) {
      console.log('ë¶„ì„í•  ë¶€í’ˆ ì •ë³´:', part)
    }
      
      // ë¶€í’ˆ ì •ë³´ í™•ì¸ ë° ì •ë¦¬
      const partName = part.part?.name || part.name || 'Unknown'
      const partNum = part.part_num || part.part?.part_num || 'Unknown'
      const partImgUrl = part.part?.part_img_url || part.part_img_url || null
      
      console.log('ì •ë¦¬ëœ ë¶€í’ˆ ì •ë³´:', { partName, partNum, partImgUrl })
      
      // ì´ë¯¸ì§€ URLì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰
      if (!partImgUrl) {
        console.warn(`ë¶€í’ˆ ${partNum}ì˜ ì´ë¯¸ì§€ URLì´ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.`)
        return createTextOnlyAnalysis(part, partName, partNum)
      }
      
      const prompt = `ë‹¤ìŒ ë ˆê³  ë¶€í’ˆì„ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.

ë¶€í’ˆ ì •ë³´:
- ë¶€í’ˆëª…: ${partName}
- ë¶€í’ˆ ë²ˆí˜¸: ${partNum}

ì‘ë‹µ í˜•ì‹:
{
  "shape": "ë¶€í’ˆì˜ ê¸°ë³¸ í˜•íƒœ",
  "center_stud": true/false,
  "groove": true/false,
  "connection": "ì—°ê²° ë°©ì‹",
  "function": "ì£¼ìš” ê¸°ëŠ¥",
  "feature_text": "ë¶€í’ˆ íŠ¹ì§•ì„ ì„¤ëª…í•˜ëŠ” í…ìŠ¤íŠ¸",
  "recognition_hints": {
    "top_view": "ìœ„ì—ì„œ ë³¸ ëª¨ìŠµ",
    "side_view": "ì˜†ì—ì„œ ë³¸ ëª¨ìŠµ",
    "unique_features": ["ê³ ìœ  íŠ¹ì§•ë“¤"]
  },
  "similar_parts": ["ìœ ì‚¬í•œ ë¶€í’ˆ ë²ˆí˜¸ë“¤"],
  "distinguishing_features": ["êµ¬ë³„ë˜ëŠ” íŠ¹ì§•ë“¤"],
  "confidence": 0.95
}

ì‹ ë¢°ë„(confidence) ê¸°ì¤€:
- 0.9-1.0: ë§¤ìš° ëª…í™•í•œ ë¶€í’ˆ (ê¸°ë³¸ ë¸”ë¡, í”Œë ˆì´íŠ¸ ë“±)
- 0.7-0.9: ë¹„êµì  ëª…í™•í•œ ë¶€í’ˆ (íŠ¹ìˆ˜ ë¶€í’ˆ, ì¥ì‹ ìš”ì†Œ)
- 0.5-0.7: ì• ë§¤í•œ ë¶€í’ˆ (ë³µì¡í•œ í˜•íƒœ, ì¸ì‡„ê°€ ìˆëŠ” ë¶€í’ˆ)
- 0.3-0.5: ë¶ˆí™•ì‹¤í•œ ë¶€í’ˆ (ì´ë¯¸ì§€ê°€ íë¦¬ê±°ë‚˜ ê°ë„ê°€ ë‚˜ì¨)
- 0.0-0.3: ë¶„ì„ ë¶ˆê°€ëŠ¥ (ì´ë¯¸ì§€ ì—†ìŒ ë˜ëŠ” ë„ˆë¬´ íë¦¼)`

      const requestBody = {
        model: LLM_CONFIG.model,
        messages: [
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: prompt
              },
              {
                type: 'image_url',
                image_url: {
                  url: partImgUrl,
                  detail: 'high'
                }
              }
            ]
          }
        ],
        max_tokens: LLM_CONFIG.maxTokens,
        temperature: LLM_CONFIG.temperature
      }

      console.log('API ìš”ì²­ ì •ë³´:', {
        model: LLM_CONFIG.model,
        apiKey: LLM_CONFIG.apiKey ? 'ì„¤ì •ë¨' : 'ì—†ìŒ',
        imageUrl: partImgUrl,
        promptLength: prompt.length
      })

      const response = await fetch(`${LLM_CONFIG.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${LLM_CONFIG.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(requestBody)
      })

      if (!response.ok) {
        const errorText = await response.text()
        console.error('API ì˜¤ë¥˜ ì‘ë‹µ:', errorText)
        throw new Error(`LLM API Error: ${response.status} - ${errorText}`)
      }

      const data = await response.json()
      if (import.meta.env.DEV) {
      console.log('LLM raw response:', data)
    }
      
      // ì‘ë‹µ êµ¬ì¡° í™•ì¸
      if (!data.choices || !data.choices[0] || !data.choices[0].message) {
        console.error('ì‘ë‹µ êµ¬ì¡° ì˜¤ë¥˜:', data)
        return createDefaultAnalysis(part)
      }
      
      const llmResponse = data.choices[0].message.content
      console.log('LLM ì‘ë‹µ ë‚´ìš©:', llmResponse)

      // JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json ... ``` ë˜ëŠ” { ... } íŒ¨í„´ ì°¾ê¸°)
      let jsonText = llmResponse
      
      // ```json ... ``` íŒ¨í„´ ì°¾ê¸°
      const jsonBlockMatch = llmResponse.match(/```json\s*([\s\S]*?)\s*```/)
      if (jsonBlockMatch) {
        jsonText = jsonBlockMatch[1].trim()
        console.log('ì¶”ì¶œëœ JSON ë¸”ë¡:', jsonText)
      } else {
        // ```json íŒ¨í„´ì´ ì—†ìœ¼ë©´ { ... } íŒ¨í„´ ì°¾ê¸°
        const jsonObjectMatch = llmResponse.match(/\{[\s\S]*\}/)
        if (jsonObjectMatch) {
          jsonText = jsonObjectMatch[0]
          console.log('ì¶”ì¶œëœ JSON ê°ì²´:', jsonText)
        }
      }

      // JSON íŒŒì‹±
      let analysisResult
      try {
        analysisResult = JSON.parse(jsonText)
        // part_num ì¶”ê°€ (LLM ì‘ë‹µì—ëŠ” ì—†ìœ¼ë¯€ë¡œ ìˆ˜ë™ ì¶”ê°€)
        analysisResult.part_num = partNum
        console.log('íŒŒì‹±ëœ ë¶„ì„ ê²°ê³¼:', analysisResult)
      } catch (parseError) {
        console.error('JSON íŒŒì‹± ì‹¤íŒ¨:', parseError)
        console.log('ì¶”ì¶œëœ JSON í…ìŠ¤íŠ¸:', jsonText)
        console.log('ì›ë³¸ ì‘ë‹µ:', llmResponse)
        // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
        analysisResult = createDefaultAnalysis(part)
        analysisResult.part_num = partNum
      }

      return analysisResult
    } catch (err) {
      console.error('LLM analysis failed:', err)
      return createDefaultAnalysis(part)
    }
  }

  // ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµìš©)
  const createDefaultAnalysis = (part) => {
    const partNum = part.part_num || part.part?.part_num || 'unknown'
    return {
      part_num: partNum,
      shape: `ë¶„ì„ ì‹¤íŒ¨: ${part.name || part.part?.name}`,
      center_stud: false,
      groove: false,
      connection: 'unknown',
      function: 'unknown',
      feature_text: `ë¶„ì„ ì‹¤íŒ¨: ${part.name || part.part?.name}`,
      recognition_hints: {
        top_view: 'ë¶„ì„ ì‹¤íŒ¨',
        side_view: 'ë¶„ì„ ì‹¤íŒ¨',
        unique_features: []
      },
      similar_parts: [],
      distinguishing_features: [],
      confidence: 0.3
    }
  }

  // í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ì„ (ì´ë¯¸ì§€ URLì´ ì—†ì„ ë•Œ)
  const createTextOnlyAnalysis = (part, partName, partNum) => {
    return {
      part_num: partNum,
      shape: `í…ìŠ¤íŠ¸ ë¶„ì„: ${partName}`,
      center_stud: false,
      groove: false,
      connection: 'unknown',
      function: 'unknown',
      feature_text: `í…ìŠ¤íŠ¸ ë¶„ì„: ${partName}`,
      recognition_hints: {
        top_view: 'ì´ë¯¸ì§€ ì—†ìŒ',
        side_view: 'ì´ë¯¸ì§€ ì—†ìŒ',
        unique_features: []
      },
      similar_parts: [],
      distinguishing_features: [],
      confidence: 0.3
    }
  }

  // CLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµìš©)
  const generateClipTextEmbedding = async (featureText) => {
    try {
      if (!CLIP_CONFIG.apiKey) {
        throw new Error('Missing VITE_OPENAI_API_KEY')
      }
      const response = await fetch(`${CLIP_CONFIG.baseUrl}/embeddings`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${CLIP_CONFIG.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: CLIP_CONFIG.model,
          input: featureText,
          dimensions: CLIP_CONFIG.dimensions
        })
      })

      if (!response.ok) {
        throw new Error(`CLIP API Error: ${response.status}`)
      }

      const data = await response.json()
      return data.data[0].embedding
    } catch (err) {
      console.error('CLIP text embedding generation failed:', err)
      throw err
    }
  }

  // CLIP ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± (ì‹¤ì‹œê°„ ë§¤ì¹­ìš©)
  const generateClipImageEmbedding = async (imageUrl) => {
    try {
      // ì´ë¯¸ì§€ ì„ë² ë”©ì€ ì„œë²„/ì™¸ë¶€ APIì—ì„œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
      // í™˜ê²½ë³€ìˆ˜: VITE_CLIP_IMAGE_API_URL (POST base64 or URL)
      const endpoint = import.meta.env.VITE_CLIP_IMAGE_API_URL
      if (!endpoint) {
        throw new Error('Missing VITE_CLIP_IMAGE_API_URL')
      }
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ image_url: imageUrl, dimensions: CLIP_CONFIG.dimensions })
      })
      if (!response.ok) throw new Error(`Image embedding API Error: ${response.status}`)
      const data = await response.json()
      if (!data.embedding) throw new Error('Image embedding API response missing embedding')
      return data.embedding
    } catch (err) {
      console.error('CLIP image embedding generation failed:', err)
      throw err
    }
  }

  // ë§ˆìŠ¤í„° ë¶€í’ˆ DBì— ì €ì¥ (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµìš©)
  const saveToMasterPartsDB = async (analysisResults) => {
    try {
      const records = analysisResults.map(result => ({
        part_id: result.part_num,
        part_name: result.part?.name || 'Unknown',
        part_category: result.part?.part_cat_id || null,
        color_id: result.color?.id || null,
        feature_json: {
          shape: result.shape,
          center_stud: result.center_stud,
          groove: result.groove,
          connection: result.connection,
          function: result.function
        },
        feature_text: result.feature_text,
        clip_text_emb: result.clip_text_emb,
        recognition_hints: result.recognition_hints,
        similar_parts: result.similar_parts,
        distinguishing_features: result.distinguishing_features,
        confidence: result.confidence,
        usage_frequency: 0,
        detection_accuracy: 0.0,
        created_at: new Date().toISOString()
      }))

      const { data, error: dbError } = await supabase
        .from('parts_master_features')
        .upsert(records, {
          onConflict: 'part_id,color_id'
        })
        .select()

      if (dbError) throw dbError
      return data
    } catch (err) {
      error.value = err.message
      throw err
    }
  }

  // ì „ì²´ ë§ˆìŠ¤í„° DB êµ¬ì¶• í”„ë¡œì„¸ìŠ¤
  const buildMasterPartsDatabase = async () => {
    processing.value = true
    error.value = null
    progress.value = 0

    try {
      console.log('Starting master parts database construction...')

      // 1. ëª¨ë“  Rebrickable ë¶€í’ˆ ìˆ˜ì§‘
      console.log('Step 1: Collecting all Rebrickable parts...')
      const allParts = await collectAllRebrickableParts()
      console.log(`Collected ${allParts.length} parts`)

      // 2. ë¶€í’ˆë³„ LLM ë¶„ì„ (ë°°ì¹˜ ì²˜ë¦¬)
      console.log('Step 2: Analyzing parts with LLM...')
      const analysisResults = await analyzePartsBatch(allParts, 5) // ì‘ì€ ë°°ì¹˜ í¬ê¸°
      console.log(`Analyzed ${analysisResults.results.length} parts successfully`)
      console.log(`Failed to analyze ${analysisResults.errors.length} parts`)

      // 3. CLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
      console.log('Step 3: Generating CLIP text embeddings...')
      const embeddingResults = await generateTextEmbeddingsBatch(analysisResults.results)

      // 4. ë§ˆìŠ¤í„° DBì— ì €ì¥
      console.log('Step 4: Saving to master database...')
      const savedRecords = await saveToMasterPartsDB(embeddingResults)

      console.log(`Master parts database construction completed!`)
      console.log(`Total records saved: ${savedRecords.length}`)

      return {
        totalParts: allParts.length,
        analyzedParts: analysisResults.results.length,
        failedParts: analysisResults.errors.length,
        savedRecords: savedRecords.length
      }
    } catch (err) {
      error.value = err.message
      throw err
    } finally {
      processing.value = false
    }
  }

  // CLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ë°°ì¹˜ ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµìš©)
  const generateTextEmbeddingsBatch = async (analysisResults) => {
    const results = []

    for (const result of analysisResults) {
      try {
        // part_num í™•ì¸ ë° ì²˜ë¦¬
        const partNum = result.part_num || 'unknown'
        
        if (!result.feature_text) {
          console.warn(`ë¶€í’ˆ ${partNum}ì˜ feature_textê°€ ì—†ìŠµë‹ˆë‹¤.`)
          results.push({
            ...result,
            clip_text_emb: null
          })
          continue
        }

        const textEmbedding = await generateClipTextEmbedding(result.feature_text)
        results.push({
          ...result,
          clip_text_emb: textEmbedding
        })
      } catch (err) {
        const partNum = result.part_num || 'unknown'
        console.error(`Failed to generate text embedding for ${partNum}:`, err)
        results.push({
          ...result,
          clip_text_emb: null
        })
      }
    }

    return results
  }

  // ë§ˆìŠ¤í„° DB ìƒíƒœ í™•ì¸ (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµìš©)
  const checkMasterDBStatus = async () => {
    try {
      const { data, error: dbError } = await supabase
        .from('parts_master_features')
        .select('part_id, created_at, confidence, usage_frequency, detection_accuracy')
        .order('created_at', { ascending: false })
        .limit(1000)

      if (dbError) throw dbError

      const stats = {
        totalRecords: data.length,
        averageConfidence: data.reduce((sum, record) => sum + (record.confidence || 0), 0) / data.length,
        lastUpdated: data[0]?.created_at,
        highConfidence: data.filter(r => r.confidence > 0.8).length,
        lowConfidence: data.filter(r => r.confidence < 0.5).length,
        averageUsageFrequency: data.reduce((sum, record) => sum + (record.usage_frequency || 0), 0) / data.length,
        averageDetectionAccuracy: data.reduce((sum, record) => sum + (record.detection_accuracy || 0), 0) / data.length
      }

      return stats
    } catch (err) {
      error.value = err.message
      throw err
    }
  }

  // ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í™•ì¸ (ì¤‘ë³µ ë°©ì§€ìš©)
  const checkExistingAnalysis = async (partColorPairs) => {
    try {
      const { data, error: dbError } = await supabase
        .from('parts_master_features')
        .select('part_id, color_id, feature_json, feature_text, clip_text_emb, confidence')
        .in('part_id', partColorPairs.map(p => p.part_num))
        .in('color_id', partColorPairs.map(p => p.color_id))

      if (dbError) throw dbError
      
      // ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ë¥¼ í˜„ì¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
      const existingResults = data.map(record => ({
        part_num: record.part_id,
        color_id: record.color_id,
        shape: record.feature_json?.shape || 'unknown',
        center_stud: record.feature_json?.center_stud || false,
        groove: record.feature_json?.groove || false,
        connection: record.feature_json?.connection || 'unknown',
        function: record.feature_json?.function || 'unknown',
        feature_text: record.feature_text,
        recognition_hints: record.feature_json?.recognition_hints || {},
        similar_parts: record.feature_json?.similar_parts || [],
        distinguishing_features: record.feature_json?.distinguishing_features || [],
        confidence: record.confidence,
        clip_text_emb: record.clip_text_emb,
        is_existing: true // ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ì„ì„ í‘œì‹œ
      }))
      
      console.log(`ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ${existingResults.length}ê°œ ë°œê²¬`)
      return existingResults
    } catch (err) {
      console.error('ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í™•ì¸ ì‹¤íŒ¨:', err)
      return []
    }
  }

  return {
    loading,
    error,
    processing,
    progress,
    collectAllRebrickableParts,
    analyzePartsBatch,
    analyzePartWithLLM,
    generateClipTextEmbedding,
    generateClipImageEmbedding,
    saveToMasterPartsDB,
    buildMasterPartsDatabase,
    generateTextEmbeddingsBatch,
    checkMasterDBStatus,
    checkExistingAnalysis
  }
}
