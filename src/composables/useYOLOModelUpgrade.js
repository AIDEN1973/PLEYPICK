import { ref, reactive } from 'vue'

/**
 * YOLO ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ ë¡œì§ (ê¸°ìˆ ë¬¸ì„œ 4.1)
 * ì†Œí˜• Recall â‰¥ 0.95 and FPS â‰¥ 5 â†’ ì±„íƒ
 * ë¯¸ë‹¬ ì‹œ 960 ì¬ë²¤ì¹˜ â†’ ë¯¸ë‹¬ ì‹œ v8-L-seg ìŠ¹ê¸‰
 */
export function useYOLOModelUpgrade() {
  const loading = ref(false)
  const error = ref(null)
  const upgradeStats = reactive({
    totalBenchmarks: 0,
    successfulUpgrades: 0,
    fallbackToV8L: 0,
    currentModel: 'yolo11m-seg@768'
  })

  // ë²¤ì¹˜ë§ˆí¬ ì„¤ì •
  const benchmarkConfig = {
    testSets: 3,           // ëŒ€í‘œ ì„¸íŠ¸ 3ì¢…
    testFrames: 500,       // ì„¸íŠ¸ë‹¹ 500í”„ë ˆì„
    smallRecallThreshold: 0.95, // ì†Œí˜• Recall â‰¥ 0.95
    fpsThreshold: 5,       // FPS â‰¥ 5
    models: [
      { name: 'yolo11m-seg', size: 768, priority: 1 },
      { name: 'yolo11m-seg', size: 960, priority: 2 },
      { name: 'yolov8-l-seg', size: 768, priority: 3 }
    ]
  }

  /**
   * ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ê¸°ìˆ ë¬¸ì„œ 4.2)
   */
  const benchmarkModel = async (modelConfig, realData) => {
    const startTime = performance.now()
    
    try {
      console.log(`ğŸ” ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: ${modelConfig.name}@${modelConfig.size} (ê¸°ìˆ ë¬¸ì„œ 4.2)`)
      
      const results = {
        model: modelConfig.name,
        size: modelConfig.size,
        smallRecall: 0,
        fps: 0,
        avgLatency: 0,
        totalFrames: 0,
        successfulDetections: 0
      }
      
      // ê° ì„¸íŠ¸ì— ëŒ€í•´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
      for (const dataSet of realData) {
        const setResults = await runSetBenchmark(modelConfig, dataSet)
        
        results.smallRecall += setResults.smallRecall
        results.fps += setResults.fps
        results.avgLatency += setResults.avgLatency
        results.totalFrames += setResults.totalFrames
        results.successfulDetections += setResults.successfulDetections
      }
      
      // í‰ê·  ê³„ì‚°
      const setCount = realData.length
      results.smallRecall /= setCount
      results.fps /= setCount
      results.avgLatency /= setCount
      
      const benchmarkTime = performance.now() - startTime
      results.benchmarkTime = benchmarkTime
      
      console.log(`âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: ${modelConfig.name}@${modelConfig.size}`)
      console.log(`ğŸ“Š ê²°ê³¼: Recall=${results.smallRecall.toFixed(3)}, FPS=${results.fps.toFixed(1)}`)
      
      return results
      
    } catch (err) {
      console.error(`âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: ${modelConfig.name}@${modelConfig.size}`, err)
      throw err
    }
  }

  /**
   * ì„¸íŠ¸ë³„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
   */
  const runSetBenchmark = async (modelConfig, testSet) => {
    const { setNum, frames, smallParts } = testSet
    const results = {
      smallRecall: 0,
      fps: 0,
      avgLatency: 0,
      totalFrames: frames.length,
      successfulDetections: 0
    }
    
    let totalLatency = 0
    let smallPartDetections = 0
    
    for (const frame of frames) {
      const startTime = performance.now()
      
      try {
        // ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
        const detections = await runModelInference(modelConfig, frame)
        
        const latency = performance.now() - startTime
        totalLatency += latency
        
        // ì†Œí˜• ë¶€í’ˆ ê²€ì¶œ í™•ì¸
        const smallPartsDetected = detections.filter(det => 
          smallParts.some(part => isSmallPart(det, part))
        )
        smallPartDetections += smallPartsDetected.length
        
        results.successfulDetections += detections.length
        
      } catch (err) {
        console.warn(`í”„ë ˆì„ ì²˜ë¦¬ ì‹¤íŒ¨: ${frame.id}`, err)
      }
    }
    
    // ì†Œí˜• Recall ê³„ì‚°
    const totalSmallParts = smallParts.length * frames.length
    results.smallRecall = totalSmallParts > 0 ? smallPartDetections / totalSmallParts : 0
    
    // FPS ê³„ì‚°
    results.avgLatency = totalLatency / frames.length
    results.fps = 1000 / results.avgLatency
    
    return results
  }

  /**
   * ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (ì‹¤ì œ êµ¬í˜„ í•„ìš”)
   */
  const runModelInference = async (modelConfig, frame) => {
    // ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  ë¡œì§ êµ¬í˜„ í•„ìš”
    // ì‹¤ì œ êµ¬í˜„ í•„ìš”
    throw new Error('ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  ë¡œì§ì´ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤')
  }

  /**
   * ì†Œí˜• ë¶€í’ˆ íŒë³„
   */
  const isSmallPart = (detection, smallPart) => {
    // ì†Œí˜• ë¶€í’ˆ íŒë³„ ë¡œì§ (êµ¬í˜„ í•„ìš”)
    const bbox = detection.boundingBox
    const area = bbox.width * bbox.height
    return area < 0.01 // 1% ì´í•˜ ë©´ì 
  }

  /**
   * ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ íŒŒì´í”„ë¼ì¸
   */
  const runUpgradePipeline = async (realData, options = {}) => {
    try {
      loading.value = true
      console.log('ğŸš€ YOLO ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ íŒŒì´í”„ë¼ì¸ ì‹œì‘...')
      
      const results = []
      
      // 1. ê¸°ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ (yolo11m-seg@768)
      const baseModel = benchmarkConfig.models[0]
      const baseResults = await benchmarkModel(baseModel, realData)
      results.push(baseResults)
      
      // 2. SLO í™•ì¸
      if (baseResults.smallRecall >= benchmarkConfig.smallRecallThreshold && 
          baseResults.fps >= benchmarkConfig.fpsThreshold) {
        console.log('âœ… ê¸°ë³¸ ëª¨ë¸ SLO ì¶©ì¡±, ì—…ê·¸ë ˆì´ë“œ ë¶ˆí•„ìš”')
        upgradeStats.currentModel = `${baseModel.name}@${baseModel.size}`
        return { selected: baseResults, candidates: results }
      }
      
      console.log('âš ï¸ ê¸°ë³¸ ëª¨ë¸ SLO ë¯¸ë‹¬, 960 ì¬ë²¤ì¹˜ ì‹œì‘')
      
      // 3. 960 í¬ê¸°ë¡œ ì¬ë²¤ì¹˜
      const resizedModel = { ...baseModel, size: 960 }
      const resizedResults = await benchmarkModel(resizedModel, realData)
      results.push(resizedResults)
      
      // 4. 960 ê²°ê³¼ í™•ì¸
      if (resizedResults.smallRecall >= benchmarkConfig.smallRecallThreshold && 
          resizedResults.fps >= benchmarkConfig.fpsThreshold) {
        console.log('âœ… 960 ëª¨ë¸ SLO ì¶©ì¡±')
        upgradeStats.currentModel = `${resizedModel.name}@${resizedModel.size}`
        upgradeStats.successfulUpgrades++
        return { selected: resizedResults, candidates: results }
      }
      
      console.log('âš ï¸ 960 ëª¨ë¸ë„ SLO ë¯¸ë‹¬, v8-L-seg ìŠ¹ê¸‰')
      
      // 5. v8-L-segë¡œ ìŠ¹ê¸‰
      const v8LModel = benchmarkConfig.models[2]
      const v8LResults = await benchmarkModel(v8LModel, realData)
      results.push(v8LResults)
      
      upgradeStats.currentModel = `${v8LModel.name}@${v8LModel.size}`
      upgradeStats.fallbackToV8L++
      
      console.log('âœ… v8-L-seg ìŠ¹ê¸‰ ì™„ë£Œ')
      
      return { selected: v8LResults, candidates: results }
      
    } catch (err) {
      error.value = err.message
      console.error('âŒ ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ ì‹¤íŒ¨:', err)
      throw err
    } finally {
      loading.value = false
    }
  }

  /**
   * ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
   */
  const compareModels = (results) => {
    const comparison = {
      bestRecall: null,
      bestFPS: null,
      bestLatency: null,
      overallBest: null
    }
    
    // ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
    comparison.bestRecall = results.reduce((best, current) => 
      current.smallRecall > best.smallRecall ? current : best
    )
    
    comparison.bestFPS = results.reduce((best, current) => 
      current.fps > best.fps ? current : best
    )
    
    comparison.bestLatency = results.reduce((best, current) => 
      current.avgLatency < best.avgLatency ? current : best
    )
    
    // ì¢…í•© ì ìˆ˜ ê³„ì‚°
    const scoredResults = results.map(result => ({
      ...result,
      score: (result.smallRecall * 0.4) + (result.fps / 10 * 0.3) + ((1000 - result.avgLatency) / 1000 * 0.3)
    }))
    
    comparison.overallBest = scoredResults.reduce((best, current) => 
      current.score > best.score ? current : best
    )
    
    return comparison
  }

  /**
   * ëª¨ë¸ ë°°í¬ ì¤€ë¹„
   */
  const prepareModelDeployment = async (selectedModel, options = {}) => {
    console.log(`ğŸš€ ëª¨ë¸ ë°°í¬ ì¤€ë¹„: ${selectedModel.model}@${selectedModel.size}`)
    
    const deployment = {
      model: selectedModel.model,
      size: selectedModel.size,
      config: {
        confThreshold: 0.15,
        iouThreshold: 0.60,
        maxDetections: 1200,
        imgsz: selectedModel.size
      },
      performance: {
        smallRecall: selectedModel.smallRecall,
        fps: selectedModel.fps,
        avgLatency: selectedModel.avgLatency
      },
      deploymentTime: new Date().toISOString()
    }
    
    // ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë°°í¬ (êµ¬í˜„ í•„ìš”)
    await downloadModelFiles(selectedModel)
    await updateModelRegistry(deployment)
    
    console.log('âœ… ëª¨ë¸ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ')
    return deployment
  }

  /**
   * ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
   */
  const downloadModelFiles = async (model) => {
    // ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë¡œì§ (êµ¬í˜„ í•„ìš”)
    console.log(`ğŸ“¥ ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ: ${model.model}@${model.size}`)
  }

  /**
   * ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸
   */
  const updateModelRegistry = async (deployment) => {
    // ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸ ë¡œì§ (êµ¬í˜„ í•„ìš”)
    console.log('ğŸ“ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸')
  }

  /**
   * í†µê³„ ì¡°íšŒ
   */
  const getUpgradeStats = () => {
    return {
      ...upgradeStats,
      config: benchmarkConfig
    }
  }

  /**
   * í†µê³„ ë¦¬ì…‹
   */
  const resetStats = () => {
    upgradeStats.totalBenchmarks = 0
    upgradeStats.successfulUpgrades = 0
    upgradeStats.fallbackToV8L = 0
    upgradeStats.currentModel = 'yolo11m-seg@768'
  }

  return {
    loading,
    error,
    upgradeStats,
    benchmarkModel,
    runUpgradePipeline,
    compareModels,
    prepareModelDeployment,
    getUpgradeStats,
    resetStats
  }
}
