#!/usr/bin/env python3
"""
CLIP ì„ë² ë”© ì„œë¹„ìŠ¤ (FastAPI)
- ViT-L/14 ëª¨ë¸ ì‚¬ìš©, 768ì°¨ì› ì¶œë ¥
- L2 ì •ê·œí™” ì ìš©
- OpenAI APIì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
"""

import os
import sys
import json
import asyncio
from typing import List, Dict, Any, Optional
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import torch
from transformers import CLIPProcessor, CLIPModel
import numpy as np
from pydantic import BaseModel
from PIL import Image
import base64
from io import BytesIO

# ëª¨ë¸ ë¡œë“œ (ì „ì—­)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = None
processor = None

class EmbeddingRequest(BaseModel):
    input: str
    model: str = "clip-vit-l/14"
    dimensions: int = 768

class ImageEmbeddingRequest(BaseModel):
    image_base64: str
    model: str = "clip-vit-l/14"
    dimensions: int = 768

class EmbeddingResponse(BaseModel):
    object: str = "list"
    data: List[Dict[str, Any]]
    model: str
    usage: Dict[str, int]

async def load_clip_model():
    """CLIP ëª¨ë¸ ë¡œë“œ (transformers ì‚¬ìš©)"""
    global model, processor
    try:
        print(f"Loading CLIP ViT-L/14 on {device}...")
        model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(device)
        processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
        print("CLIP model loaded successfully")
        return True
    except Exception as e:
        print(f"Failed to load CLIP model: {e}")
        return False

async def generate_clip_embedding(text: str) -> List[float]:
    """CLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (768ì°¨ì›, L2 ì •ê·œí™”)"""
    if not model or not processor:
        raise HTTPException(status_code=500, detail="CLIP model not loaded")
    
    try:
        # í…ìŠ¤íŠ¸ í† í°í™” (transformers ì‚¬ìš©)
        inputs = processor(text=[text], return_tensors="pt", padding=True, truncation=True).to(device)
        
        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            text_features = model.get_text_features(**inputs)
            # L2 ì •ê·œí™” (OpenAI ìŠ¤íƒ€ì¼)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        # CPUë¡œ ì´ë™ í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
        embedding = text_features.cpu().numpy()[0].tolist()
        
        # ì°¨ì› ê²€ì¦
        if len(embedding) != 768:
            raise ValueError(f"Expected 768 dimensions, got {len(embedding)}")
        
        return embedding
        
    except Exception as e:
        print(f"CLIP embedding generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Embedding generation failed: {str(e)}")

async def generate_clip_image_embedding(image_base64: str) -> List[float]:
    """CLIP ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± (768ì°¨ì›, L2 ì •ê·œí™”)"""
    if not model or not processor:
        raise HTTPException(status_code=500, detail="CLIP model not loaded")
    
    try:
        # base64 ë””ì½”ë”©
        if image_base64.startswith('data:image'):
            # data:image/png;base64,xxx í˜•ì‹ ì²˜ë¦¬
            image_base64 = image_base64.split(',')[1]
        
        image_data = base64.b64decode(image_base64)
        image = Image.open(BytesIO(image_data)).convert('RGB')
        
        # ğŸ”§ ìˆ˜ì •ë¨: ì´ë¯¸ì§€ í¬ê¸° ê²€ì¦ (ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ ë°©ì§€)
        width, height = image.size
        if width < 1 or height < 1:
            raise ValueError(f"Image size too small: {width}x{height}")
        if width > 10000 or height > 10000:
            raise ValueError(f"Image size too large: {width}x{height}")
        
        # ğŸ”§ ìˆ˜ì •ë¨: ìµœì†Œ í¬ê¸° ë³´ì¥ (1x1 ê°™ì€ ê²½ìš° ë¦¬ì‚¬ì´ì¦ˆ) - í¬ë¡­ í’ˆì§ˆ ê°œì„ 
        min_size = 128  # 64 â†’ 128 (ë” ë‚˜ì€ ì„ë² ë”© í’ˆì§ˆ, ê·¼ë³¸ ì›ì¸ í•´ê²°)
        if width < min_size or height < min_size:
            # ì‘ì€ ì´ë¯¸ì§€ëŠ” ìµœì†Œ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ë¹„ìœ¨ ìœ ì§€)
            if width < height:
                new_width = min_size
                new_height = int(height * (min_size / width))
            else:
                new_height = min_size
                new_width = int(width * (min_size / height))
            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            print(f"[WARNING] Image resized from {width}x{height} to {new_width}x{new_height} (min_size: {min_size})")
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        # ğŸ”§ ìˆ˜ì •ë¨: processor í˜¸ì¶œ ì‹œ ëª…ì‹œì ìœ¼ë¡œ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
        try:
            inputs = processor(images=[image], return_tensors="pt", padding=True).to(device)
        except Exception as proc_error:
            # ì „ì²˜ë¦¬ ì˜¤ë¥˜ ì‹œ ìƒì„¸ ì •ë³´ ì¶œë ¥
            print(f"[ERROR] Image preprocessing failed: {proc_error}")
            print(f"[ERROR] Image size: {image.size}, mode: {image.mode}")
            raise ValueError(f"Image preprocessing failed: {str(proc_error)}")
        
        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            image_features = model.get_image_features(**inputs)
            # L2 ì •ê·œí™” (OpenAI ìŠ¤íƒ€ì¼)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        
        # CPUë¡œ ì´ë™ í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
        embedding = image_features.cpu().numpy()[0].tolist()
        
        # ì°¨ì› ê²€ì¦
        if len(embedding) != 768:
            raise ValueError(f"Expected 768 dimensions, got {len(embedding)}")
        
        return embedding
        
    except HTTPException:
        raise
    except Exception as e:
        error_msg = str(e)
        print(f"CLIP image embedding generation failed: {error_msg}")
        print(f"Error type: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Image embedding generation failed: {error_msg}")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="CLIP Embedding Service",
    description="CLIP ViT-L/14 ê¸°ë°˜ 768ì°¨ì› ì„ë² ë”© ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    """ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    success = await load_clip_model()
    if not success:
        print("[ERROR] Failed to load CLIP model, service may not work properly")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    model_loaded = model is not None and processor is not None
    return {
        "status": "healthy" if model_loaded else "unhealthy",
        "model": "clip-vit-l/14",
        "device": device,
        "dimensions": 768,
        "model_loaded": model_loaded
    }

@app.post("/v1/embeddings", response_model=EmbeddingResponse)
async def create_embeddings(request: EmbeddingRequest):
    """ì„ë² ë”© ìƒì„± ì—”ë“œí¬ì¸íŠ¸ (OpenAI API í˜¸í™˜)"""
    try:
        # ì…ë ¥ ê²€ì¦
        if not request.input or not request.input.strip():
            raise HTTPException(status_code=400, detail="Input text is required")
        
        # ì°¨ì› ê²€ì¦ (CLIP ViT-L/14ëŠ” 768ì°¨ì› ê³ ì •)
        if request.dimensions != 768:
            print(f"[WARNING] Requested {request.dimensions} dimensions, using 768 (CLIP ViT-L/14)")
        
        # ì„ë² ë”© ìƒì„±
        embedding = await generate_clip_embedding(request.input.strip())
        
        # OpenAI API í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
        response = EmbeddingResponse(
            data=[{
                "object": "embedding",
                "index": 0,
                "embedding": embedding
            }],
            model="clip-vit-l/14",
            usage={
                "prompt_tokens": len(request.input.split()),
                "total_tokens": len(request.input.split())
            }
        )
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"Embedding request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.post("/v1/image-embeddings")
async def create_image_embeddings(request: ImageEmbeddingRequest):
    """ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ì…ë ¥ ê²€ì¦
        if not request.image_base64 or not request.image_base64.strip():
            raise HTTPException(status_code=400, detail="image_base64 is required")
        
        # ì°¨ì› ê²€ì¦ (CLIP ViT-L/14ëŠ” 768ì°¨ì› ê³ ì •)
        if request.dimensions != 768:
            print(f"[WARNING] Requested {request.dimensions} dimensions, using 768 (CLIP ViT-L/14)")
        
        # ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±
        embedding = await generate_clip_image_embedding(request.image_base64.strip())
        
        # OpenAI API í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
        return {
            "object": "list",
            "data": [{
                "object": "embedding",
                "index": 0,
                "embedding": embedding
            }],
            "model": "clip-vit-l/14",
            "usage": {
                "prompt_tokens": 0,
                "total_tokens": 0
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"Image embedding request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.get("/")
@app.head("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ (GET/HEAD ì§€ì›)"""
    return {
        "service": "CLIP Embedding Service",
        "version": "1.0.0",
        "model": "clip-vit-l/14",
        "dimensions": 768,
        "endpoints": {
            "health": "/health",
            "embeddings": "/v1/embeddings",
            "image_embeddings": "/v1/image-embeddings"
        }
    }

if __name__ == "__main__":
    # ê³ ì • í¬íŠ¸ 3021 ì‚¬ìš© (ê·¼ë³¸ ë¬¸ì œ í•´ê²°)
    port = 3021
    host = "0.0.0.0"
    
    print(f"Starting CLIP Embedding Service on {host}:{port}")
    print(f"Model: CLIP ViT-L/14, Device: {device}")
    print(f"Dimensions: 768")
    
    uvicorn.run(
        "clip-embedding-service:app",
        host=host,
        port=port,
        reload=False,
        log_level="info"
    )
